seed: &seed 847582
levels: 2
agent: hima_dreaming
project: four_rooms_swap_dreaming_test
entity: pkuderov  # don't forget to change if you use logging

scenario: '../../experiments/htm_agent/scenarios/four_rooms_swap.yaml'

vis_options:
  size: 21
  max_options: 50
  action_displace: [[0, 1], [1, 0], [0, -1], [-1, 0]]
  action_rotation: [0, 0, 0, 0]

log: true  # wand logging
path_to_store_logs: '/Users/kuderov/dev/logs/hima/'  # important: change, if you are Windows user

run_options:
  n_episodes: 100000
  log_every_episode: 100
  log_values_int: True
  log_values_ext: True
  log_priorities: True
  log_values: False
  log_policy: False
  log_option_values: False
  log_option_policy: False
  log_options_usage: False
  log_td_error: True
  log_anomaly: False
  log_confidence: False
  log_modulation: False
  log_segments: False
  log_terminal_stat: True
  log_empowerment: False
  log_number_of_clusters: False
  draw_options: False
  draw_options_stats: False
  opt_threshold: 50
  train_patterns: True
  animation_fps: 3

environment:
  seed: 432921
  shape_xy: [9, 9]

  actions_cost:
    base_cost: -0.005
    weights:
      stay: 2.0
      turn: 1.0
      move: 1.0
  actions:
    - move right
    - move down
    - move left
    - move up
    - stay

  rendering:
#    view_rectangle: [[-1, -1], [1, 1]]
    view_rectangle: [[-2, -2], [2, 2]]
#    view_rectangle: [[-3, -3], [3, 3]]

  areas:
#    n_types: 4
    n_types: 5

  obstacle:
    map_name: '../../experiments/htm_agent/maps/four_rooms_9x9.map'

  food:
    n_items: 1
    reward: 1.
    positions: [[4, 6]]

  agent:
    change_position: true
    direction: up
    positions: [ [ 1, 2 ], [ 7, 2 ], [ 7, 6 ], [ 1, 6 ] ]

  terminate:
    episode_max_steps: 200
#    episode_max_steps: 100
    early_stop: true
    n_items_to_collect: 1

cagent:
  use_intrinsic_reward: false
  use_dreaming: true
  alpha: 1.0
  punish_intrinsic_reward: 0
  action:
    do_nothing_action: 4
    noise_tolerance: 0.1
    patterns:
      - [ 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
      - [ 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0 ]
      - [ 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0 ]
      - [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1 ]
  muscles:
    connected_permanence: 0.5
    depolarized_decrement: 0.01
    initial_permanence: 0.6
    max_segments_per_cell: 32
    noise_tolerance: 0.1
    permanence_decrement: 0.01
    permanence_increment: 0.1
    sample_size: 1.0
    seed: *seed
  empowerment:
    horizon: 4
    similarity_threshold: 0.6
    memory: false
    evaluate: false
    tm_config:
      noise_tolerance: 0.25
      learning_margin: 0.35
      cellsPerColumn: 20
      initialPermanence: 0.5
      connectedPermanence: 0.5
      permanenceIncrement: 0.02
      permanenceDecrement: 0.001
      predictedSegmentDecrement: 0.001
      maxSegmentsPerCell: 5
    seed: *seed
    filename: '../../experiments/htm_agent/empowerments/empowerment_real_4_p1_3.json'
  dreaming:
    enabled: true
    anomaly_based_falling_asleep:
      anomaly_threshold: [0.4, 0.3, 0.45]
      probability: [.0, .005, .24]
      breaking_point: .25
      power: 1.25

    on_wake_dreaming_match_rate_threshold: .871
    on_wake_dreaming_match_rate_prob_inh: .0018
    on_wake_dreaming_match_rate_an_th_inh: .0002
    on_wake_prob_decay: 1.

    on_awake_step_surprise_threshold: .2
#    on_awake_step_anticipation_decay: .987
    on_awake_step_anticipation_decay: .98
    on_awake_step_reward_ma_eps: .003
    on_awake_step_prob_delta_anticipation_scale: .00275
    on_awake_step_anomaly_err_ma_eps: .0073
    on_awake_step_ma_based_an_th_delta: .00064
    on_awake_step_long_ma_based_an_th_threshold: .0071
    on_awake_step_long_ma_based_an_th_delta: .00126

    dreaming_match_rate_ma_tracker: [1.0, 0.1, 0.5]
    reward_ma_tracker: [0.02, 0.005, 0.0]
    anomaly_error_ma_tracker: [0.1, 0.02, -0.1]

    prediction_depth: 20
    max_n_rollouts: 1

    sa_encoder:
      state_clusters:
        similarity_threshold: [.7, .6, .8]
        similarity_threshold_delta_on_hit: .02
        similarity_threshold_delta_on_miss: -0.05
        max_n_clusters: 80
        max_tracked_bits_rate: 2.

    reward_model:
      learning_rate: [ .5, 1. ]

    anomaly_model:
      learning_rate: [ .2, 1. ]

    transition_model:
      tm:
        cells_per_column: 1
        initial_permanence: .41
        connected_permanence: .5
        activation_threshold: .8
        learning_threshold: .65
        max_new_synapse_count: 1.1
        max_synapses_per_segment: 1.2
        predictedSegmentDecrement: .001
        permanenceIncrement: .1
        permanenceDecrement: .05
        maxSegmentsPerCell: 12
    seed: *seed

# specify blocks and substitute default parameters
blocks:
  0:
    bg: null
    block:
      level: 1
    sm: {}
    sp:
      #
#      boostStrength: 0.0
      boostStrength: 1.0  # !important!
      localAreaDensity: 0.02
      potentialPct: 0.5
      # ------new overriden params ---------
      dutyCyclePeriod: 40000  # should be big enough if boostStrength > 0: 20k+
      minPctOverlapDutyCycle: 0.0002  # or set to 0: !important!
      synPermConnected: 0.5
      synPermActiveInc: 0.02  # lower LR to slow down encoding change: may be unimportant
      synPermInactiveDec: 0.01  # -//-
    tm:
      basal_columns: 1000
  1:
    bg:
      alpha: 0.1
      beta: 0.1
      alpha_int: 0.01
      beta_int: 0.01
      off_policy: true
      softmax_beta: 1.2
      epsilon_noise: 0.0
    block:
      modulate_tm_lr: true
      level: 1
    sm: {}
    sp:
      boostStrength: 0.0
      localAreaDensity: 0.02
      potentialPct: 0.5
    tm:
      basal_columns: 1000

cells_per_column: &cells_per_column 15

# specify connections
hierarchy:
  block_connections: [
    {
      'basal_in': [ ],
      'apical_in': [ ],
      'feedback_in': [ ],
      'basal_out': [ 2 ],
      'apical_out': [ ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ ],
      'apical_in': [ ],
      'feedback_in': [ ],
      'basal_out': [ 3 ],
      'apical_out': [ ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ 0 ],
      'apical_in': [ 3 ],
      'feedback_in': [ ],
      'basal_out': [ ],
      'apical_out': [ 3 ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ 1 ],
      'apical_in': [ 2 ],
      'feedback_in': [ ],
      'basal_out': [ ],
      'apical_out': [ 2 ],
      'feedback_out': [ ]
    }
  ]
  input_blocks:
  - 0
  - 1
  output_block: 3
  visual_block: 2

input_block_default:
  columns: 0
  level: 0

muscles_size: 12

# default parameters
spatial_memory_default:
  activation_threshold: 1
  initial_permanence: 1.0
  overlap_threshold: 0.9
  permanence_decrement: 0.0
  permanence_forgetting_decrement: 0.0
  permanence_increment: 0.0
  permanence_threshold: 0.0
spatial_pooler_default:
  boostStrength: 0.0
  columnDimensions:
  - 1000
  dutyCyclePeriod: 1000
  globalInhibition: true
  localAreaDensity: 0.04
  minPctOverlapDutyCycle: 0.001
  numActiveColumnsPerInhArea: 0
  potentialPct: 0.5
  seed: *seed
  spVerbosity: 0
  stimulusThreshold: 1
  synPermActiveInc: 0.1
  synPermConnected: 0.1
  synPermInactiveDec: 0.01
  wrapAround: true
temporal_memory_default:
  apical_cells_per_column: *cells_per_column
  basal_cells_per_column: *cells_per_column
  noise_tolerance: 0.1
  max_segments_per_cell: 8
  max_segments_per_cell_apical: 8
  max_segments_per_cell_exec: 8
  max_segments_per_cell_inhib: 8
  anomaly_window: 500
  confidence_window: 500
  initial_permanence: 0.1
  connected_threshold: 0.5
  permanence_increment: 0.1
  permanence_decrement: 0.01
  predicted_segment_decrement: 0.001
  initial_permanence_apical: 0.1
  connected_threshold_apical: 0.5
  permanence_increment_apical: 0.1
  permanence_decrement_apical: 0.01
  predicted_segment_decrement_apical: 0.001
  initial_permanence_exec: 0.1
  connected_threshold_exec: 0.5
  permanence_increment_exec: 0.01
  permanence_decrement_exec: 0.001
  predicted_segment_decrement_exec: 0.0001
  initial_permanence_inhib: 0.1
  connected_threshold_inhib: 0.5
  permanence_increment_inhib: 0.1
  permanence_decrement_inhib: 0.001
  predicted_segment_decrement_inhib: 0.005
  enable_pruning_basal: False
  enable_pruning_apical: False
  enable_pruning_exec: False
  enable_pruning_inhib: False
  pruning_period_basal: 3000
  pruning_period_apical: 2000
  pruning_period_exec: 5000
  pruning_period_inhib: 2000
  prune_zero_synapses: True
  seed: *seed
  timeseries: True

basal_ganglia_default:
  alpha: 0.01  # external striatum learning rate
  alpha_int: 0.1  # internal striatum learning rate
  beta: 0.01  # external striatum learning rate
  beta_int: 0.1  # internal striatum learning rate
  discount_factor: 0.95  # external reward
  discount_factor_int: 0.5 # internal reward
  off_policy: true  # true: use Q-learning, false: use SARSA for external striatum region
  off_policy_int: true  # the same for internal striatum region
  softmax_beta: 1.0  # softmax inverse temperature, controls thalamus noise
  epsilon_noise: 0.0  # another way to control thalamus noisiness
  priority_ext: 1.0  # scale factor for external reward
  priority_int: 0.1  # scale factor for internal reward
  td_error_threshold: 0.01  # margin for td error condition: td < -threshold, experimental
  priority_inc_factor: 1.1  # multiply priority if td error is positive, experimental
  priority_dec_factor: 0.9  # else, experimental
  use_reward_modulation: true  # true: instead of TD error use reward modulation for priority
  sm_reward_inc: 0.9  # smooth factor for reward modulation during increment
  sm_reward_dec: 0.998  # smooth factor for reward modulation during decrement
  sm_max_reward: 0.99  # smooth factor for max reward
  sm_min_reward: 0.99  # smooth factor for min reward
  max_reward_decay: 0.999  # decay factor for max reward
  min_reward_decay: 0.999  # decay factor for min reward
  seed: *seed

block_default:
  gamma: 0.997  # discount factor for option rewards
  predicted_boost: 0.0  # boost predicted elementary actions
  feedback_boost_range: [0.0, 0.0]  # max and min boost percents for elementary actions corresponding to an option, switch off options [0.0, 0.0]
  modulate_tm_lr: false  # use reward modulation for lr of TM
  sm_reward_inc: 0.9  # smooth factor for reward modulation during increment
  sm_reward_dec: 0.98  # smooth factor for reward modulation during decrement
  sm_max_reward: 0.99  # smooth factor for max reward
  sm_min_reward: 0.99  # smooth factor for min reward
  max_reward_decay: 0.999  # decay factor for max reward
  min_reward_decay: 0.999  # decay factor for min reward
  d_an_th: 0.1  # threshold for anomaly deviation from mean, used for conditions
  d_cn_th: 0.1  # threshold for confidence deviation from mean, used for conditions
  sm_da: 0.9  # smooth factor for dopamine(squared td error)
  sm_dda: 0.995  # smooth factor for derivative of dopamine
