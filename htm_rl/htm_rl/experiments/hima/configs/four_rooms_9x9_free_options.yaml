seed: &seed 847582
levels: 2
agent: hima
project: four_rooms_9x9_free_options
entity: you  # don't forget to change if you use logging

scenario: '../../experiments/hima/scenarios/four_rooms_9x9.yaml'

vis_options:
  size: 21
  max_options: 50
  action_displace: [[0, 1], [1, 0], [0, -1], [-1, 0]]
  action_rotation: [0, 0, 0, 0]

log: true  # wand logging
path_to_store_logs: '/tmp'  # important: change, if you are Windows user

run_options:
  n_episodes: 4000
  log_every_episode: 25
  log_values_int: False
  log_values_ext: False
  log_priorities: False
  log_values: True
  log_policy: True
  log_option_values: False
  log_option_policy: False
  log_options_usage: True
  log_td_error: True
  log_anomaly: False
  log_confidence: False
  log_modulation: True
  log_segments: True
  log_terminal_stat: True
  log_empowerment: False
  log_number_of_clusters: False
  draw_options: True
  draw_options_stats: True
  opt_threshold: 50
  train_patterns: True
  animation_fps: 3

environment:
  seed: 432921
  shape_xy: [9, 9]

  actions_cost:
    base_cost: -0.005
    weights:
      stay: 2.0
      turn: 1.0
      move: 1.0
  actions:
    - move right
    - move down
    - move left
    - move up
    - stay

  rendering:
    view_rectangle: [[-1, -1], [1, 1]]

  areas:
    n_types: 4

  obstacle:
    density: 0.1
    map_name: '../../experiments/hima/maps/four_rooms_9x9.map'

  food:
    n_items: 1
    reward: 1.
    positions: [ [ 5, 7 ] ]

  agent:
    change_position: true
    direction: up
  #  rendering:
  #    what:
  #      - position
  ##      - view direction
  #    bucket_size: 3

  terminate:
    episode_max_steps: 200
    early_stop: true
    n_items_to_collect: 1

cagent:
  use_intrinsic_reward: false
  use_dreaming: false
  alpha: 1.0
  punish_intrinsic_reward: -10
  action:
    do_nothing_action: 4
    noise_tolerance: 0.1
    patterns:
    - [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    - [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]
    - [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]
    - [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
  muscles:
    connected_permanence: 0.5
    depolarized_decrement: 0.01
    initial_permanence: 0.6
    max_segments_per_cell: 32
    noise_tolerance: 0.1
    permanence_decrement: 0.01
    permanence_increment: 0.1
    sample_size: 1.0
    seed: *seed
  empowerment:
    horizon: 3
    similarity_threshold: 0.6
    memory: true
    evaluate: false
    tm_config:
      noise_tolerance: 0.25
      learning_margin: 0.35
      cellsPerColumn: 20
      initialPermanence: 0.5
      connectedPermanence: 0.5
      permanenceIncrement: 0.02
      permanenceDecrement: 0.001
      predictedSegmentDecrement: 0.001
      maxSegmentsPerCell: 5
    seed: *seed
  dreaming:
    enabled: false
    anomaly_based_falling_asleep:
      anomaly_threshold: 1.0
      alpha: .8
      beta: 2.
      max_prob: .15

    prediction_depth: 18
    n_prediction_rollouts: [ 3, 12 ]

    sa_encoder:
      clusters_similarity_threshold: .7

    reward_model:
      learning_rate: [ 1., 1. ]

    anomaly_model:
      learning_rate: [ .1, 1. ]

    transition_model:
      tm:
        cells_per_column: 1
        initial_permanence: .3
        connected_permanence: .3
        activation_threshold: .88
        learning_threshold: .8
        max_new_synapse_count: 1.0
        max_synapses_per_segment: 1.0
        predictedSegmentDecrement: .001
        permanenceIncrement: .05
        permanenceDecrement: .01
        maxSegmentsPerCell: 8
    seed: *seed

# specify blocks and substitute default parameters
blocks:
  0:
    bg: null
    block:
      level: 1
    sm: {}
    sp:
      boostStrength: 0.0
      localAreaDensity: 0.02
      potentialPct: 0.5
    tm:
      basal_columns: 1000
  1:
    bg:
      alpha: 0.1
      beta: 0.1
      alpha_int: 0.01
      beta_int: 0.01
      off_policy: true
      softmax_beta: 1.0
      epsilon_noise: 0.0
    block:
      modulate_tm_lr: true
      level: 1
    sm: {}
    sp:
      boostStrength: 0.0
      localAreaDensity: 0.02
      potentialPct: 0.5
    tm:
      basal_columns: 1000

  2:
    bg: null
    block:
      level: 2
    sm: { }
    sp:
      boostStrength: 0.0
      localAreaDensity: 0.02
      potentialPct: 0.5
    tm:
      basal_columns: 1000
  3:
    bg:
      alpha: 0.1
      beta: 0.1
      off_policy: true
      softmax_beta: 1.0
      epsilon_noise: 0.0
    block:
      level: 2
    sm: { }
    sp:
      boostStrength: 0.0
      localAreaDensity: 0.02
      potentialPct: 0.5
    tm:
      basal_columns: 1000

cells_per_column: &cells_per_column 15

# specify connections
hierarchy:
  block_connections: [
    {
      'basal_in': [ ],
      'apical_in': [ ],
      'feedback_in': [ ],
      'basal_out': [ 2 ],
      'apical_out': [ ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ ],
      'apical_in': [ ],
      'feedback_in': [ ],
      'basal_out': [ 3 ],
      'apical_out': [ ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ 0 ],
      'apical_in': [ 3 ],
      'feedback_in': [ 4 ],
      'basal_out': [ 4 ],
      'apical_out': [ 3 ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ 1 ],
      'apical_in': [ 2 ],
      'feedback_in': [ 5 ],
      'basal_out': [ 5 ],
      'apical_out': [ 2 ],
      'feedback_out': [ ]
    },

    {
      'basal_in': [ 2 ],
      'apical_in': [ 5 ],
      'feedback_in': [ ],
      'basal_out': [ ],
      'apical_out': [ 5 ],
      'feedback_out': [ 2 ]
    },

    {
      'basal_in': [ 3 ],
      'apical_in': [ 4 ],
      'feedback_in': [ ],
      'basal_out': [ ],
      'apical_out': [ 4 ],
      'feedback_out': [ 3 ]
    }
  ]
  input_blocks:
  - 0
  - 1
  output_block: 3
  visual_block: 2

input_block_default:
  columns: 0
  level: 0

muscles_size: 12

# default parameters
spatial_memory_default:
  activation_threshold: 1
  initial_permanence: 1.0
  overlap_threshold: 0.9
  permanence_decrement: 0.0
  permanence_forgetting_decrement: 0.0
  permanence_increment: 0.0
  permanence_threshold: 0.0
spatial_pooler_default:
  boostStrength: 0.0
  columnDimensions:
  - 1000
  dutyCyclePeriod: 1000
  globalInhibition: true
  localAreaDensity: 0.04
  minPctOverlapDutyCycle: 0.001
  numActiveColumnsPerInhArea: 0
  potentialPct: 0.5
  seed: *seed
  spVerbosity: 0
  stimulusThreshold: 1
  synPermActiveInc: 0.1
  synPermConnected: 0.1
  synPermInactiveDec: 0.01
  wrapAround: true
temporal_memory_default:
  apical_cells_per_column: *cells_per_column
  basal_cells_per_column: *cells_per_column
  noise_tolerance: 0.1
  max_segments_per_cell: 8
  max_segments_per_cell_apical: 8
  max_segments_per_cell_exec: 8
  max_segments_per_cell_inhib: 8
  anomaly_window: 500
  confidence_window: 500
  initial_permanence: 0.1
  connected_threshold: 0.5
  permanence_increment: 0.1
  permanence_decrement: 0.01
  predicted_segment_decrement: 0.001
  initial_permanence_apical: 0.1
  connected_threshold_apical: 0.5
  permanence_increment_apical: 0.1
  permanence_decrement_apical: 0.01
  predicted_segment_decrement_apical: 0.001
  initial_permanence_exec: 0.1
  connected_threshold_exec: 0.5
  permanence_increment_exec: 0.01
  permanence_decrement_exec: 0.001
  predicted_segment_decrement_exec: 0.0001
  initial_permanence_inhib: 0.1
  connected_threshold_inhib: 0.5
  permanence_increment_inhib: 0.1
  permanence_decrement_inhib: 0.001
  predicted_segment_decrement_inhib: 0.005
  enable_pruning_basal: False
  enable_pruning_apical: False
  enable_pruning_exec: False
  enable_pruning_inhib: False
  pruning_period_basal: 3000
  pruning_period_apical: 2000
  pruning_period_exec: 5000
  pruning_period_inhib: 2000
  prune_zero_synapses: True
  seed: *seed
  timeseries: True

basal_ganglia_default:
  alpha: 0.01  # external striatum learning rate
  alpha_int: 0.1  # internal striatum learning rate
  beta: 0.01  # external striatum learning rate
  beta_int: 0.1  # internal striatum learning rate
  discount_factor: 0.95  # external reward
  discount_factor_int: 0.5 # internal reward
  off_policy: true  # true: use Q-learning, false: use SARSA for external striatum region
  off_policy_int: true  # the same for internal striatum region
  softmax_beta: 1.0  # softmax inverse temperature, controls thalamus noise
  epsilon_noise: 0.0  # another way to control thalamus noisiness
  priority_ext: 1.0  # scale factor for external reward
  priority_int: 0.1  # scale factor for internal reward
  td_error_threshold: 0.01  # margin for td error condition: td < -threshold, experimental
  priority_inc_factor: 1.1  # multiply priority if td error is positive, experimental
  priority_dec_factor: 0.9  # else, experimental
  use_reward_modulation: true  # true: instead of TD error use reward modulation for priority
  sm_reward_inc: 0.9  # smooth factor for reward modulation during increment
  sm_reward_dec: 0.998  # smooth factor for reward modulation during decrement
  sm_max_reward: 0.99  # smooth factor for max reward
  sm_min_reward: 0.99  # smooth factor for min reward
  max_reward_decay: 0.999  # decay factor for max reward
  min_reward_decay: 0.999  # decay factor for min reward
  seed: *seed

block_default:
  gamma: 0.997  # discount factor for option rewards
  predicted_boost: 0.0  # boost predicted elementary actions
  feedback_boost_range: [0.0, 0.8]  # max and min boost percents for elementary actions corresponding to an option, switch off options [0.0, 0.0]
  modulate_tm_lr: false  # use reward modulation for lr of TM
  sm_reward_inc: 0.9  # smooth factor for reward modulation during increment
  sm_reward_dec: 0.98  # smooth factor for reward modulation during decrement
  sm_max_reward: 0.99  # smooth factor for max reward
  sm_min_reward: 0.99  # smooth factor for min reward
  max_reward_decay: 0.999  # decay factor for max reward
  min_reward_decay: 0.999  # decay factor for min reward
  d_an_th: 0.1  # threshold for anomaly deviation from mean, used for conditions
  d_cn_th: 0.1  # threshold for confidence deviation from mean, used for conditions
  sm_da: 0.9  # smooth factor for dopamine(squared td error)
  sm_dda: 0.995  # smooth factor for derivative of dopamine
