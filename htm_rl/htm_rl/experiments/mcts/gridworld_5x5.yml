verbosity: &verbosity 1

# ============== seed =================
random_seed_setter: !RandomSeedSetter
  seed: &seed 8713

# =========== environment =============
gridworld_map_generator: !GridworldMapGenerator &gridworld_generator
  seed: *seed
  size: 5
  density: .7
  verbosity: *verbosity

.env_dimensions:
  n_states: &n_states 25
  n_actions: &n_actions 4

transfer_learning_experiment_runner: !TransferLearningExperimentRunner2
  env_generator: *gridworld_generator
  n_episodes_all_fixed: 50
  n_initial_states: 1
  n_terminal_states: 1
  n_environments: 1
  verbosity: *verbosity

# agent runners
.base_agent_runner: &base_runner
  env:
  n_episodes:
  max_steps: 30
  agent:
  verbosity: *verbosity

# test run results processing
run_results_processor: !RunResultsProcessor
  env_name: gridworld_5x5
  moving_average: 4
  verbosity: *verbosity
  test_dir:                   # leave empty for default behavior - output to the config's dir

# =========== encoder =================
.state_bits: &state_bits 50

sa_sdr_encoder: &sa_sdr_encoder !SaSdrEncoder
  state_encoder: !RandomSdrEncoder
    name: state
    n_states: *n_states
    fixed_base: [0, 0]
    total_bits: *state_bits
    sparsity: .2
    seed: *seed
    default_format: full
  action_encoder: !IntSdrEncoder
    name: action
    n_values: *n_actions
    value_bits: 10
    activation_threshold: 8
    default_format: full

# ========== Temporal Memory / Memory ==========
# should be unique for each agent
.base_temporal_memory: &base_tm
  n_columns: !property [*sa_sdr_encoder, total_bits]
  cells_per_column: 1
  initial_permanence: .5
  connected_permanence: .4
  activation_threshold: 18
  learning_threshold: 14
  predictedSegmentDecrement: .0001
  permanenceIncrement: .1
  permanenceDecrement: .03
#  maxNewSynapseCount: !property [*sa_sdr_encoder, value_bits]
#  maxSynapsesPerSegment: !property [*sa_sdr_encoder, value_bits]
  maxNewSynapseCount: 20
  maxSynapsesPerSegment: 23
  maxSegmentsPerCell: 12               # as only 4 actions available
  seed: *seed

.base_agent_memory: &base_agent_memory
  tm:
  encoder: *sa_sdr_encoder
  sdr_formatter: !property [*sa_sdr_encoder, format]
  sa_superposition_formatter: !sa_superposition_formatter
  collect_anomalies: False

# =========== Agents and Runners ==============
.base_htm_agent: &base_htm_agent
  memory:
  planner:
  n_actions: *n_actions

.base_htm_agent_runner: &base_htm_runner
  <<: *base_runner
  pretrain: 0

.base_mcts_planner: &base_mcts_planner
  memory:
  planning_horizon: 1
  n_actions: *n_actions

.base_mcts_actor_critic: &base_mcts_actor_critic
  cells_sdr_size: *state_bits
  update_cell_ratio: 1.
  discount_factor: .95
  seed: *seed
  online: True

agent_runners:
  dqn: !DqnAgentRunner
    <<: *base_runner
    agent: !DqnAgent
      n_states: *n_states
      n_actions: *n_actions
      epsilon: .15
      gamma: .975
      learning_rate: .3e-3
      seed: *seed

  mcts_sp: !MctsAgentRunner
    <<: *base_htm_runner
    agent: !MctsAgent
      <<: *base_htm_agent
      memory: !Memory &tm_mcts
        <<: *base_agent_memory
        tm: !TemporalMemory { <<: *base_tm }
      planner: !MctsPlanner
        <<: *base_mcts_planner
        memory: *tm_mcts
      mcts_actor_critic: !MctsActorCritic
        <<: *base_mcts_actor_critic
