verbosity: &verbosity 1

# seed
random_seed_setter: !RandomSeedSetter
  seed: &seed 1337

# environment
.base_env: &base_env
  cell_transitions:
  initial_cell: 0
  initial_direction: 0
  cell_gonality: 4
  allow_clockwise_action: False
  seed: *seed

env: !env &env
  <<: *base_env
  cell_transitions: !preset_env_transitions multi_way_v0

.env_dimensions:
  n_states: !property &n_states [*env, n_states]
  n_actions: !property &n_actions [*env, n_actions]

# htm agent related parts
.base_encoder: &base_encoder
  name:
  n_values:
  value_bits: 8
  activation_threshold: 7
  default_format: short

sa_sdr_encoder: &sa_sdr_encoder !SaSdrEncoder
  state_encoder: !IntSdrEncoder
    <<: *base_encoder
    name: state
    n_values: *n_states
  action_encoder: !IntSdrEncoder
    <<: *base_encoder
    name: action
    n_values: *n_actions

sar_sdr_encoder: &sar_sdr_encoder !SarSdrEncoder
  <<: *sa_sdr_encoder
  reward_encoder: !IntSdrEncoder
    <<: *base_encoder
    name: state
    n_values: 2

sar_temporal_memory: !TemporalMemory &sar_tm
  n_columns: !property [*sar_sdr_encoder, total_bits]
  cells_per_column: 1
  initial_permanence: .5
  connected_permanence: .4
  activation_threshold: 21            # 7*3
  learning_threshold: 18              # [a+r].value_bits < x < activation_threshold
  predictedSegmentDecrement: .0001
  permanenceIncrement: .1
  permanenceDecrement: .05
  maxNewSynapseCount: 24              # enc.value_bits
  maxSynapsesPerSegment: 24           # enc.value_bits
  seed: *seed

legacy_agent_memory: !LegacyMemory &legacy_agent_memory
  tm: *sar_tm
  encoder: *sar_sdr_encoder
  sdr_formatter: !property [*sar_sdr_encoder, format]
  sar_superposition_formatter: !sar_superposition_formatter
  collect_anomalies: False

temporal_memory: !TemporalMemory &tm
  n_columns: !property [*sa_sdr_encoder, total_bits]
  cells_per_column: 1
  initial_permanence: .5
  connected_permanence: .4
  activation_threshold: 14            # 7*2
  learning_threshold: 11              # action.value_bits < x < activation_threshold
  predictedSegmentDecrement: .0001
  permanenceIncrement: .1
  permanenceDecrement: .05
  maxNewSynapseCount: !property [*sa_sdr_encoder, value_bits]
  maxSynapsesPerSegment: !property [*sa_sdr_encoder, value_bits]
  seed: *seed

agent_memory: !Memory &agent_memory
  tm: *tm
  encoder: *sa_sdr_encoder
  sdr_formatter: !property [*sa_sdr_encoder, format]
  sa_superposition_formatter: !sa_superposition_formatter
  collect_anomalies: False

# NB: all agents will use the same TM object!
.base_legacy_htm_agent: &base_legacy_htm_agent
  memory: *legacy_agent_memory
  planner: &legacy_base_planner
    memory: *legacy_agent_memory
    planning_horizon:
  n_actions: *n_actions
  use_cooldown: False

.base_htm_agent: &base_htm_agent
  memory: *agent_memory
  planner: &base_planner
    memory: *agent_memory
    planning_horizon:
    goal_memory_size: 1
  n_actions: *n_actions

agents:
  dqn: !DqnAgent &dqn_agent
    n_states: *n_states
    n_actions: *n_actions
    epsilon: .15
    gamma: .975
    learning_rate: .3e-3
    seed: *seed

  htm_0: !LegacyAgent &htm_0_agent
    <<: *base_legacy_htm_agent
    planner: !LegacyPlanner
      <<: *legacy_base_planner
      planning_horizon: 0

  htm_1: !LegacyAgent &htm_1_agent
    <<: *base_legacy_htm_agent
    planner: !LegacyPlanner
      <<: *legacy_base_planner
      planning_horizon: 1

  htm_2: !LegacyAgent &htm_2_agent
    <<: *base_legacy_htm_agent
    planner: !LegacyPlanner
      <<: *legacy_base_planner
      planning_horizon: 2

  htm_4: !LegacyAgent &htm_4_agent
    <<: *base_legacy_htm_agent
    planner: !LegacyPlanner
      <<: *legacy_base_planner
      planning_horizon: 4

  htm_8: !LegacyAgent &htm_8_agent
    <<: *base_legacy_htm_agent
    planner: !LegacyPlanner
      <<: *legacy_base_planner
      planning_horizon: 8

  htm_1_: !Agent &htm_1_agent_
    <<: *base_htm_agent
    use_cooldown: False
    planner: !Planner
      <<: *base_planner
      planning_horizon: 1

  htm_2_: !Agent &htm_2_agent_
    <<: *base_htm_agent
    use_cooldown: False
    planner: !Planner
      <<: *base_planner
      planning_horizon: 2

  htm_4_: !Agent &htm_4_agent_
    <<: *base_htm_agent
    use_cooldown: False
    planner: !Planner
      <<: *base_planner
      planning_horizon: 4

  htm_8_: !Agent &htm_8_agent_
    <<: *base_htm_agent
    use_cooldown: False
    planner: !Planner
      <<: *base_planner
      planning_horizon: 8

# agent runners
.base_agent_runner: &base_runner
  env: *env
  n_episodes: 200
  max_steps: 100
  agent:
  verbosity: *verbosity

.base_htm_agent_runner: &base_htm_runner
  <<: *base_runner
  pretrain: 0

agent_runners:
  dqn: !DqnAgentRunner
    <<: *base_runner
    agent: *dqn_agent

  htm_0: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_0_agent

  htm_1: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_1_agent

  htm_2: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_2_agent

  htm_4: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_4_agent

  htm_8: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_8_agent

  htm_1_: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_1_agent_

  htm_2_: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_2_agent_

  htm_4_: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_4_agent_

  htm_8_: !AgentRunner
    <<: *base_htm_runner
    agent: *htm_8_agent_

# test run results processing
run_results_processor: !RunResultsProcessor
  env_name: multi_way_v0
  optimal_len: 7              # hack
  moving_average: 20
  verbosity: *verbosity
  test_dir:                   # leave empty for default behavior - output to the config's dir

