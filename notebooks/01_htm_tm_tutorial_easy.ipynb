{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to access the Temporal Memory algorithm directly\n",
    "\n",
    "This program demonstrates how to create a TM instance, train it, get predictions and\n",
    "anomalies, and inspect the state.\n",
    "\n",
    "The code here runs a very simple version of sequence learning, with one cell per column. The TM is trained with the simple sequence A->B->C->D->E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm.bindings.sdr import SDR\n",
    "from htm.algorithms import TemporalMemory as TM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux functions\n",
    "\n",
    "Utility routine for printing an SDR in a particular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatBits(sdr):\n",
    "    s = ''\n",
    "    for c in range(sdr.size):\n",
    "        if c > 0 and c % 10 == 0:\n",
    "            s += ' '\n",
    "        s += str(sdr.dense.flatten()[c])\n",
    "    s += ' '\n",
    "    return s\n",
    "\n",
    "def printStateTM( tm ):\n",
    "    # Useful for tracing internal states\n",
    "    print(\"Active cells         \" + formatBits(tm.getActiveCells()))\n",
    "    print(\"Winner cells         \" + formatBits(tm.getWinnerCells()))\n",
    "    tm.activateDendrites(True)\n",
    "    print(\"Predictive cells     \" + formatBits(tm.getPredictiveCells()))\n",
    "    print(\"Anomaly\", tm.anomaly * 100, \"%\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Temporal Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TM(\n",
    "    columnDimensions = (50,),\n",
    "    cellsPerColumn=1,\n",
    "    initialPermanence=0.5,\n",
    "    connectedPermanence=0.5,\n",
    "    minThreshold=8,\n",
    "    maxNewSynapseCount=20,\n",
    "    permanenceIncrement=0.1,\n",
    "    permanenceDecrement=0.0,\n",
    "    activationThreshold=8,\n",
    ")\n",
    "tm.printParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__My edit__. Optional playing around w/ SDR: cached property, in-place update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SDR(10)\n",
    "print(\"before: \", x.dense, \"; or sparse: \", x.sparse)\n",
    "x.dense[3] = 1\n",
    "print(\"after: \", x.dense, \"; or sparse: \", x.sparse, \" ==> changes aren't visible in sparse, because we haven't notified about them\")\n",
    "x.dense[3] = 1\n",
    "x.dense = x.dense\n",
    "print(\"after: \", x.dense, \"; or sparse: \", x.sparse, \" ==> set operation notifies about them, so now they're visible\")\n",
    "print(\"That's because magical properties (dense, sparse, coordinates) are just a cache and can be out of sync with the data they should represent\")\n",
    "print(\"Accessing another property invalidates cache! So you should always notify about changes before accessing another magical property\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating inputs to feed to the TM\n",
    "\n",
    "Each input is an SDR representing the active mini-columns.\n",
    "\n",
    "Here we create a simple sequence of 5 SDRs representing the sequence A -> B -> C -> D -> E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = { inp : SDR( tm.numberOfColumns() ) for inp in \"ABCDE\" }\n",
    "dataset['A'].dense[0:10]  = 1     # Input SDR representing \"A\", corresponding to mini-columns 0-9\n",
    "dataset['B'].dense[10:20] = 1     # Input SDR representing \"B\", corresponding to mini-columns 10-19\n",
    "dataset['C'].dense[20:30] = 1     # Input SDR representing \"C\", corresponding to mini-columns 20-29\n",
    "dataset['D'].dense[30:40] = 1     # Input SDR representing \"D\", corresponding to mini-columns 30-39\n",
    "dataset['E'].dense[40:50] = 1     # Input SDR representing \"E\", corresponding to mini-columns 40-49\n",
    "\n",
    "# Notify the SDR object that we've updated its dense data in-place.\n",
    "for z in dataset.values():\n",
    "    z.dense = z.dense\n",
    "\n",
    "for inp in \"ABCDE\":\n",
    "    print(\"Input:\", inp, \" Bits:\", formatBits( dataset[inp]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send this simple sequence to the temporal memory for learning.\n",
    "\n",
    "The compute method performs one step of learning and/or inference.\n",
    "\n",
    "__Note__: here we just perform learning but you can perform prediction/inference and learning in the same step if you want (online learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in \"ABCDE\": # Send each letter in the sequence in order\n",
    "    print(\"Input:\", inp)\n",
    "    activeColumns = dataset[inp]\n",
    "\n",
    "    print(\">>> tm.compute()\")\n",
    "    tm.compute(activeColumns, learn = True)\n",
    "\n",
    "    printStateTM(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset command\n",
    "\n",
    "The reset command tells the TM that a sequence just ended and essentially zeros out all the states. It is not strictly necessary but it's a bit messier without resets, and the TM learns quicker with resets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> tm.reset()\")\n",
    "tm.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resend the same sequence and look at predictions made by TM\n",
    "\n",
    "The following prints out the active cells, predictive cells, active segments and winner cells.\n",
    "\n",
    "What you should notice is that the mini-columns where active state is 1 represent the SDR for the current input pattern and the columns where predicted state is 1 represent the SDR for the next expected pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in \"ABCDE\":\n",
    "    print(\"Input:\", inp)\n",
    "    activeColumns = dataset[inp]\n",
    "\n",
    "    print(\">>> tm.compute()\")\n",
    "    tm.compute(activeColumns, learn = False)\n",
    "\n",
    "    printStateTM(tm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
