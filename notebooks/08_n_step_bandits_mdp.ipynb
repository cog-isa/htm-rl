{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-step bandits\n",
    "## $\\epsilon$-greedy and UCB1\n",
    "\n",
    "Here I adapt eps-greedy and ucb1 to a N-step bandits problem.\n",
    "\n",
    "Each step $t$ has $n_t$ available actions. And there are $S_T = \\prod_0^T n_t$ possible intermediate or final steps (or T-paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rngs(seed, n):\n",
    "    \"\"\"Makes n identical separate random generators\"\"\"\n",
    "    return [np.random.default_rng(seed) for i in range(n)]\n",
    "\n",
    "def simulate_rewards(rng, p_choice, p_opt_choice):\n",
    "    \"\"\"Simulates one trial for two bandits: chosen and optimal. Both are represented as probs to get reward +1.\"\"\"\n",
    "    x = rng.random()\n",
    "    r = 1. if x <= p_choice else 0.\n",
    "    r_opt = 1. if x <= p_opt_choice else 0.\n",
    "    return r, r_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def test_eps_greedy(n_trials, bandits, eps=.01):\n",
    "    rew_rng, eps_rng = make_rngs(seed, 2)\n",
    "    \n",
    "    # For each t, there're S_t = s_0 * s_1 *...*s_t paths. Shapes[t] - the shape of this t-dim array containing all possible t-paths.\n",
    "    # Each path tau=(a_0, a_1,...a_t) has its own stats - Rs[t][tau] and Ns[t][tau]\n",
    "    shapes = [bandits.shape[:i+1] for i in range(bandits.ndim)]\n",
    "    Rs = [np.zeros(shape) for shape in shapes]\n",
    "    Ns = [np.full(shape, 1e-5) for shape in shapes]\n",
    "    optimal = np.max(bandits)\n",
    "    \n",
    "    def select_simulate_backprop(t, idx):\n",
    "        # idx: t-1 path, so R and N are 1-dim vectors of size n_t (num of actions at t)\n",
    "        R, N = Rs[t][idx], Ns[t][idx]\n",
    "        n_actions = R.shape[0]\n",
    "        Q = R/N\n",
    "        action = eps_rng.choice(n_actions) if eps_rng.random() <= eps else np.argmax(Q)\n",
    "        idx = idx + (action,)\n",
    "        if t+1 < len(shapes):\n",
    "            r, r_opt = select_simulate_backprop(t+1, idx)\n",
    "        else:\n",
    "            r, r_opt = simulate_rewards(rew_rng, bandits[idx], optimal)\n",
    "        \n",
    "        R[action] += r\n",
    "        N[action] += 1\n",
    "        return r, r_opt\n",
    "\n",
    "    every_k = max(n_trials / 1000, 1)\n",
    "    cum_regret, regret = 0, []\n",
    "    \n",
    "    for trial in range(1, n_trials+1):\n",
    "        r, r_opt = select_simulate_backprop(0, ())\n",
    "        \n",
    "        cum_regret += r_opt - r\n",
    "        if trial % every_k == 0:\n",
    "            regret.append(cum_regret)        \n",
    "        \n",
    "    return regret, Rs[0], Ns[0]\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "# MDP states are declared from the end: 3 actions for t=0, 4 actions for t=1, 6 actions for t=2\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.35, .4, .44])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "\n",
    "n_trials = 100000\n",
    "for eps_perc in [5]:\n",
    "    eps = eps_perc / 100.\n",
    "    regret, R, N = test_eps_greedy(n_trials, bandits, eps)\n",
    "    print(R/N, N.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'$\\epsilon$={eps_perc}%')\n",
    "    \n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def test_ucb1(n_trials, bandits, eps=.01):\n",
    "    rew_rng, exp_rng = make_rngs(seed, 2)\n",
    "    \n",
    "    # For each t, there're S_t = s_0 * s_1 *...*s_t paths. Shapes[t] - the shape of this t-dim array containing all possible t-paths.\n",
    "    # Each path tau=(a_0, a_1,...a_t) has its own stats - Rs[t][tau] and Ns[t][tau]\n",
    "    shapes = [bandits.shape[:i+1] for i in range(bandits.ndim)]\n",
    "    Rs = [np.zeros(shape) for shape in shapes]\n",
    "    Ns = [np.full(shape, 1e-5) for shape in shapes]\n",
    "    optimal = np.max(bandits)\n",
    "    \n",
    "    def select_simulate_backprop(t, idx, trial):\n",
    "        # idx: t-1 path, so R and N are 1-dim vectors of size n_t (num of actions at t)\n",
    "        R, N = Rs[t][idx], Ns[t][idx]\n",
    "        Q = R/N\n",
    "        U = (2 * np.log(trial) / N)**.5\n",
    "        action = np.argmax(Q + U)\n",
    "        idx = idx + (action,)\n",
    "        if t+1 < len(shapes):\n",
    "            r, r_opt = select_simulate_backprop(t + 1, idx, trial)\n",
    "        else:\n",
    "            r, r_opt = simulate_rewards(rew_rng, bandits[idx], optimal)\n",
    "        \n",
    "        R[action] += r\n",
    "        N[action] += 1\n",
    "        return r, r_opt\n",
    "\n",
    "    every_k = max(n_trials / 1000, 1)\n",
    "    cum_regret, regret = 0, []\n",
    "    \n",
    "    for trial in range(1, n_trials):\n",
    "        r, r_opt = select_simulate_backprop(0, (), trial)\n",
    "        \n",
    "        cum_regret += r_opt - r\n",
    "        if trial % every_k == 0:\n",
    "            regret.append(cum_regret)        \n",
    "        \n",
    "    return regret, Rs[0], Ns[0]\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "# MDP states are declared from the end: 3 actions for t=0, 4 actions for t=1, 6 actions for t=2\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.35, .4, .44])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "\n",
    "n_trials = 100000\n",
    "regret, R, N = test_ucb1(n_trials, bandits)\n",
    "print(R/N, N.astype(np.int))\n",
    "_ = plt.plot(regret, label='ucb1')\n",
    "\n",
    "\n",
    "for eps_perc in [1, 5]:\n",
    "    eps = eps_perc / 100.\n",
    "    regret, R, N = test_eps_greedy(n_trials, bandits, eps)\n",
    "    print(R/N, N.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'$\\epsilon$={eps_perc}%')\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB1 on SDR cells\n",
    "\n",
    "Here I adapt MCTS with UCB1 policy to states represented as sets of cells. Each cell accumulate its own R and N statistics independently.\n",
    "\n",
    "UCB1 for a state is calculated as an aggregate of UCB1 values of its cells, i.e. each action cell produces its own UCB1 value, and then they're accumulated (for now - as average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cells(shape, n_cells, n_shared_cells):\n",
    "    # shape == t-path\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    idx = int(np.prod(shape[:-1]))\n",
    "    n_actions = shape[-1]\n",
    "    \n",
    "    # path is split into merged t-1 path and (n_t x n_cells). The last array of cells is split between actions at time t with n_shared_cells overlap\n",
    "    shape = (idx, ) + (n_actions * n_cells, )\n",
    "    R, N = np.zeros(shape), np.full(shape, 1e-5)\n",
    "\n",
    "    # list of a_t arrays by (n_cells + n_shared_cells) indices. Each \"row\" - action cells indices\n",
    "    action_cell_indices = []\n",
    "    for j in range(idx):\n",
    "        action_cell_indices.append([])\n",
    "        for i in range(n_actions):\n",
    "            base_indices = list(range(i*n_cells, (i+1)*n_cells))\n",
    "            shared_indices = [\n",
    "                x if x < i*n_cells else x + n_cells\n",
    "                for x in rng.choice((n_actions - 1) * n_cells, n_shared_cells, replace=False)\n",
    "            ]\n",
    "            action_cell_indices[j].append(sorted(base_indices + shared_indices))\n",
    "        action_cell_indices[j] = np.array(action_cell_indices[j])\n",
    "    return R, N, np.array(action_cell_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cells_ucb1(n_trials, bandits, n_cells, k_cells, n_shared_cells):\n",
    "    rew_rng, exp_rng, upd_rng = make_rngs(seed, 3)\n",
    "    \n",
    "    shapes = [bandits.shape[:i+1] for i in range(bandits.ndim)]\n",
    "    Rs, Ns, action_cell_indices = zip(*[init_cells(shape, n_cells, n_shared_cells) for shape in shapes])\n",
    "    optimal, every_k, cum_regret, regret = np.max(bandits), max(n_trials / 1000, 1), 0, []\n",
    "    \n",
    "    def select_simulate_backprop(t, flat_idx, idx, trial):\n",
    "        # idx: t-1 path; flat_idx: merged t-1 path\n",
    "        R, N, cell_indices = Rs[t][flat_idx], Ns[t][flat_idx], action_cell_indices[t][flat_idx]\n",
    "        Q = R/N\n",
    "        U = ((k_cells / n_cells) * 2 * np.log(trial) / N)**.5\n",
    "        \n",
    "        n_actions = len(cell_indices)\n",
    "        # V is a 2d arr, each row i has UCB1 values for a_i cells. Then they are averaged for each action.\n",
    "        V = (Q + U)[cell_indices]        \n",
    "        action = np.argmax(V.mean(axis=1))\n",
    "        \n",
    "        flat_idx, idx = flat_idx * n_actions + action, idx + (action, )\n",
    "        if t+1 < len(shapes):\n",
    "            r, r_opt = select_simulate_backprop(t + 1, flat_idx, idx, trial)\n",
    "        else:\n",
    "            r, r_opt = simulate_rewards(rew_rng, bandits[idx], optimal)\n",
    "        \n",
    "        # chooses k_cells from action cells to update\n",
    "        upd_indices = upd_rng.choice(cell_indices[action], size=k_cells, replace=False)\n",
    "        R[upd_indices] += r\n",
    "        N[upd_indices] += 1\n",
    "        return r, r_opt\n",
    "\n",
    "    for trial in range(1, n_trials):\n",
    "        r, r_opt = select_simulate_backprop(0, 0, (), trial)\n",
    "        \n",
    "        cum_regret += r_opt - r\n",
    "        if trial % every_k == 0:\n",
    "            regret.append(cum_regret) \n",
    "    \n",
    "    # returns Q_0: values for the first step, i.e. Q(s_0, a_0) for each a_0^i\n",
    "    Q_0 = Rs[0][0] / Ns[0][0]\n",
    "    return regret, Q_0[action_cell_indices[0][0]].mean(axis=1), Ns[0][0][action_cell_indices[0][0]].mean(axis=1)\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.35, .4, .44])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "print(bandits.shape)\n",
    "\n",
    "n_trials = 100000\n",
    "k = 4\n",
    "for n_shared_cells in [0, 2, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "n_shared_cells = 8\n",
    "for k in [2, 4, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "\n",
    "regret, R, N = test_ucb1(n_trials, bandits)\n",
    "print(R/N, N.astype(np.int))\n",
    "_ = plt.plot(regret, label='ucb1')\n",
    "\n",
    "\n",
    "for eps_perc in [1, 5]:\n",
    "    eps = eps_perc / 100.\n",
    "    regret, R, N = test_eps_greedy(n_trials, bandits, eps)\n",
    "    print(R/N, N.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'$\\epsilon$={eps_perc}%')\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that UCB1 on cells performs quite similar to the simple UCB1 and better than $\\epsilon$-greedy strategy even with overlapping action cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.35, .4, .44])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "print(bandits.shape)\n",
    "\n",
    "n_trials = 100000\n",
    "k = 4\n",
    "for n_shared_cells in [2, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "n_shared_cells = 8\n",
    "for k in [2, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "\n",
    "regret, R, N = test_ucb1(n_trials, bandits)\n",
    "print(R/N, N.astype(np.int))\n",
    "_ = plt.plot(regret, label='ucb1')\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is presented the similar experiment (without eps-greedy) to give a sence of how close to simple UCB1 is our approach.\n",
    "\n",
    "And below is the experiment with overlapping. At each trial we update the whole action cells set (i.e. k = n_cells + n_shared_cells). The question is how good is our approach against overlapping? It resilient to a small overlapping, but as it grows over some threshold it start making much difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.35, .4, .44])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "print(bandits.shape)\n",
    "\n",
    "n_trials = 100000\n",
    "k = 12\n",
    "for n_shared_cells in [0, 6, 12, 18]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, k, k + n_shared_cells, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "regret, R, N = test_ucb1(n_trials, bandits)\n",
    "print(R/N, N.astype(np.int))\n",
    "_ = plt.plot(regret, label='ucb1')\n",
    "\n",
    "\n",
    "# for eps_perc in [1, 5]:\n",
    "#     eps = eps_perc / 100.\n",
    "#     regret, R, N = test_eps_greedy(n_trials, bandits, eps)\n",
    "#     print(R/N, N.astype(np.int))\n",
    "#     _ = plt.plot(regret, label=f'$\\epsilon$={eps_perc}%')\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last experiment - is for 2-step MDP instead of 3-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "bandits = np.array([.1, .15, .4, .5, .65, .7])\n",
    "bandits = np.expand_dims(bandits, -1) * np.array([.3, .5, .8, .88])\n",
    "bandits = bandits.T if bandits.ndim > 1 else bandits\n",
    "print(bandits.reshape(bandits.shape[0], -1).max(axis=-1))\n",
    "print(bandits.reshape(bandits.shape[0], -1).mean(axis=-1))\n",
    "print(bandits.shape)\n",
    "\n",
    "n_trials = 100000\n",
    "k = 4\n",
    "for n_shared_cells in [2, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "n_shared_cells = 8\n",
    "for k in [2, 8, 16]:\n",
    "    regret, probs, cnts = test_cells_ucb1(n_trials, bandits, 12, k, n_shared_cells)\n",
    "    print(probs, cnts.astype(np.int))\n",
    "    _ = plt.plot(regret, label=f'{k}-{n_shared_cells}')\n",
    "\n",
    "\n",
    "regret, R, N = test_ucb1(n_trials, bandits)\n",
    "print(R/N, N.astype(np.int))\n",
    "_ = plt.plot(regret, label='ucb1')\n",
    "\n",
    "_ = plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
