{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Pooler autorecurrence test\n",
    "\n",
    "Here I test how SP react to reccurent input propagation.\n",
    "\n",
    "In this test I take one $NxN$ SP and, starting from random input, sequentially propagate it's output to an input in order to find out:\n",
    "\n",
    "- conditions, when it's prone to converge into static pattern or pattern oscillations\n",
    "- how to measure this property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minigrid as mg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler, Classifier\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "from htm_rl.htm_plugins.temporal_memory import TemporalMemory\n",
    "from htm_rl.common.int_sdr_encoder import IntSdrEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, Counter\n",
    "\n",
    "def count_similar(x, a):\n",
    "    return any(\n",
    "        1\n",
    "        for y in a\n",
    "        if len(x & y) >= sim_threshold\n",
    "    )\n",
    "\n",
    "def update_similar_score(sparse_sdr):\n",
    "    sparse_sdr = frozenset(sparse_sdr)\n",
    "    similar_cnt = count_similar(sparse_sdr, timeline)\n",
    "    \n",
    "    uniques[sparse_sdr] += 1\n",
    "    if len(timeline) > 0:\n",
    "        leftmost = timeline[0]\n",
    "        uniques[leftmost] -= 1\n",
    "        if uniques[leftmost] == 0:\n",
    "            del uniques[leftmost]\n",
    "    timeline.append(sparse_sdr)\n",
    "    uniques_hist.append(len(uniques))\n",
    "    \n",
    "    ma = similars_ma[-1]\n",
    "    if len(similars) == similars.maxlen:\n",
    "        ma -= similars.popleft()\n",
    "    ma += similar_cnt\n",
    "    similars.append(similar_cnt)\n",
    "    similars_ma.append(ma)\n",
    "\n",
    "n_ = 32\n",
    "n = n_**2\n",
    "act_threshold = 18\n",
    "sim_threshold = 17\n",
    "sparsity = act_threshold / n\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "sdr = SDR(n)\n",
    "sp = SpatialPooler(\n",
    "    inputDimensions=[n], columnDimensions=[n], globalInhibition=True, potentialPct=.4, localAreaDensity=sparsity,\n",
    "    stimulusThreshold=1, synPermConnected=.4, synPermActiveInc=.1, synPermInactiveDec=.01, boostStrength=4.,\n",
    "    dutyCyclePeriod=int(np.sqrt(act_threshold)/sparsity), minPctOverlapDutyCycle=sparsity/2,\n",
    "    seed=seed, \n",
    ")\n",
    "sdr.randomize(sparsity=sparsity, seed=seed)\n",
    "dense_sdr = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "initial_sdr = sdr\n",
    "sdr = SDR(n)\n",
    "sdr.sparse = initial_sdr.sparse\n",
    "\n",
    "t = n\n",
    "print(f'T={t}')\n",
    "\n",
    "sdr_stats = Metrics(sdr, t*10)\n",
    "hist_len = max(1000, t)\n",
    "timeline, similars, similars_ma, uniques, uniques_hist = deque([], t), deque([], t), deque([0], hist_len), Counter(), deque([], hist_len)\n",
    "for i in range(t*20):\n",
    "    sp.compute(sdr, learn=True, output=sdr)\n",
    "    \n",
    "for i in range(hist_len*3):\n",
    "    sp.compute(sdr, learn=True, output=sdr)\n",
    "    update_similar_score(sdr.sparse)\n",
    "    \n",
    "print(sdr_stats.activationFrequency)\n",
    "\n",
    "# Moving avg probability to see almost the same output for sliding window T. Almost the same == overlap score at least `sim_threshold`\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 9))\n",
    "\n",
    "ax1.set_title(f'Moving avg probability to see almost the same output (=overlap score {sim_threshold} out of {act_threshold}) for sliding window T={t}')\n",
    "xs = np.arange(len(similars_ma))\n",
    "ys = np.array(similars_ma) * 100 / t\n",
    "ax1.plot(xs, ys)\n",
    "ax1.set_ylim(-10, 110)\n",
    "print(f'Similar count mean, %: {ys.mean()}')\n",
    "\n",
    "# Ratio of exactly unique outputs for sliding window T\n",
    "ax2.set_title(f'Ratio of exactly unique outputs for sliding window T={t}')\n",
    "xs = np.arange(len(uniques_hist))\n",
    "ys = np.array(uniques_hist) * 100 / t\n",
    "ax2.set_ylim(-10, 110)\n",
    "_ = ax2.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_square(dense_sdr):\n",
    "    return dense_sdr.reshape((n_, n_))\n",
    "\n",
    "def plot_vec(sparse_sdr, ax):\n",
    "    dense_sdr[sparse_sdr] = 1\n",
    "    ax.imshow(to_square(dense_sdr))\n",
    "    ax.set_axis_off()\n",
    "    dense_sdr[sparse_sdr] = 0\n",
    "\n",
    "def plot_timeline(nrows=6, ncols=12, learn=False):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*2, nrows*2))\n",
    "\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            sp.compute(sdr, learn=learn, output=sdr)\n",
    "            plot_vec(sdr.sparse, ax=axes[row][col])\n",
    "\n",
    "n_ = 15\n",
    "n = n_**2\n",
    "act_threshold = 10\n",
    "sim_threshold = 8\n",
    "sparsity = act_threshold / n\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "sdr = SDR(n)\n",
    "sp = SpatialPooler(\n",
    "    inputDimensions=[n], columnDimensions=[n], globalInhibition=True, potentialPct=.4, localAreaDensity=sparsity,\n",
    "    stimulusThreshold=1, synPermConnected=.4, synPermActiveInc=.1, synPermInactiveDec=.01, boostStrength=2.,\n",
    "    dutyCyclePeriod=int(np.sqrt(act_threshold)/sparsity), minPctOverlapDutyCycle=sparsity/5,\n",
    "    seed=seed, \n",
    ")\n",
    "sdr.randomize(sparsity=sparsity, seed=seed)\n",
    "dense_sdr = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "initial_sdr = sdr\n",
    "sdr = SDR(n)\n",
    "sdr.sparse = initial_sdr.sparse\n",
    "\n",
    "t = n\n",
    "print(f'T={t}')\n",
    "\n",
    "sdr_stats = Metrics(sdr, t*10)\n",
    "hist_len = max(1000, t)\n",
    "timeline, similars, similars_ma, uniques, uniques_hist = deque([], t), deque([], t), deque([0], hist_len), Counter(), deque([], hist_len)\n",
    "for i in range(t*20):\n",
    "    sp.compute(sdr, learn=True, output=sdr)\n",
    "#     sdr.addNoise(1.2 / act_threshold)\n",
    "    \n",
    "for i in range(hist_len*3):\n",
    "    sp.compute(sdr, learn=True, output=sdr)\n",
    "    update_similar_score(sdr.sparse)\n",
    "    \n",
    "print(sdr_stats.activationFrequency)\n",
    "\n",
    "# Moving avg probability to see almost the same output for sliding window T. Almost the same == overlap score at least `sim_threshold`\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 9))\n",
    "\n",
    "ax1.set_title(f'Moving avg probability to see almost the same output (=overlap score {sim_threshold} out of {act_threshold}) for sliding window T={t}')\n",
    "xs = np.arange(len(similars_ma))\n",
    "ys = np.array(similars_ma) * 100 / t\n",
    "ax1.plot(xs, ys)\n",
    "ax1.set_ylim(-10, 110)\n",
    "print(f'Similar count mean, %: {ys.mean()}')\n",
    "\n",
    "# Ratio of exactly unique outputs for sliding window T\n",
    "ax2.set_title(f'Ratio of exactly unique outputs for sliding window T={t}')\n",
    "xs = np.arange(len(uniques_hist))\n",
    "ys = np.array(uniques_hist) * 100 / t\n",
    "ax2.set_ylim(-10, 110)\n",
    "_ = ax2.plot(xs, ys)\n",
    "\n",
    "plot_timeline(nrows=4, ncols=10, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "- sparsity\n",
    "  - important, but I didn't get relation\n",
    "  - less is better (KO)\n",
    "\n",
    "\n",
    "- `potentialPct`\n",
    "  - sparsity among potential synapses intialization\n",
    "  - very important, less is better until X, also faster\n",
    "  - for this particular case\n",
    "    - X loosely depends on stimulusThreshold\n",
    "    - I think 0.2-0.4 is a very promising range\n",
    "  - but before I used 0.8 and everything was ok too\n",
    "\n",
    "\n",
    "- `synPermActiveInc`/`synPermActiveDec`\n",
    "  - delta for learning\n",
    "  - high decrement can break things\n",
    " \n",
    "\n",
    "- `boostStrength`\n",
    "  - important only if unstable setup\n",
    "  - otherwise 2-4 is ok,.. even 1, which is \"no boost\"\n",
    "  \n",
    "\n",
    "- `dutyCyclePeriod`, `minPctOverlapDutyCycle`\n",
    "  - `dutyCyclePeriod` affects max delay for entropy bumping, more important\n",
    "  - `minPctOverlapDutyCycle` affects how many cols are affected, less important\n",
    "  - equation for the 1st looks creepy, but it should be read like this:\n",
    "    - how frequent we want re-check entropy?\n",
    "    - ideally every col fires every $1 / sparsity)$ steps\n",
    "    - e.g. 400 cells, 2% sparsity (=8 cells), then with perfectly random output all cells will be fired after 400/8 = 50 steps = 1 / (2%)\n",
    "    - but in reality input is not uniformly random ==> output is not perfectly random\n",
    "    - so it's highly likely that after 50 steps there're a lot of cells that aren't fired, i.e. statistics is probably skewed\n",
    "    - that's why it's wise to increase this period x2-x10. Sqrt(activation_threshold) is a balanced value inbetween.\n",
    "\n",
    "\n",
    "- actual parameters used in this notebook __are good only for this special case__\n",
    "  - they can be used as starting point in real cases\n",
    "  - these params are optimised for forcing SP to raise entropy to the max\n",
    "  - I haven't measured how it's achieved\n",
    "    - either by breaking static patterns and short oscillations, i.e. SP learns a function with high order of periodicity\n",
    "    - or by continually changing synapses permanence bumping low-firing cols\n",
    "  - but I'm almost sure that the effect of the latter is not neglible\n",
    "  - so in reality you should adapt parameters in order to reduce high-entropy pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
