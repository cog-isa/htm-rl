{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minigrid as mg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# fetch datasets from www.openML.org/ \n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler, Classifier\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "\n",
    "def load_ds(name, num_test, shape=None):\n",
    "    \"\"\" \n",
    "    fetch dataset from openML.org and split to train/test\n",
    "    @param name - ID on openML (eg. 'mnist_784')\n",
    "    @param num_test - num. samples to take as test\n",
    "    @param shape - new reshape of a single data point (ie data['data'][0]) as a list. Eg. [28,28] for MNIST\n",
    "    \"\"\"\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz=data['target'].shape[0]\n",
    "\n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape)\n",
    "\n",
    "    y = data['target'].astype(np.int32)\n",
    "    # split to train/test data\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "def encode(data, out):\n",
    "    \"\"\"\n",
    "    encode the (image) data\n",
    "    @param data - raw data\n",
    "    @param out  - return SDR with encoded data\n",
    "    \"\"\"\n",
    "    out.dense = data >= np.mean(data) # convert greyscale image to binary B/W.\n",
    "    #TODO improve. have a look in htm.vision etc. For MNIST this is ok, for fashionMNIST in already loses too much information\n",
    "\n",
    "\n",
    "# These parameters can be improved using parameter optimization,\n",
    "# see py/htm/optimization/ae.py\n",
    "# For more explanation of relations between the parameters, see \n",
    "# src/examples/mnist/MNIST_CPP.cpp \n",
    "default_parameters = {\n",
    "    'potentialRadius': 7,\n",
    "    'boostStrength': 7.0,\n",
    "    'columnDimensions': (79, 79),\n",
    "    'dutyCyclePeriod': 1402,\n",
    "    'localAreaDensity': 0.1,\n",
    "    'minPctOverlapDutyCycle': 0.2,\n",
    "    'potentialPct': 0.1,\n",
    "    'stimulusThreshold': 6,\n",
    "    'synPermActiveInc': 0.14,\n",
    "    'synPermConnected': 0.5,\n",
    "    'synPermInactiveDec': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', 10000, shape=[28,28]) # HTM: ~95.6%\n",
    "# train_labels, train_images, test_labels, test_images = load_ds('Fashion-MNIST', 10000, shape=[28,28]) # HTM baseline: ~83%\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "train_images, train_labels = shuffle_data(train_images, train_labels)\n",
    "test_images, test_labels = shuffle_data(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bare HTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def run_bare_classifier(parameters=default_parameters, argv=None, verbose=True):\n",
    "    training_data = list(zip(train_images, train_labels))\n",
    "    test_data     = list(zip(test_images, test_labels))\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    # Setup the AI.\n",
    "    enc = SDR(train_images[0].shape)\n",
    "    columns_stats = Metrics( enc, 99999999 )\n",
    "    sdrc = Classifier()\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(len(train_images)):\n",
    "        img, lbl = training_data[i]\n",
    "        encode(img, enc)\n",
    "        sdrc.learn( enc, lbl )\n",
    "\n",
    "    print(str(columns_stats))\n",
    "\n",
    "    # Testing Loop\n",
    "    score = 0\n",
    "    for img, lbl in test_data:\n",
    "        encode(img, enc)\n",
    "        if lbl == np.argmax( sdrc.infer( enc ) ):\n",
    "            score += 1\n",
    "    score = score / len(test_data)\n",
    "\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "run_bare_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTM SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def run_htm_sp(parameters=default_parameters, argv=None, verbose=True):\n",
    "    training_data = list(zip(train_images, train_labels))\n",
    "    test_data     = list(zip(test_images, test_labels))\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    # Setup the AI.\n",
    "    enc = SDR(train_images[0].shape)\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions            = enc.dimensions,\n",
    "        columnDimensions           = parameters['columnDimensions'],\n",
    "        potentialRadius            = parameters['potentialRadius'],\n",
    "        potentialPct               = parameters['potentialPct'],\n",
    "        globalInhibition           = True,\n",
    "        localAreaDensity           = parameters['localAreaDensity'],\n",
    "        stimulusThreshold          = int(round(parameters['stimulusThreshold'])),\n",
    "        synPermInactiveDec         = parameters['synPermInactiveDec'],\n",
    "        synPermActiveInc           = parameters['synPermActiveInc'],\n",
    "        synPermConnected           = parameters['synPermConnected'],\n",
    "        minPctOverlapDutyCycle     = parameters['minPctOverlapDutyCycle'],\n",
    "        dutyCyclePeriod            = int(round(parameters['dutyCyclePeriod'])),\n",
    "        boostStrength              = parameters['boostStrength'],\n",
    "        seed                       = 0, # this is important, 0=\"random\" seed which changes on each invocation\n",
    "        spVerbosity                = 99,\n",
    "        wrapAround                 = False)\n",
    "    columns = SDR( sp.getColumnDimensions() )\n",
    "    columns_stats = Metrics( columns, 99999999 )\n",
    "    sdrc = Classifier()\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(len(train_images)):\n",
    "        img, lbl = training_data[i]\n",
    "        encode(img, enc)\n",
    "        sp.compute( enc, True, columns )\n",
    "        sdrc.learn( columns, lbl )\n",
    "\n",
    "    print(str(sp))\n",
    "    print(str(columns_stats))\n",
    "\n",
    "    # Testing Loop\n",
    "    score = 0\n",
    "    for img, lbl in test_data:\n",
    "        encode(img, enc)\n",
    "        sp.compute( enc, False, columns )\n",
    "        if lbl == np.argmax( sdrc.infer( columns ) ):\n",
    "            score += 1\n",
    "    score = score / len(test_data)\n",
    "\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "run_htm_sp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bare Sklearn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def encode_to_csr(x, enc: SDR):    \n",
    "    encoded_images_flatten = []\n",
    "    indptr = [0]\n",
    "    for img in x:\n",
    "        encode(img, enc)\n",
    "        encoded_images_flatten.extend(enc.sparse)\n",
    "        indptr.append(len(encoded_images_flatten))\n",
    "    \n",
    "    data = np.ones(len(encoded_images_flatten))\n",
    "    csr = csr_matrix((data, encoded_images_flatten, indptr), shape=(x.shape[0], enc.size))\n",
    "    return csr\n",
    "\n",
    "def run_bare_sklearn_classifier(x_tr,  y_tr, x_tst, y_tst, parameters=default_parameters):    \n",
    "    enc = SDR(x_tr[0].shape)\n",
    "    csr = encode_to_csr(x_tr, enc)\n",
    "    \n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    csr = encode_to_csr(x_tst, enc)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "run_bare_sklearn_classifier(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTM SP + Sklearn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def encode_to_csr_sp(x, enc: SDR, sp, columns, learn):            \n",
    "    encoded_images_flatten = []\n",
    "    indptr = [0]\n",
    "    for img in x:\n",
    "        encode(img, enc)\n",
    "        sp.compute( enc, learn, columns )\n",
    "        encoded_images_flatten.extend(columns.sparse)\n",
    "        indptr.append(len(encoded_images_flatten))\n",
    "\n",
    "    \n",
    "    data = np.ones(len(encoded_images_flatten))\n",
    "    csr = csr_matrix((data, encoded_images_flatten, indptr), shape=(x.shape[0], columns.size))\n",
    "    return csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def run_htm_sp_sklearn(x_tr,  y_tr, x_tst, y_tst, parameters=default_parameters):\n",
    "    enc = SDR(train_images[0].shape)\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions            = enc.dimensions,\n",
    "        columnDimensions           = parameters['columnDimensions'],\n",
    "        potentialRadius            = parameters['potentialRadius'],\n",
    "        potentialPct               = parameters['potentialPct'],\n",
    "        globalInhibition           = True,\n",
    "        localAreaDensity           = parameters['localAreaDensity'],\n",
    "        stimulusThreshold          = int(round(parameters['stimulusThreshold'])),\n",
    "        synPermInactiveDec         = parameters['synPermInactiveDec'],\n",
    "        synPermActiveInc           = parameters['synPermActiveInc'],\n",
    "        synPermConnected           = parameters['synPermConnected'],\n",
    "        minPctOverlapDutyCycle     = parameters['minPctOverlapDutyCycle'],\n",
    "        dutyCyclePeriod            = int(round(parameters['dutyCyclePeriod'])),\n",
    "        boostStrength              = parameters['boostStrength'],\n",
    "        seed                       = 0, # this is important, 0=\"random\" seed which changes on each invocation\n",
    "        spVerbosity                = 0,\n",
    "        wrapAround                 = False)\n",
    "    columns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "    # train SP\n",
    "    for img in x_tr[:1000]:\n",
    "        encode(img, enc)\n",
    "        sp.compute( enc, True, columns )\n",
    "    \n",
    "    # train linreg\n",
    "    csr = encode_to_csr_sp(x_tr, enc, sp, columns, True)\n",
    "    \n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    csr = encode_to_csr_sp(x_tst, enc, sp, columns, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "run_htm_sp_sklearn(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTM SP + Sklearn classifier small size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def run_htm_sp_sklearn_small(x_tr,  y_tr, x_tst, y_tst, output_dim, parameters=default_parameters):\n",
    "    # Get training data\n",
    "    enc = SDR(train_images[0].shape)\n",
    "    sp = SpatialPooler(\n",
    "        inputDimensions            = enc.dimensions,\n",
    "        columnDimensions           = output_dim,\n",
    "        potentialRadius            = parameters['potentialRadius'],\n",
    "        potentialPct               = parameters['potentialPct'],\n",
    "        globalInhibition           = True,\n",
    "        localAreaDensity           = parameters['localAreaDensity'],\n",
    "        stimulusThreshold          = int(round(parameters['stimulusThreshold'])),\n",
    "        synPermInactiveDec         = parameters['synPermInactiveDec'],\n",
    "        synPermActiveInc           = parameters['synPermActiveInc'],\n",
    "        synPermConnected           = parameters['synPermConnected'],\n",
    "        minPctOverlapDutyCycle     = parameters['minPctOverlapDutyCycle'],\n",
    "        dutyCyclePeriod            = int(round(parameters['dutyCyclePeriod'])),\n",
    "        boostStrength              = parameters['boostStrength'],\n",
    "        seed                       = 0, # this is important, 0=\"random\" seed which changes on each invocation\n",
    "        spVerbosity                = 0,\n",
    "        wrapAround                 = False)\n",
    "    columns = SDR( sp.getColumnDimensions() )\n",
    "\n",
    "    # train SP\n",
    "    for img in x_tr[:1000]:\n",
    "        encode(img, enc)\n",
    "        sp.compute( enc, True, columns )\n",
    "    \n",
    "    # train linreg\n",
    "    csr = encode_to_csr_sp(x_tr, enc, sp, columns, True)\n",
    "    \n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    csr = encode_to_csr_sp(x_tst, enc, sp, columns, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "d = 50\n",
    "run_htm_sp_sklearn_small(x_tr, y_tr, x_tst, y_tst, (d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dims = list(range(20, 50, 5))\n",
    "scores = [run_htm_sp_sklearn_small(x_tr, y_tr, x_tst, y_tst, (d, d)) for d in output_dims]\n",
    "\n",
    "plt.plot(output_dims, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My SP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = {\n",
    "    'potentialRadius': 7,\n",
    "    'boostStrength': 7.0,\n",
    "    'columnDimensions': (79, 79),\n",
    "    'dutyCyclePeriod': 1402,\n",
    "    'localAreaDensity': 0.1,\n",
    "    'minPctOverlapDutyCycle': 0.2,\n",
    "    'potentialPct': 0.1,\n",
    "    'stimulusThreshold': 6,\n",
    "    'synPermActiveInc': 0.14,\n",
    "    'synPermConnected': 0.5,\n",
    "    'synPermInactiveDec': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img(img):\n",
    "    return (img >= img.mean()).astype(np.int8)\n",
    "\n",
    "sample = train_images[0]\n",
    "sample = encode_img(sample)\n",
    "\n",
    "class MySpatialPooler:\n",
    "    def __init__(self, input_shape, output_shape, permanence_threshold, sparsity_level, syn_perm_deltas, min_activation_threshold=1, max_boost_factor=1.5, boost_sliding_window=(1000, 1000)):\n",
    "        assert isinstance(input_shape, tuple) and isinstance(output_shape, tuple)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.joint_shape = output_shape + input_shape\n",
    "        self.output_size = output_shape[0] * output_shape[1]\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = syn_perm_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle, self.overlap_duty_cycle = boost_sliding_window\n",
    "        \n",
    "        # init \n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[.2, .8])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        self.time_avg_activity = np.full(self.output_shape, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_overlap = np.ones(self.output_shape, dtype=np.float)\n",
    "        self.dp = np.empty(input_shape, dtype=np.float)\n",
    "        self._compute_boost()\n",
    "        \n",
    "    def compute(self, x, learn):\n",
    "        x = x.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, :, x] >= self.permanence_threshold\n",
    "        overlaps = (np.count_nonzero(active_cells, -1) * self.boost).ravel()\n",
    "        activated_indices = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_indices = activated_indices[overlaps[activated_indices] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            rows, cols = np.unravel_index(activated_indices, self.output_shape)\n",
    "            self._update_permanence(x, rows, cols)\n",
    "            self._update_activity_boost(rows, cols)\n",
    "#             self._update_overlap_boost(x, rows, cols, overlaps)\n",
    "\n",
    "        return activated_indices\n",
    "    \n",
    "    def _update_permanence(self, x, rows, cols):\n",
    "        dp = self.dp\n",
    "        dp[x] = self.syn_perm_inc\n",
    "        dp[~x] = -self.syn_perm_dec\n",
    "        perm = self.connections_permanence[rows, cols]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[rows, cols], 0, 1)\n",
    "        \n",
    "    def _update_activity_boost(self, rows, cols):\n",
    "        self.time_avg_activity *= (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        self.time_avg_activity[rows, cols] += 1 / self.activity_duty_cycle\n",
    "        self._compute_boost()\n",
    "        \n",
    "    def _update_overlap_boost(self, x, rows, cols, overlaps):\n",
    "        self.time_avg_overlap += (overlaps.reshape(self.output_shape) - self.time_avg_overlap) / self.overlap_duty_cycle\n",
    "        k = int(.05 * self.output_size)\n",
    "        to_boost_indices = np.argpartition(self.time_avg_overlap.ravel(), k)[:k]\n",
    "        to_boost_indices = np.unravel_index(to_boost_indices, self.output_shape)\n",
    "        to_boost = self.connections_permanence[to_boost_indices]\n",
    "        to_boost = np.clip(to_boost + .1 * self.permanence_threshold, 0, 1)\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        self.boost = np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity.mean()))\n",
    "        \n",
    "\n",
    "np.random.seed(1337)\n",
    "my_sp = MySpatialPooler(train_images[0].shape, (10, 10), .5, .04, (.1, .02), 4)\n",
    "my_sp.compute(sample, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def encode_to_csr_my_sp(x, sp, learn=False):            \n",
    "    encoded_images_flatten = []\n",
    "    indptr = [0]\n",
    "    for img in x:\n",
    "        img = encode_img(img)\n",
    "        encoded_images_flatten.extend(sp.compute(img, learn))\n",
    "        indptr.append(len(encoded_images_flatten))\n",
    "\n",
    "    \n",
    "    data = np.ones(len(encoded_images_flatten))\n",
    "    csr = csr_matrix((data, encoded_images_flatten, indptr), shape=(x.shape[0], sp.output_size))\n",
    "    return csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def run_my_sp_sklearn_small(x_tr,  y_tr, x_tst, y_tst, sp):\n",
    "    enc = SDR(sp.input_shape)\n",
    "    columns = SDR(sp.output_shape)\n",
    "\n",
    "    # train SP\n",
    "    for img in x_tr[:1000]:\n",
    "        img = encode_img(img)\n",
    "        sp.compute(img, True)\n",
    "    \n",
    "    # train linreg\n",
    "    csr = encode_to_csr_my_sp(x_tr, sp, True)\n",
    "    \n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    csr = encode_to_csr_my_sp(x_tst, sp, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "my_sp = MySpatialPooler(train_images[0].shape, (50, 50), .5, .04, (.1, .02), 4, max_boost_factor=5)\n",
    "run_my_sp_sklearn_small(x_tr, y_tr, x_tst, y_tst, my_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
