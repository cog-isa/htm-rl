{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Следующая ячейка загружает датасет MNIST (займет порядка 10-20 сек)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name, num_test, shape=None):\n",
    "    \"\"\" \n",
    "    fetch dataset from openML.org and split to train/test\n",
    "    @param name - ID on openML (eg. 'mnist_784')\n",
    "    @param num_test - num. samples to take as test\n",
    "    @param shape - new reshape of a single data point (ie data['data'][0]) as a list. Eg. [28,28] for MNIST\n",
    "    \"\"\"\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz=data['target'].shape[0]\n",
    "\n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape)\n",
    "\n",
    "    y = data['target'].astype(np.int32)\n",
    "    # split to train/test data\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "\n",
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', 10000, shape=[28,28])\n",
    "\n",
    "np.random.seed(seed)\n",
    "train_images, train_labels = shuffle_data(train_images, train_labels)\n",
    "test_images, test_labels = shuffle_data(test_images, test_labels)\n",
    "\n",
    "n_train_samples = train_images.shape[0]\n",
    "n_test_samples = test_images.shape[0]\n",
    "image_shape = train_images[0].shape\n",
    "image_side = image_shape[0]\n",
    "image_size = image_side ** 2\n",
    "\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример формата данных датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])\n",
    "print(f'Label: {train_labels[0]}')\n",
    "print(f'Image shape: {image_shape}')\n",
    "print(f'Image middle row: {train_images[0][image_side//2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем датасет в бинарные изображения и дальше будем работать с бинарными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatten_image(flatten_image, image_height=28):\n",
    "    plt.imshow(flatten_image.reshape((image_height, -1)))\n",
    "\n",
    "def to_binary_flatten_images(images):\n",
    "    n_samples = images.shape[0]\n",
    "    # flatten every image to vector\n",
    "    images = images.reshape((n_samples, -1))\n",
    "    # binary encoding: each image pixel is encoded either 0 or 1 depending on that image mean value\n",
    "    images = (images >= images.mean(axis=1, keepdims=True)).astype(np.int8)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = to_binary_flatten_images(train_images)\n",
    "test_images = to_binary_flatten_images(test_images)\n",
    "plot_flatten_image(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Baseline: classifier on raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(x_tst) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "# 87.3; 888ms\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Spatial Pooler: skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoOpSpatialPooler:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        return np.nonzero(dense_sdr)[0]\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "sp = NoOpSpatialPooler(train_images[0].size)\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size)\n",
    "assert sparse_sdr.size < sp.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train/test SP performance aux pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def pretrain_sp(sp, images, n_samples):\n",
    "    for img in images[:n_samples]:\n",
    "        sp.compute(img, True)\n",
    "    \n",
    "def encode_to_csr_with_sp(images, sp, learn):\n",
    "    flatten_encoded_sdrs = []\n",
    "    indptr = [0]\n",
    "    for img in images:\n",
    "        encoded_sparse_sdr = sp.compute(img, learn)\n",
    "        flatten_encoded_sdrs.extend(encoded_sparse_sdr)\n",
    "        indptr.append(len(flatten_encoded_sdrs))\n",
    "\n",
    "    data = np.ones(len(flatten_encoded_sdrs))\n",
    "    csr = csr_matrix((data, flatten_encoded_sdrs, indptr), shape=(images.shape[0], sp.output_size))\n",
    "    return csr\n",
    "\n",
    "def test_classification_with_sp(x_tr,  y_tr, x_tst, y_tst, sp):\n",
    "    # a small pretrain SP before real work\n",
    "    pretrain_sp(sp, x_tr, n_samples=1000)\n",
    "    \n",
    "    # encode images and continuously train SP\n",
    "    csr = encode_to_csr_with_sp(x_tr, sp, learn=True)\n",
    "    \n",
    "    # train linreg\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    # encode test images (without SP learning) and then test score\n",
    "    csr = encode_to_csr_with_sp(x_tst, sp, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "my_sp = NoOpSpatialPooler(train_images[0].size)\n",
    "\n",
    "# 87.3; 1.16s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, my_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "sp = NoOpSpatialPooler(train_images[0].size)\n",
    "\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Spatial Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size,\n",
    "        permanence_threshold, sparsity_level, synapse_permanence_deltas, min_activation_threshold=1, potential_synapses_p=.8,\n",
    "        max_boost_factor=1.5, boost_sliding_window=(1000, 1000)\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle, self.overlap_duty_cycle = boost_sliding_window\n",
    "        \n",
    "        # init \n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[1-potential_synapses_p, potential_synapses_p])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        self.time_avg_activity = np.full(self.output_size, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_overlap = np.ones(self.output_size, dtype=np.float)\n",
    "        self.dp = np.empty(input_size, dtype=np.float)\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        dense_sdr = dense_sdr.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, dense_sdr] >= self.permanence_threshold\n",
    "        overlaps = np.count_nonzero(active_cells, -1) * self.boost\n",
    "        \n",
    "        activated_cols = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_cols = activated_cols[overlaps[activated_cols] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            self._update_permanence(dense_sdr, activated_cols)\n",
    "            self._update_activity_boost(activated_cols)\n",
    "#             self._update_overlap_boost(dense_sdr, activated_cols, overlaps)\n",
    "\n",
    "        return activated_cols\n",
    "    \n",
    "    def _update_permanence(self, dense_sdr, activated_cols):\n",
    "        dp = self.dp\n",
    "        dp[dense_sdr] = self.syn_perm_inc\n",
    "        dp[~dense_sdr] = -self.syn_perm_dec\n",
    "        perm = self.connections_permanence[activated_cols]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[activated_cols], 0, 1)\n",
    "        \n",
    "    def _update_activity_boost(self, activated_cols):\n",
    "        self.time_avg_activity *= (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        self.time_avg_activity[activated_cols] += 1 / self.activity_duty_cycle\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _update_overlap_boost(self, x, rows, cols, overlaps):\n",
    "        self.time_avg_overlap += (overlaps - self.time_avg_overlap) / self.overlap_duty_cycle\n",
    "        k = int(.05 * self.output_size)\n",
    "        to_boost_indices = np.argpartition(self.time_avg_overlap, k)[:k]\n",
    "        to_boost = self.connections_permanence[to_boost_indices]\n",
    "        to_boost = np.clip(to_boost + .1 * self.permanence_threshold, 0, 1)\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity.mean()))\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "my_sp = DenseSpatialPooler(train_images[0].size, 10**2, .5, .04, (.1, .02), 4, potential_synapses_p=.8)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = DenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = DenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = DenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense SP Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape((3, 4))\n",
    "np.minimum(a[[1, 2]], 6, out=a[[1,2]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDenseSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, sparsity_level=.04,\n",
    "        permanence_threshold=.5, min_activation_threshold=1, synapse_permanence_deltas=(.1, .02), potential_synapses_p=.8,\n",
    "        max_boost_factor=1.5, activity_duty_cycle=1000, overlap_low_bound_pct=.25, overlap_duty_cycle=1000\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle = activity_duty_cycle\n",
    "        self.boost_ma_decay = (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        \n",
    "        self.overlap_low_bound_pct = overlap_low_bound_pct\n",
    "        self.overlap_duty_cycle = overlap_duty_cycle\n",
    "        self.overlap_duty_cycle_period = max(100, int(np.sqrt(overlap_duty_cycle)))\n",
    "        self.overlap_ma_decay = (self.overlap_duty_cycle - 1) / self.overlap_duty_cycle\n",
    "        self.timestamp = 0\n",
    "        \n",
    "        # init \n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[1-potential_synapses_p, potential_synapses_p]).astype(np.bool)\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        self.connections_activity = self.connections_permanence.T >= self.permanence_threshold\n",
    "        \n",
    "        self.time_avg_activity = np.full(self.output_size, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_activity_mean = self.time_avg_activity.mean()\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "        self.time_avg_overlap = np.zeros(self.output_size, dtype=np.float)\n",
    "        self.time_avg_overlap_mean = self.time_avg_overlap.mean()\n",
    "        \n",
    "        # cache\n",
    "        self.dp = np.full(input_size, -self.syn_perm_dec, dtype=np.float)\n",
    "        self.dps = np.full((self.n_active_bits, input_size), -self.syn_perm_dec, dtype=np.float)\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        sparse_sdr = np.flatnonzero(dense_sdr)\n",
    "        active_cells = self.connections_activity[sparse_sdr]\n",
    "        overlaps = np.count_nonzero(active_cells, 0) * self.boost\n",
    "        \n",
    "        activated_cols = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_cols = activated_cols[overlaps[activated_cols] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            self.timestamp += 1\n",
    "            self._update_permanence(sparse_sdr, activated_cols)\n",
    "            self._update_activity_boost(activated_cols)\n",
    "            self._update_overlap_boost(activated_cols, overlaps)\n",
    "\n",
    "        return activated_cols\n",
    "    \n",
    "    def _update_permanence(self, sparse_sdr, activated_cols):\n",
    "        self.dp[sparse_sdr] = self.syn_perm_inc\n",
    "        \n",
    "        dps = self.dps\n",
    "        dps[:] = self.connections_permanence[activated_cols]\n",
    "        dps[:, sparse_sdr] = np.maximum(dps[:, sparse_sdr], 0.)\n",
    "        dps += self.dp * self.receptive_fields[activated_cols]\n",
    "        dps[:, sparse_sdr] = np.minimum(dps[:, sparse_sdr], 1.)\n",
    "        \n",
    "        self.connections_permanence[activated_cols] = dps\n",
    "        \n",
    "        self.dp[sparse_sdr] = -self.syn_perm_dec\n",
    "        self.connections_activity[:, activated_cols] = dps.T >= self.permanence_threshold\n",
    "        \n",
    "    def _update_activity_boost(self, activated_cols):\n",
    "        decay = self.boost_ma_decay\n",
    "        \n",
    "        self.time_avg_activity *= decay\n",
    "        self.time_avg_activity[activated_cols] += 1. - decay\n",
    "        self.time_avg_activity_mean = decay * self.time_avg_activity_mean + (1 - decay) * activated_cols.size / self.output_size\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _update_overlap_boost(self, activated_cols, overlaps):\n",
    "        decay = self.overlap_ma_decay\n",
    "        self.time_avg_overlap = decay * self.time_avg_overlap + (1 - decay) * overlaps\n",
    "        self.time_avg_overlap_mean = decay * self.time_avg_overlap_mean + (1 - decay) * overlaps.mean()\n",
    "        \n",
    "        if self.timestamp % self.overlap_duty_cycle_period == 0:\n",
    "            boosting_mask = self.time_avg_overlap < self.overlap_low_bound_pct * self.time_avg_overlap_mean\n",
    "            boosting_indices = np.flatnonzero(boosting_mask)\n",
    "            \n",
    "            if boosting_indices.any():\n",
    "                print('+')\n",
    "                np.minimum(\n",
    "                    self.connections_permanence[boosting_indices] + self.syn_perm_inc * self.receptive_fields[boosting_indices],\n",
    "                    1,\n",
    "                    out=self.connections_permanence[boosting_indices]\n",
    "                )\n",
    "                self.connections_activity[:, boosting_indices] = self.connections_permanence[boosting_indices].T >= self.permanence_threshold\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity_mean))\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "my_sp = OptimizedDenseSpatialPooler(train_images[0].size, 10**2)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=2,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.5\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=2,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.5\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=2,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.5\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=2,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.5\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=2,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.5\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=65**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.15\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedDenseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=65**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.15\n",
    ")\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Spatial Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class SparseSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, permanence_threshold=.5, sparsity_level=.04, potenrial_synapses_p=.8,\n",
    "        synapse_permanence_deltas=(.1, .02), min_activation_threshold=1,\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        self.synapse_permanence_deltas = np.array(synapse_permanence_deltas)\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        forward_connections = [\n",
    "            self._make_random_connections(self.output_size, p=potenrial_synapses_p)\n",
    "            for input_bit in range(self.input_size)\n",
    "        ]\n",
    "        self.backward_connections = self._to_backward_connections(self.output_size, forward_connections)\n",
    "        self.active_forward_connections = [\n",
    "            connections[permanences >= self.permanence_threshold]\n",
    "            for connections, permanences in forward_connections\n",
    "        ]\n",
    "        \n",
    "        self.overlaps = np.zeros(self.output_size, dtype=np.int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_random_connections(size, p):\n",
    "        connections_mask = np.random.binomial(1, p, size=size)\n",
    "        connections = np.flatnonzero(connections_mask)\n",
    "        permanences = np.random.uniform(size=connections.size)\n",
    "\n",
    "        return connections, permanences\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_backward_connections(size, forward_connections):\n",
    "        presynaptic_connections = [[] for _ in range(size)]\n",
    "        presynaptic_permanences = [[] for _ in range(size)]\n",
    "        \n",
    "        for input_bit, (connections, permanences) in enumerate(forward_connections):\n",
    "            for output_bit, permanence in zip(connections, permanences):\n",
    "                presynaptic_connections[output_bit].append(input_bit)\n",
    "                presynaptic_permanences[output_bit].append(permanence)\n",
    "                \n",
    "        return [\n",
    "            (np.array(connections), np.array(permanences))\n",
    "            for connections, permanences in zip(presynaptic_connections, presynaptic_permanences)\n",
    "        ]\n",
    "    \n",
    "    def _get_active_columns(self, sparse_sdr):\n",
    "        self.overlaps[:] = 0\n",
    "        for input_bit in sparse_sdr:\n",
    "            self.overlaps[self.active_forward_connections[input_bit]] += 1\n",
    "\n",
    "        activated_cols = np.argpartition(-self.overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        if self.min_activation_threshold > 0:\n",
    "            min_activation_mask = self.overlaps[activated_cols] >= self.min_activation_threshold\n",
    "            activated_cols = activated_cols[min_activation_mask]\n",
    "            \n",
    "        return activated_cols\n",
    "            \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        sparse_sdr = np.flatnonzero(dense_sdr)\n",
    "        dense_sdr = dense_sdr.astype(np.bool)\n",
    "        \n",
    "        activated_cols = self._get_active_columns(sparse_sdr)\n",
    "        if learn:\n",
    "            self._update_permanence(dense_sdr, sparse_sdr, activated_cols)\n",
    "\n",
    "        return activated_cols\n",
    "\n",
    "    def _update_permanence(self, dense_sdr, sparse_sdr, activated_cols):\n",
    "        to_change = defaultdict(list)\n",
    "        \n",
    "        for output_bit in activated_cols:\n",
    "            connections, permanences = self.backward_connections[output_bit]\n",
    "\n",
    "            indices = np.flatnonzero(dense_sdr[connections])\n",
    "            not_indices = np.flatnonzero(1 - dense_sdr[connections])\n",
    "            \n",
    "            to_disconnect_candidates = permanences[not_indices]\n",
    "            to_disconnect = np.logical_and(\n",
    "                to_disconnect_candidates >= self.permanence_threshold,\n",
    "                to_disconnect_candidates < self.permanence_threshold + self.syn_perm_dec\n",
    "            )\n",
    "            to_disconnect = connections[not_indices[np.flatnonzero(to_disconnect)]]\n",
    "            \n",
    "            to_connect_candidates = permanences[indices]\n",
    "            to_connect = np.logical_and(\n",
    "                to_connect_candidates < self.permanence_threshold,\n",
    "                to_connect_candidates >= self.permanence_threshold - self.syn_perm_inc\n",
    "            )\n",
    "            to_connect = connections[indices[np.flatnonzero(to_connect)]]\n",
    "        \n",
    "            for input_bit in to_disconnect:\n",
    "                to_change[input_bit].append(-1 - output_bit)\n",
    "            for input_bit in to_connect:\n",
    "                to_change[input_bit].append(output_bit)\n",
    "            \n",
    "            np.maximum(permanences[indices], 0, out=permanences[indices])\n",
    "            permanences -= self.syn_perm_dec\n",
    "            permanences[indices] += self.syn_perm_dec + self.syn_perm_inc            \n",
    "            np.minimum(permanences[indices], 1, out=permanences[indices])\n",
    "        \n",
    "#         for input_bit, changes in to_change.items():\n",
    "#             cols = set(self.active_forward_connections[input_bit])\n",
    "#             for x in changes:\n",
    "#                 if x >= 0:\n",
    "#                     cols.add(x)\n",
    "#                 else:\n",
    "#                     x = -x - 1\n",
    "#                     cols.remove(x)\n",
    "#             self.active_forward_connections[input_bit] = np.array(list(sorted(cols)))\n",
    "\n",
    "np.random.seed(1337)\n",
    "# my_sp = SparseSpatialPooler(10, 8, potenrial_synapses_p=.4)\n",
    "my_sp = SparseSpatialPooler(train_images[0].size, 10**2, potenrial_synapses_p=.4)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice(10, size=(6, 3))\n",
    "b = np.arange(10) * -2\n",
    "\n",
    "print(a)\n",
    "print(b[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0, 1, 1, 0], [1, 0, 1, 0]])\n",
    "b = np.array([[1, 1, 0, 0], [0, 1, 0, 0]])\n",
    "print(a)\n",
    "print(b)\n",
    "print(a != b)\n",
    "np.nonzero(a!=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class OptimizedSparseSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, sparsity_level=.04,\n",
    "        permanence_threshold=.5, synapse_permanence_deltas=(.1, .02), potential_synapses_p=None,\n",
    "        max_boost_factor=1.5, activity_duty_cycle=1000\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        if potential_synapses_p is None:\n",
    "            potential_synapses_p = .05 + (20000 - np.clip(output_size, 2000, 20000)) / 18000. * (.25 - .05)\n",
    "        assert(.05 <= potential_synapses_p <= .35)\n",
    "        self.potential_synapses_p = potential_synapses_p\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle = activity_duty_cycle\n",
    "        self.boost_ma_decay = (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        \n",
    "        # init\n",
    "        # (output_size, ~input_size)\n",
    "        self.connections = self._random_choice_noreplace_repeated(input_size, int(input_size * potential_synapses_p), output_size)        \n",
    "        self.connections_permanence = np.random.uniform(size=self.connections.shape)\n",
    "        \n",
    "        active_connections, active_connections_lengths, active_connections_max_len = self._get_active_connections()\n",
    "        self.active_connections = active_connections\n",
    "        self.active_connections_lengths = active_connections_lengths\n",
    "        self.active_connections_max_len = active_connections_max_len\n",
    "        \n",
    "        self.time_avg_activity = np.full(self.output_size, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_activity_mean = self.time_avg_activity.mean()\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "        # cache\n",
    "        self._active_synapses = None\n",
    "        self.dp = np.full(input_size, -self.syn_perm_dec, dtype=np.float)\n",
    "        self.dps = np.full((self.n_active_bits, self.connections.shape[1]), -self.syn_perm_dec, dtype=np.float)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _random_choice_noreplace_repeated(max_val, n_samples, n_times):\n",
    "        result = np.empty((n_times, n_samples), dtype=np.int)\n",
    "        for i in range(n_times):\n",
    "            result[i] = np.random.choice(max_val, size=n_samples, replace=False)\n",
    "            result[i].sort()\n",
    "        return result\n",
    "    \n",
    "    def _get_active_connections(self):\n",
    "        max_len = int(self.output_size * self.potential_synapses_p * 1.25 / 2)\n",
    "        result = np.empty((self.input_size, max_len), dtype=np.int)\n",
    "        lengths = np.zeros(self.input_size, dtype=np.int)\n",
    "        \n",
    "        mask = self.connections_permanence >= self.permanence_threshold\n",
    "        for ob in range(self.output_size):\n",
    "            active_connections = self.connections[ob, mask[ob]]\n",
    "            for ib in active_connections:\n",
    "                if lengths[ib] < max_len:\n",
    "                    result[ib, lengths[ib]] = ob\n",
    "                    lengths[ib] += 1\n",
    "        return result, lengths, max_len\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        sparse_sdr = np.flatnonzero(dense_sdr)\n",
    "        active_columns = self._get_active_columns(sparse_sdr)        \n",
    "        if learn:\n",
    "            self._update_permanence(sparse_sdr, active_columns)\n",
    "            self._update_activity_boost(active_columns)\n",
    "            pass\n",
    "\n",
    "        return active_columns\n",
    "\n",
    "    def _get_active_columns(self, sparse_sdr):\n",
    "        active_synapses = np.concatenate([\n",
    "            self.active_connections[ib][:self.active_connections_lengths[ib]]\n",
    "            for ib in sparse_sdr\n",
    "        ])\n",
    "        negative_overlaps = np.bincount(active_synapses, minlength=self.output_size) * -self.boost\n",
    "\n",
    "        active_columns = np.argpartition(negative_overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        return active_columns\n",
    "    \n",
    "    def _update_permanence(self, sparse_sdr, active_columns):\n",
    "        origin_active_status = (self.connections_permanence[active_columns] >= self.permanence_threshold)\n",
    "        \n",
    "        self.dp[sparse_sdr] = self.syn_perm_inc\n",
    "        dps = self.dps\n",
    "        dps[:] = self.connections_permanence[active_columns]\n",
    "        dps += self.dp[self.connections[active_columns]]\n",
    "        np.clip(dps, 0., 1., out=dps)\n",
    "        self.connections_permanence[active_columns] = dps\n",
    "        self.dp[sparse_sdr] = -self.syn_perm_dec\n",
    "        \n",
    "        changed_active_status = np.nonzero(\n",
    "            origin_active_status != (self.connections_permanence[active_columns] >= self.permanence_threshold)\n",
    "        )\n",
    "        \n",
    "        obs, poss = changed_active_status\n",
    "        ibs, to_remove, to_add = set(), defaultdict(list), defaultdict(list)\n",
    "        for col, ob, pos in zip(obs, active_columns[obs], poss):\n",
    "            ib = self.connections[ob, pos]\n",
    "            ibs.add(ib)\n",
    "            if origin_active_status[col, pos]:\n",
    "                to_remove[ib].append(ob)\n",
    "            else:\n",
    "                to_add[ib].append(ob)\n",
    "\n",
    "        for ib in ibs:\n",
    "            n = self.active_connections_lengths[ib]\n",
    "            \n",
    "            if to_remove[ib]:\n",
    "                lr = len(to_remove[ib])\n",
    "                mask = np.isin(self.active_connections[ib, :n], to_remove[ib], assume_unique=True, invert=True)\n",
    "                self.active_connections[ib, :n-lr] = self.active_connections[ib, :n][mask]\n",
    "                n -= lr\n",
    "                                \n",
    "            if to_add[ib]:\n",
    "                la = len(to_add[ib])\n",
    "                if n + la < self.active_connections_max_len:\n",
    "                    self.active_connections[ib, n:n+la] = to_add[ib]\n",
    "                    n += la\n",
    "                else:\n",
    "                    for ob in to_add:\n",
    "                        if n < self.active_connections_max_len:\n",
    "                            self.active_connections[ib, n] = ob\n",
    "                            n += 1\n",
    "            self.active_connections_lengths[ib] = n\n",
    "            \n",
    "        self.dp[sparse_sdr] = self.syn_perm_inc\n",
    "        \n",
    "    def _update_activity_boost(self, active_columns):\n",
    "        decay = self.boost_ma_decay\n",
    "        \n",
    "        self.time_avg_activity *= decay\n",
    "        self.time_avg_activity[active_columns] += 1. - decay\n",
    "        self.time_avg_activity_mean = decay * self.time_avg_activity_mean + (1 - decay) * active_columns.size / self.output_size\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity_mean))\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "my_sp = OptimizedSparseSpatialPooler(train_images[0].size, 10**2)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "a = np.random.choice(10, size=(6, 3))\n",
    "b = np.random.choice(2, size=(6, 3)).astype(np.bool)\n",
    "c = np.random.choice(2, size=10).astype(np.bool)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(a[b])\n",
    "print(a * b)\n",
    "print(c[a] * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class OptimizedSparseSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, sparsity_level=.04,\n",
    "        permanence_threshold=.5, synapse_permanence_deltas=(.1, .02), potential_synapses_p=None,\n",
    "        max_boost_factor=1.5, activity_duty_cycle=1000\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        if potential_synapses_p is None:\n",
    "            potential_synapses_p = .05 + (20000 - np.clip(output_size, 2000, 20000)) / 18000. * (.25 - .05)\n",
    "        assert(.05 <= potential_synapses_p <= .35)\n",
    "        self.potential_synapses_p = potential_synapses_p\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle = activity_duty_cycle\n",
    "        self.boost_ma_decay = (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        \n",
    "        # init\n",
    "        # (output_size, ~input_size)\n",
    "        self.connections = self._random_choice_noreplace_repeated(input_size, int(input_size * potential_synapses_p), output_size)        \n",
    "        self.connections_permanence = np.random.uniform(size=self.connections.shape)\n",
    "        \n",
    "#         active_connections, active_connections_lengths, active_connections_max_len = self._get_active_connections()\n",
    "#         self.active_connections = active_connections\n",
    "#         self.active_connections_lengths = active_connections_lengths\n",
    "#         self.active_connections_max_len = active_connections_max_len\n",
    "        self.active_connections_status = self.connections_permanence >= self.permanence_threshold\n",
    "        \n",
    "        self.time_avg_activity = np.full(self.output_size, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_activity_mean = self.time_avg_activity.mean()\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "        # cache\n",
    "        self._active_synapses = None\n",
    "        self.dp = np.full(input_size, -self.syn_perm_dec, dtype=np.float)\n",
    "        self.dps = np.full((self.n_active_bits, self.connections.shape[1]), -self.syn_perm_dec, dtype=np.float)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _random_choice_noreplace_repeated(max_val, n_samples, n_times):\n",
    "        result = np.empty((n_times, n_samples), dtype=np.int)\n",
    "        for i in range(n_times):\n",
    "            result[i] = np.random.choice(max_val, size=n_samples, replace=False)\n",
    "            result[i].sort()\n",
    "        return result\n",
    "    \n",
    "    def _get_active_connections(self):\n",
    "        max_len = int(self.output_size * self.potential_synapses_p * 1.25 / 2)\n",
    "        result = np.empty((self.input_size, max_len), dtype=np.int)\n",
    "        lengths = np.zeros(self.input_size, dtype=np.int)\n",
    "        \n",
    "        mask = self.connections_permanence >= self.permanence_threshold\n",
    "        for ob in range(self.output_size):\n",
    "            active_connections = self.connections[ob, mask[ob]]\n",
    "            for ib in active_connections:\n",
    "                if lengths[ib] < max_len:\n",
    "                    result[ib, lengths[ib]] = ob\n",
    "                    lengths[ib] += 1\n",
    "        return result, lengths, max_len\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        sparse_sdr = np.flatnonzero(dense_sdr)\n",
    "        active_columns = self._get_active_columns2(sparse_sdr, dense_sdr)        \n",
    "        if learn:\n",
    "            self._update_permanence(sparse_sdr, active_columns)\n",
    "            self._update_activity_boost(active_columns)\n",
    "            pass\n",
    "\n",
    "        return active_columns\n",
    "\n",
    "    def _get_active_columns2(self, sparse_sdr, dense_sdr):\n",
    "        negative_overlaps = -np.count_nonzero(dense_sdr[self.connections] * self.active_connections_status, axis=-1)\n",
    "#         active_synapses = np.concatenate([\n",
    "#             self.active_connections[ib][:self.active_connections_lengths[ib]]\n",
    "#             for ib in sparse_sdr\n",
    "#         ])\n",
    "#         negative_overlaps = np.bincount(active_synapses, minlength=self.output_size) * -self.boost\n",
    "\n",
    "        active_columns = np.argpartition(negative_overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        return active_columns\n",
    "\n",
    "    def _get_active_columns(self, sparse_sdr, dense_sdr):\n",
    "        active_synapses = np.concatenate([\n",
    "            self.active_connections[ib][:self.active_connections_lengths[ib]]\n",
    "            for ib in sparse_sdr\n",
    "        ])\n",
    "        negative_overlaps = np.bincount(active_synapses, minlength=self.output_size) * -self.boost\n",
    "\n",
    "        active_columns = np.argpartition(negative_overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        return active_columns\n",
    "    \n",
    "    def _update_permanence(self, sparse_sdr, active_columns):\n",
    "#         origin_active_status = self.active_connections_status[active_columns]\n",
    "        \n",
    "        self.dp[sparse_sdr] = self.syn_perm_inc\n",
    "        dps = self.dps\n",
    "        dps[:] = self.connections_permanence[active_columns]\n",
    "        dps += self.dp[self.connections[active_columns]]\n",
    "        np.clip(dps, 0., 1., out=dps)\n",
    "        self.connections_permanence[active_columns] = dps\n",
    "        self.dp[sparse_sdr] = -self.syn_perm_dec\n",
    "        \n",
    "        self.active_connections_status[active_columns] = self.connections_permanence[active_columns] >= self.permanence_threshold\n",
    "#         changed_active_status = np.nonzero(origin_active_status != self.active_connections_status[active_columns])\n",
    "        \n",
    "#         obs, poss = changed_active_status\n",
    "#         ibs, to_remove, to_add = set(), defaultdict(list), defaultdict(list)\n",
    "#         for col, ob, pos in zip(obs, active_columns[obs], poss):\n",
    "#             ib = self.connections[ob, pos]\n",
    "#             ibs.add(ib)\n",
    "#             if origin_active_status[col, pos]:\n",
    "#                 to_remove[ib].append(ob)\n",
    "#             else:\n",
    "#                 to_add[ib].append(ob)\n",
    "\n",
    "#         for ib in ibs:\n",
    "#             n = self.active_connections_lengths[ib]\n",
    "            \n",
    "#             if to_remove[ib]:\n",
    "#                 lr = len(to_remove[ib])\n",
    "#                 mask = np.isin(self.active_connections[ib, :n], to_remove[ib], assume_unique=True, invert=True)\n",
    "#                 self.active_connections[ib, :n-lr] = self.active_connections[ib, :n][mask]\n",
    "#                 n -= lr\n",
    "                                \n",
    "#             if to_add[ib]:\n",
    "#                 la = len(to_add[ib])\n",
    "#                 if n + la < self.active_connections_max_len:\n",
    "#                     self.active_connections[ib, n:n+la] = to_add[ib]\n",
    "#                     n += la\n",
    "#                 else:\n",
    "#                     for ob in to_add:\n",
    "#                         if n < self.active_connections_max_len:\n",
    "#                             self.active_connections[ib, n] = ob\n",
    "#                             n += 1\n",
    "#             self.active_connections_lengths[ib] = n\n",
    "            \n",
    "        self.dp[sparse_sdr] = self.syn_perm_inc\n",
    "        \n",
    "    def _update_activity_boost(self, active_columns):\n",
    "        decay = self.boost_ma_decay\n",
    "        \n",
    "        self.time_avg_activity *= decay\n",
    "        self.time_avg_activity[active_columns] += 1. - decay\n",
    "        self.time_avg_activity_mean = decay * self.time_avg_activity_mean + (1 - decay) * active_columns.size / self.output_size\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity_mean))\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "my_sp = OptimizedSparseSpatialPooler(train_images[0].size, 10**2)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 3000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedSparseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=65**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=.15\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "pretrain_sp(sp, x_tr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = OptimizedSparseSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=65**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    max_boost_factor=3,\n",
    "    potential_synapses_p=None\n",
    ")\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
