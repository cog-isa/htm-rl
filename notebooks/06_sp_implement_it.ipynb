{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Spatial Pooler\n",
    "\n",
    "Данная тетрадь содержит задачу реализации Spatial Pooler'а.\n",
    "\n",
    "Для начала посмотри эпизоды 0-8 видео гайда [HTMSchool](https://www.youtube.com/watch?v=XMB0ri4qgwc&list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Getting ready\n",
    "\n",
    "Данная секция содержит:\n",
    "\n",
    "- [опционально] установка `htm.core`\n",
    "- импорт необходимых пакетов (убедись, что все они установлены)\n",
    "- загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTM.Core\n",
    "\n",
    "Если у тебя не установлен пакет `htm.core`, раскомментируй и запусти следующую ячейку. В случае проблем, обратись к официальной странице пакета на [гитхабе](https://github.com/htm-community/htm.core#installation) и проверь требуемые зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -i https://test.pypi.org/simple/ htm.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Следующая ячейка загружает датасет MNIST (займет порядка 10-20 сек)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name, num_test, shape=None):\n",
    "    \"\"\" \n",
    "    fetch dataset from openML.org and split to train/test\n",
    "    @param name - ID on openML (eg. 'mnist_784')\n",
    "    @param num_test - num. samples to take as test\n",
    "    @param shape - new reshape of a single data point (ie data['data'][0]) as a list. Eg. [28,28] for MNIST\n",
    "    \"\"\"\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz=data['target'].shape[0]\n",
    "\n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape)\n",
    "\n",
    "    y = data['target'].astype(np.int32)\n",
    "    # split to train/test data\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "\n",
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', 10000, shape=[28,28])\n",
    "\n",
    "np.random.seed(seed)\n",
    "train_images, train_labels = shuffle_data(train_images, train_labels)\n",
    "test_images, test_labels = shuffle_data(test_images, test_labels)\n",
    "\n",
    "n_train_samples = train_images.shape[0]\n",
    "n_test_samples = test_images.shape[0]\n",
    "image_shape = train_images[0].shape\n",
    "image_side = image_shape[0]\n",
    "image_size = image_side ** 2\n",
    "\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример формата данных датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])\n",
    "print(f'Label: {train_labels[0]}')\n",
    "print(f'Image shape: {image_shape}')\n",
    "print(f'Image middle row: {train_images[0][image_side//2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем датасет в бинарные изображения и дальше будем работать с бинарными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatten_image(flatten_image, image_height=28):\n",
    "    plt.imshow(flatten_image.reshape((image_height, -1)))\n",
    "\n",
    "def to_binary_flatten_images(images):\n",
    "    n_samples = images.shape[0]\n",
    "    # flatten every image to vector\n",
    "    images = images.reshape((n_samples, -1))\n",
    "    # binary encoding: each image pixel is encoded either 0 or 1 depending on that image mean value\n",
    "    images = (images >= images.mean(axis=1, keepdims=True)).astype(np.int8)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = to_binary_flatten_images(train_images)\n",
    "test_images = to_binary_flatten_images(test_images)\n",
    "plot_flatten_image(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Baseline: classifier on raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(x_tst) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "# 87.3; 888ms\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Spatial Pooler: skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoOpSpatialPooler:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        return np.nonzero(dense_sdr)[0]\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "sp = NoOpSpatialPooler(train_images[0].size)\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size)\n",
    "assert sparse_sdr.size < sp.output_size\n",
    "sparse_sdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train/test SP performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def pretrain_sp(sp, images, n_samples):\n",
    "    for img in images[:n_samples]:\n",
    "        sp.compute(img, True)\n",
    "    \n",
    "def encode_to_csr_with_sp(images, sp, learn):\n",
    "    flatten_encoded_sdrs = []\n",
    "    indptr = [0]\n",
    "    for img in images:\n",
    "        encoded_sparse_sdr = sp.compute(img, learn)\n",
    "        flatten_encoded_sdrs.extend(encoded_sparse_sdr)\n",
    "        indptr.append(len(flatten_encoded_sdrs))\n",
    "\n",
    "    data = np.ones(len(flatten_encoded_sdrs))\n",
    "    csr = csr_matrix((data, flatten_encoded_sdrs, indptr), shape=(images.shape[0], sp.output_size))\n",
    "    return csr\n",
    "\n",
    "def test_classification_with_sp(x_tr,  y_tr, x_tst, y_tst, sp):\n",
    "    # a small pretrain SP before real work\n",
    "    pretrain_sp(sp, x_tr, n_samples=1000)\n",
    "    \n",
    "    # encode images and continuously train SP\n",
    "    csr = encode_to_csr_with_sp(x_tr, sp, learn=True)\n",
    "    \n",
    "    # train linreg\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    # encode test images (without SP learning) and then test score\n",
    "    csr = encode_to_csr_with_sp(x_tst, sp, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "my_sp = NoOpSpatialPooler(train_images[0].size)\n",
    "\n",
    "# 87.3; 1.16s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, my_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Spatial Pooler: learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, \n",
    "        permanence_threshold, sparsity_level, synapse_permanence_deltas, min_activation_threshold\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        # todo\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.synapse_permanence_increment, self.synapse_permanence_decrement = synapse_permanence_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        # initialization\n",
    "        # todo all\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[.2, .8])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        \n",
    "        # remove\n",
    "        self.dp = np.empty(input_size, dtype=np.float)\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        dense_sdr = dense_sdr.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, dense_sdr] >= self.permanence_threshold\n",
    "        overlaps = np.count_nonzero(active_cells, -1)\n",
    "        \n",
    "        activated_cols = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_cols = activated_cols[overlaps[activated_cols] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            self._update_permanence(dense_sdr, activated_cols)\n",
    "\n",
    "        return activated_cols\n",
    "    \n",
    "    def _update_permanence(self, sdr, columns):\n",
    "        dp = self.dp\n",
    "        dp[sdr] = self.synapse_permanence_increment\n",
    "        dp[~sdr] = -self.synapse_permanence_decrement\n",
    "        perm = self.connections_permanence[columns]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[columns], 0, 1)\n",
    "\n",
    "        \n",
    "np.random.seed(seed)\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size,\n",
    "    output_size=10**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .02),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size, sp.n_active_bits)\n",
    "assert sparse_sdr.size == sp.n_active_bits\n",
    "sparse_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "# 80.2; 2.98 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size,\n",
    "        permanence_threshold, sparsity_level, synapse_permanence_deltas, min_activation_threshold=1, potenrial_synapses_p=.8,\n",
    "        max_boost_factor=1.5, boost_sliding_window=(1000, 1000)\n",
    "    ):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.joint_shape = (output_size, input_size)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = synapse_permanence_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle, self.overlap_duty_cycle = boost_sliding_window\n",
    "        \n",
    "        # init \n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[1-potenrial_synapses_p, potenrial_synapses_p])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        self.time_avg_activity = np.full(self.output_size, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_overlap = np.ones(self.output_size, dtype=np.float)\n",
    "        self.dp = np.empty(input_size, dtype=np.float)\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        dense_sdr = dense_sdr.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, dense_sdr] >= self.permanence_threshold\n",
    "        overlaps = np.count_nonzero(active_cells, -1) * self.boost\n",
    "        \n",
    "        activated_cols = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_cols = activated_cols[overlaps[activated_cols] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            self._update_permanence(dense_sdr, activated_cols)\n",
    "            self._update_activity_boost(activated_cols)\n",
    "#             self._update_overlap_boost(dense_sdr, activated_cols, overlaps)\n",
    "\n",
    "        return activated_cols\n",
    "    \n",
    "    def _update_permanence(self, dense_sdr, activated_cols):\n",
    "        dp = self.dp\n",
    "        dp[dense_sdr] = self.syn_perm_inc\n",
    "        dp[~dense_sdr] = -self.syn_perm_dec\n",
    "        perm = self.connections_permanence[activated_cols]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[activated_cols], 0, 1)\n",
    "        \n",
    "    def _update_activity_boost(self, activated_cols):\n",
    "        self.time_avg_activity *= (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        self.time_avg_activity[activated_cols] += 1 / self.activity_duty_cycle\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _update_overlap_boost(self, x, rows, cols, overlaps):\n",
    "        self.time_avg_overlap += (overlaps - self.time_avg_overlap) / self.overlap_duty_cycle\n",
    "        k = int(.05 * self.output_size)\n",
    "        to_boost_indices = np.argpartition(self.time_avg_overlap, k)[:k]\n",
    "        to_boost = self.connections_permanence[to_boost_indices]\n",
    "        to_boost = np.clip(to_boost + .1 * self.permanence_threshold, 0, 1)\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity.mean()))\n",
    "        \n",
    "\n",
    "np.random.seed(1337)\n",
    "my_sp = BoostedSpatialPooler(train_images[0].size, 10**2, .5, .04, (.1, .02), 4, potenrial_synapses_p=.8)\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    potenrial_synapses_p=.4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(x_tst) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "# 92.11; 38s\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "# 89.3; 72 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "# 93.15; 221 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 91.15; 86 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 94.44; 238 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4,\n",
    "    potenrial_synapses_p=.4,\n",
    "    max_boost_factor=3\n",
    ")\n",
    "# 84.0; 3.24 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
