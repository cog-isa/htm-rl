{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_rl.agent.agent import Agent, AgentRunner\n",
    "from htm_rl.agent.memory import Memory, TemporalMemory\n",
    "from htm_rl.agent.planner import Planner\n",
    "from htm_rl.common.sa_sdr_encoder import SaSdrEncoder, format_sa_superposition\n",
    "from htm_rl.common.base_sa import SaRelatedComposition, Sa, SaSuperposition\n",
    "from htm_rl.common.int_sdr_encoder import IntSdrEncoder, IntRangeEncoder\n",
    "from htm_rl.common.int_sdr_encoder import SequenceSdrEncoder\n",
    "from htm_rl.envs.gridworld_pomdp import GridWorld\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(pars, sa_encoder, start_indicator=None):\n",
    "    tm = TemporalMemory(**pars)\n",
    "    memory = Memory(tm, sa_encoder, sa_encoder.format, format_sa_superposition, start_indicator=start_indicator)\n",
    "    return memory\n",
    "\n",
    "def learn_way(way, memory, environment, verbosity=1):\n",
    "    memory.reset()\n",
    "    state, reward, done = environment.reset(), 0, False\n",
    "    for action in way:\n",
    "        if verbosity > 1:\n",
    "            environment.render()\n",
    "            print(f'Action {action} State: {state}')\n",
    "        memory.train(Sa(state, action), verbosity)\n",
    "        state, _, _, info = environment.step(action)\n",
    "\n",
    "def check_agent(memory, environment, goal_state, verbosity=1):\n",
    "    planner = Planner(memory, 10, 1)\n",
    "    agent = Agent(memory, planner, environment.n_actions)\n",
    "    run = AgentRunner(agent, environment, 1, max_steps, 0, verbosity)\n",
    "    run.agent.planner.add_goal(goal_state)\n",
    "    run.agent.set_planning_horizon(10)\n",
    "    run.run()\n",
    "    if run.train_stats.rewards[-1] > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_way(max_steps, n_actions):\n",
    "    return [randint(0, n_actions-1) for _ in range(max_steps)]\n",
    "\n",
    "def run_way_after_experiments(pars, sa_encoder, goal_state,\n",
    "                              start_indicator=None,\n",
    "                              n_experiments=3,\n",
    "                              verbosity=0,\n",
    "                              learning_true_count=1,\n",
    "                              max_noise_steps=1000):\n",
    "    results = list()\n",
    "    ways_history = {'way': [], 'steps': [], 'experiment': []}\n",
    "    for experiment in tqdm(range(n_experiments)):\n",
    "        n_steps = 0 # number of noise action sequences\n",
    "        while True:\n",
    "            memory = init_memory(pars, sa_encoder, start_indicator=start_indicator)\n",
    "            way = None\n",
    "            for step in range(n_steps):\n",
    "                way = random_way(max_steps, gw.n_actions)\n",
    "                learn_way(way, memory, gw)\n",
    "            for _ in range(learning_true_count):\n",
    "                learn_way(actions, memory, gw)\n",
    "            if not check_agent(memory, gw, goal_state, verbosity):\n",
    "                if way is not None:\n",
    "                    ways_history['way'].append(str(way))\n",
    "                    ways_history['steps'].append(n_steps)\n",
    "                    ways_history['experiment'].append(experiment)\n",
    "                break\n",
    "            else:\n",
    "                n_steps += 1\n",
    "                if n_steps == max_noise_steps:\n",
    "                    print('Max steps are reached')\n",
    "                    break\n",
    "        results.append(n_steps)\n",
    "    return results, ways_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 12\n",
    "actions = [2, 2, 1, 2, 2, 1, 2, 2]\n",
    "world_description = [[2,0,0],\n",
    "                     [1,1,0],\n",
    "                     [0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(world_description, (3, 3), agent_initial_position={'row': 2, 'column': 0},\n",
    "               observable_vars=['state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.observable_state, gw.filtered_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_bits = 10\n",
    "action_bits = 10\n",
    "\n",
    "state_encoder = IntSdrEncoder('state', gw.n_states+1, state_bits, state_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder = IntSdrEncoder('action', gw.n_actions+1,\n",
    "                              value_bits=action_bits, activation_threshold=action_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits, sa_encoder.value_bits, sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 1 + 2 * 9\n",
    "goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=8,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.51,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.05,\n",
    "                                predictedSegmentDecrement=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_way_after_experiments(pars, sa_encoder, goal_state,\n",
    "                                    start_indicator=Sa(gw.n_states, 3),\n",
    "                                    n_experiments=100,\n",
    "                                    learning_true_count=1,\n",
    "                                    verbosity=0,\n",
    "                                    max_noise_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = np.array(results[0])\n",
    "df_results.mean(), df_results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = init_memory(pars, sa_encoder, start_indicator=Sa(gw.n_states, 3))\n",
    "learn_way(actions, memory, gw, verbosity=1)\n",
    "learn_way(actions, memory, gw, verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, gw.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, gw, 1, max_steps, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_stats.rewards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь кодируем то же mdp, но уже отедльными числами, сначала: (кордината в вытянутом лабиринте, направление)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(world_description, (3, 3), agent_initial_position={'row': 2, 'column': 0},\n",
    "               observable_vars=['flatten_index', 'direction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.observable_state, gw.filtered_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bits = 5\n",
    "direction_bits = 5\n",
    "action_bits = 10\n",
    "\n",
    "state_encoder = SequenceSdrEncoder('state',\n",
    "                                   encoders=[\n",
    "                                             IntSdrEncoder('flatten_index',\n",
    "                                                                   gw.world_size[0]*gw.world_size[1]+1,\n",
    "                                                                   index_bits,\n",
    "                                                                   index_bits),\n",
    "                                             IntSdrEncoder('direction', 4+1, direction_bits, direction_bits)\n",
    "                                            ],\n",
    "                                   size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder = IntSdrEncoder('action', gw.n_actions+1,\n",
    "                              value_bits=action_bits, activation_threshold=action_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits, sa_encoder.value_bits, sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = (1, 2)\n",
    "goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=8,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.51,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.05,\n",
    "                                predictedSegmentDecrement=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_way_after_experiments(pars, sa_encoder, goal_state,\n",
    "                                    start_indicator=Sa((9, 4), 3),\n",
    "                                    n_experiments=10,\n",
    "                                    learning_true_count=1,\n",
    "                                    verbosity=0,\n",
    "                                    max_noise_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = np.array(results[0])\n",
    "df_results.mean(), df_results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = init_memory(pars, sa_encoder, start_indicator=Sa(gw.n_states, 3))\n",
    "learn_way(actions, memory, gw, verbosity=1)\n",
    "learn_way(actions, memory, gw, verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, gw.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, gw, 1, max_steps, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_stats.rewards[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDP тесты с непересекающейся кодировкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(world_description, (3, 3), agent_initial_position={'row': 2, 'column': 0},\n",
    "               observable_vars=['distance', 'surface'], one_value_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.observable_state, gw.filtered_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_bits = 10\n",
    "action_bits = 10\n",
    "\n",
    "state_encoder = IntSdrEncoder('state', gw.n_obs_states+1, state_bits, state_bits-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder = IntSdrEncoder('action', gw.n_actions+1,\n",
    "                              value_bits=action_bits, activation_threshold=action_bits-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits, sa_encoder.value_bits, sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 6\n",
    "goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=50,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.51,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.05,\n",
    "                                predictedSegmentDecrement=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_way_after_experiments(pars,\n",
    "                                    sa_encoder,\n",
    "                                    goal_state,\n",
    "                                    start_indicator=Sa(gw.n_obs_states, 3),\n",
    "                                    n_experiments=100,\n",
    "                                    learning_true_count=10,\n",
    "                                    verbosity=0,\n",
    "                                    max_noise_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = np.array(results[0])\n",
    "df_results.mean(), df_results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = init_memory(pars, sa_encoder, start_indicator=Sa(gw.n_obs_states, 3))\n",
    "learn_way(actions, memory, gw, verbosity=1)\n",
    "learn_way(actions, memory, gw, verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, gw.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, gw, 1, max_steps, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(\n",
    "    goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_stats.rewards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
