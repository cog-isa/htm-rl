{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Spatial Pooler\n",
    "\n",
    "В данной тетради требуется реализовать Spatial Pooler по аналогии с [описанием](https://numenta.com/assets/pdf/spatial-pooling-algorithm/Spatial-Pooling-Algorithm-Details.pdf) и [статьёй](https://www.frontiersin.org/articles/10.3389/fncom.2017.00111/pdf) от нументы. Данная тетрадь сделана на основе [примера](https://github.com/htm-community/htm.core/blob/master/py/htm/examples/mnist.py) из нументовского фреймворка `htm.core`.\n",
    "\n",
    "Для начала посмотри эпизоды 0-8 видео гайда [HTMSchool](https://www.youtube.com/watch?v=XMB0ri4qgwc&list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Getting ready\n",
    "\n",
    "Данная секция содержит:\n",
    "\n",
    "- [опционально] установка `htm.core`\n",
    "- импорт необходимых пакетов (убедись, что все они установлены)\n",
    "- загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTM.Core\n",
    "\n",
    "Если у тебя не установлен пакет `htm.core`, раскомментируй и запусти следующую ячейку. В случае проблем, обратись к официальной странице пакета на [гитхабе](https://github.com/htm-community/htm.core#installation) и проверь требуемые зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -i https://test.pypi.org/simple/ htm.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Следующая ячейка загружает датасет MNIST (займет порядка 10-20 сек)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name, num_test, shape=None):\n",
    "    \"\"\" \n",
    "    fetch dataset from openML.org and split to train/test\n",
    "    @param name - ID on openML (eg. 'mnist_784')\n",
    "    @param num_test - num. samples to take as test\n",
    "    @param shape - new reshape of a single data point (ie data['data'][0]) as a list. Eg. [28,28] for MNIST\n",
    "    \"\"\"\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz=data['target'].shape[0]\n",
    "\n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape)\n",
    "\n",
    "    y = data['target'].astype(np.int32)\n",
    "    # split to train/test data\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "\n",
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', 10000, shape=[28,28])\n",
    "\n",
    "np.random.seed(seed)\n",
    "train_images, train_labels = shuffle_data(train_images, train_labels)\n",
    "test_images, test_labels = shuffle_data(test_images, test_labels)\n",
    "\n",
    "n_train_samples = train_images.shape[0]\n",
    "n_test_samples = test_images.shape[0]\n",
    "image_shape = train_images[0].shape\n",
    "image_side = image_shape[0]\n",
    "image_size = image_side ** 2\n",
    "\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример формата данных датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])\n",
    "print(f'Label: {train_labels[0]}')\n",
    "print(f'Image shape: {image_shape}')\n",
    "print(f'Image middle row: {train_images[0][image_side//2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем датасет в бинарные изображения и дальше будем работать с бинарными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatten_image(flatten_image, image_height=28):\n",
    "    plt.imshow(flatten_image.reshape((image_height, -1)))\n",
    "\n",
    "def to_binary_flatten_images(images):\n",
    "    n_samples = images.shape[0]\n",
    "    # flatten every image to vector\n",
    "    images = images.reshape((n_samples, -1))\n",
    "    # binary encoding: each image pixel is encoded either 0 or 1 depending on that image mean value\n",
    "    images = (images >= images.mean(axis=1, keepdims=True)).astype(np.int8)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = to_binary_flatten_images(train_images)\n",
    "test_images = to_binary_flatten_images(test_images)\n",
    "plot_flatten_image(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Baseline: classifier on raw input\n",
    "\n",
    "В качестве бейзлайна возьмем стандартный sklearn'овский логрег классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(x_tst) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "# for debug purposes I chose smaller subset of the train/test set, you can set the whole set of 60k training samples\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "# 1k: 87.3; 888ms\n",
    "# 60k: 92.11; 38s\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Spatial Pooler: skeleton\n",
    "\n",
    "Временно сделаем пустой класс Spatial Pooler'а заглушку, чтобы дальше ввести весь необходимый auxiliary код для обучения и тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoOpSpatialPooler:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        return np.nonzero(dense_sdr)[0]\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "sp = NoOpSpatialPooler(train_images[0].size)\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size)\n",
    "assert sparse_sdr.size < sp.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train/test SP performance aux pipeline\n",
    "\n",
    "Ниже непосредственно код для обучения и тестирования классификации с использованием Spatial Pooler'а. Общая схема следующая - мы обучаем SP на train set'е, а дальше SDR векторы на выходе из SP используем в кач-ве входных данных для логрег классификатора в надежде, что эти данные разделимы еще лучше.\n",
    "\n",
    "__NB__: Не удивляйся, в реализации ниже обучение идет в немного полуонлайн режиме - делается небольшой претрейн, а потом полностью онлайн. Почему? прост))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def pretrain_sp(sp, images, n_samples):\n",
    "    for img in images[:n_samples]:\n",
    "        sp.compute(img, True)\n",
    "    \n",
    "def encode_to_csr_with_sp(images, sp, learn):\n",
    "    flatten_encoded_sdrs = []\n",
    "    indptr = [0]\n",
    "    for img in images:\n",
    "        encoded_sparse_sdr = sp.compute(img, learn)\n",
    "        flatten_encoded_sdrs.extend(encoded_sparse_sdr)\n",
    "        indptr.append(len(flatten_encoded_sdrs))\n",
    "\n",
    "    data = np.ones(len(flatten_encoded_sdrs))\n",
    "    csr = csr_matrix((data, flatten_encoded_sdrs, indptr), shape=(images.shape[0], sp.output_size))\n",
    "    return csr\n",
    "\n",
    "def test_classification_with_sp(x_tr,  y_tr, x_tst, y_tst, sp):\n",
    "    # a small pretrain SP before real work\n",
    "    pretrain_sp(sp, x_tr, n_samples=1000)\n",
    "    \n",
    "    # encode images and continuously train SP\n",
    "    csr = encode_to_csr_with_sp(x_tr, sp, learn=True)\n",
    "    \n",
    "    # train linreg\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    # encode test images (without SP learning) and then test score\n",
    "    csr = encode_to_csr_with_sp(x_tst, sp, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "my_sp = NoOpSpatialPooler(train_images[0].size)\n",
    "\n",
    "# 87.3; 1.16s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, my_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Spatial Pooler: learning\n",
    "\n",
    "__Здесь начинается практическое задание__\n",
    "\n",
    "В этой части требуется реализовать простую версию SP с обучением как описано в видео htm scool до бустинга (т.е. бустинг пока не нужен)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, permanence_threshold, output_sparsity, synapse_permanence_deltas, receptive_field_sparsity\n",
    "    ):\n",
    "        '''\n",
    "        params:\n",
    "            `input_size` - the size of the input SDR\n",
    "            `output_size` - the size of the output SDR\n",
    "            `permanence_threshold` - value in [0, 1], defines whether or not a connection is active\n",
    "            `output_sparsity` - value in [0, 1], defines desired output SDR sparsity, e.g. 0.02 is 2% sparsity\n",
    "            `synapse_permanence_deltas` - tuple of (p+, p-), defines permanence increment/decrement for learning\n",
    "            `receptive_field_sparsity` - value in [0, 1], defines the fraction of _potential_ synapses\n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # TODO: initialization        \n",
    "        \n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        '''\n",
    "        params:\n",
    "            `dense_sdr` - input SDR in dense for, i.e. as np.array\n",
    "            `learn` - bool flag, whether or not to do a learning step\n",
    "        \n",
    "        returns:\n",
    "            a list of activated columns indices\n",
    "        '''\n",
    "        \n",
    "        # TODO: compute and learn\n",
    "\n",
    "        # make sure that activated_cols is array of indices, not a whole binary vector\n",
    "        return activated_cols\n",
    "\n",
    "        \n",
    "np.random.seed(seed)\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size,\n",
    "    output_size=10**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8\n",
    ")\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size, sp.n_active_bits)\n",
    "assert sparse_sdr.size == sp.n_active_bits\n",
    "sparse_sdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05.1. Naive SP performance\n",
    "\n",
    "Проверь качество работы своей реализации. В комментариях примерные значения для разных `n`, на которые можно попробовать ориентироваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8\n",
    ")\n",
    "# 1k: 80.2; 2.98s\n",
    "# 60k: 89.3; 72 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересная метрика оценки качества работы SP и вырожденности датасета - энтропия выходных активаций.\n",
    "\n",
    "Добавь либо в реализацию класса, либо где-то сбоку возможность учета статистики активаций выходных клеток (ты наверняка заметил, что обычно их контринтуитивно называют столбцами, что оч круто запутывает) и функцию подсчета энтропии на основе этой статистики. Собери такую статистику на одном прогоне на всем датасете и:\n",
    "\n",
    "- нарисуй гистограмму вероятностей активаций\n",
    "- посчитай энтропию $H = -\\sum p \\cdot \\log p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prob hist & entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давай посмотрим, как выходной размер `output_size` влияет на качество работы. Протестируй разные значения `output_size` - например, $[10^2, 15^2,..., 55^2, 60^2]$ и отрисуй график зависимости [качества классификации, но можешь и времени работы тоже].\n",
    "\n",
    "_Можно для начала прикинуть для n=1000 и двигаться дальше, а позже пересчитать на всем датасете_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Spatial Pooler: boosting\n",
    "\n",
    "В этом пункте требуется реализовать бустинг. Обрати внимание, в видео htm school речь ведется о бустинге overlap score на основе активности выходных ячеек -  именно такой вид бустинга и нужно реализовать. Еще есть бустинг значений permanence синапсов выходных ячеек, имеющих слишком низкое среднее значение overlap score за последние N итераций - рассмотрение этого вида бустинга оставим в стороне. Так что дальше под бустингом будет иметься в виду только первый вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedSpatialPooler:\n",
    "    def __init__(\n",
    "        self, input_size, output_size, permanence_threshold, output_sparsity, synapse_permanence_deltas, receptive_field_sparsity,\n",
    "        max_boost_factor, boost_sliding_window\n",
    "    ):\n",
    "        '''\n",
    "        params:\n",
    "            `input_size` - the size of the input SDR\n",
    "            `output_size` - the size of the output SDR\n",
    "            `permanence_threshold` - value in [0, 1], defines whether or not a connection is active\n",
    "            `output_sparsity` - value in [0, 1], defines desired output SDR sparsity, e.g. 0.02 is 2% sparsity\n",
    "            `synapse_permanence_deltas` - tuple of (p+, p-), defines permanence increment/decrement for learning\n",
    "            `receptive_field_sparsity` - value in [0, 1], defines the fraction of _potential_ synapses\n",
    "            `max_boost_factor` - value in [1, +inf), defines maximum allowed boosting\n",
    "            `boost_sliding_window` - value in [1, +inf), defines the size of the window for moving avg output column activity\n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # TODO: initialization\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        '''\n",
    "        params:\n",
    "            `dense_sdr` - input SDR in dense for, i.e. as np.array\n",
    "            `learn` - bool flag, whether or not to do a learning step\n",
    "        \n",
    "        returns:\n",
    "            a list of activated columns indices\n",
    "        '''\n",
    "        \n",
    "        # TODO: compute and learn\n",
    "\n",
    "        # make sure that activated_cols is array of indices, not a whole binary vector\n",
    "        return activated_cols\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "my_sp = BoostedSpatialPooler(\n",
    "    train_images[0].size, 10**2, .5, .04, (.1, .02), receptive_field_sparsity=.8, \n",
    "    max_boost_factor=1.5, boost_sliding_window=1000\n",
    ")\n",
    "my_sp.compute(train_images[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.1. SP with boosting performance\n",
    "\n",
    "_Дальше все как и в пункте 05.1_\n",
    "\n",
    "Проверь качество работы своей реализации. В комментариях по-прежнему примерные значения для разных `n`, на которые можно попробовать ориентироваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8,\n",
    "    max_boost_factor=3,\n",
    "    boost_sliding_window=1000\n",
    ")\n",
    "# 1k: 84.0; 3.24 s\n",
    "# 60k: 91.15; 86 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- нарисуй гистограмму вероятностей активаций\n",
    "- посчитай энтропию $H = -\\sum p \\cdot \\log p$\n",
    "- сравни с результатами без бустинга - есть какие-то очевидные выводы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prob hist & entropy & comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируй разные значения `output_size` - например, $[10^2, 15^2,..., 55^2, 60^2]$ и отрисуй график зависимости [качества классификации, но можешь и времени работы тоже].\n",
    "\n",
    "Сравни с результатами без бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test and plot results & comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "__[Опционально]__ можешь ради интереса поиграться с таким параметром как `receptive_field_sparsity`. Возьми в качестве выходного размера `output_size` = $50^2$ и проверь результаты на разных значениях параметра $[0.4, 0.5, ..., 1.0]$ и построй график зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test receptive_field_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. TESTING\n",
    "\n",
    "Дальше просто сравнительное тестирование получившихся результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(x_tst) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "# 92.11; 38s\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8,\n",
    ")\n",
    "# 89.3; 72 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = LearnableSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8,\n",
    ")\n",
    "# 93.15; 221 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=30**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8,\n",
    "    max_boost_factor=3,\n",
    "    boost_sliding_window=1000\n",
    ")\n",
    "# 91.15; 86 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "n = 100000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "\n",
    "sp = BoostedSpatialPooler(\n",
    "    input_size=train_images[0].size, \n",
    "    output_size=50**2,\n",
    "    permanence_threshold=.5,\n",
    "    output_sparsity=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    receptive_field_sparsity=.8,\n",
    "    max_boost_factor=3,\n",
    "    boost_sliding_window=1000\n",
    ")\n",
    "# 94.44; 238 s\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
