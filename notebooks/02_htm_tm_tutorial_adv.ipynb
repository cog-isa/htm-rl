{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial that shows some features of the Temporal Memory\n",
    "\n",
    "This program demonstrates some basic properties of the TM, in particular how it handles high-order sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(1)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from htm.bindings.sdr import SDR\n",
    "from htm.algorithms import TemporalMemory as TM\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Temporal Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TM(\n",
    "    columnDimensions = (2048,),\n",
    "    cellsPerColumn=8,\n",
    "    initialPermanence=0.21,\n",
    "    connectedPermanence=0.3,\n",
    "    minThreshold=15,\n",
    "    maxNewSynapseCount=40,\n",
    "    permanenceIncrement=0.1,\n",
    "    permanenceDecrement=0.1,\n",
    "    activationThreshold=15,\n",
    "    predictedSegmentDecrement=0.01,\n",
    ")\n",
    "tm.printParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity     = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)\n",
    "sparseCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a sparse representation of characters A, B, C, D, X, and Y.  \n",
    "In this particular example we manually construct them, but usually you would use the spatial pooler to build these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {inp : SDR( tm.numberOfColumns() ) for inp in \"ABCDXY\"}\n",
    "for i, inp in enumerate(\"ABCDXY\"):\n",
    "    fr, to = i * sparseCols, (i + 1) * sparseCols\n",
    "    dataset[inp].dense[fr:to] = 1\n",
    "    dataset[inp].dense = dataset[inp].dense # This line notifies the SDR that it's dense data has changed in-place.\n",
    "    print(f\"Input {inp} is bits at indices: [{fr} - {to})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux functions for training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTM(sequence, iterations, noiseLevel):\n",
    "    \"\"\"\n",
    "    Trains the TM with given sequence for a given number of time steps and level\n",
    "    of input corruption\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence: string\n",
    "        Sequence of input characters.\n",
    "    iterations: int\n",
    "        Number of time TM will be presented with sequence.\n",
    "    noiseLevel: float\n",
    "        Amount of noise to be applied on the characters in the sequence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x: is list of timestamps / step numbers\n",
    "    y: is list of prediction accuracy at each step\n",
    "    \"\"\"\n",
    "    ts = 0\n",
    "    x = []\n",
    "    y = []\n",
    "    for t in range(iterations):\n",
    "        tm.reset()\n",
    "        for inp in sequence:\n",
    "            v = SDR(dataset[inp]).addNoise(noiseLevel)\n",
    "            tm.compute(v, learn=True)\n",
    "            \n",
    "            x.append(ts)\n",
    "            y.append(1 - tm.anomaly)\n",
    "            ts += 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def showPredictions():\n",
    "    \"\"\"\n",
    "    Shows predictions of the TM when presented with the characters A, B, C, D, X, and\n",
    "    Y without any contextual information, that is, not embedded within a sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _get_printable_ranges(a):\n",
    "        i = 0\n",
    "        prints = []\n",
    "        while i < len(a):\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(a) and a[i-1] + 1 == a[i]:\n",
    "                i += 1\n",
    "\n",
    "            if start - i == 1:\n",
    "                pr = f'{a[start]}'\n",
    "            else:\n",
    "                pr = f'{a[start]}-{a[i-1]+1}'\n",
    "            prints.append(pr)\n",
    "        \n",
    "        return str.join(' ', prints)\n",
    "    \n",
    "    for inp in sorted(dataset.keys()):\n",
    "        print(\"--- \" + inp + \" ---\")\n",
    "        sdr = dataset[inp]\n",
    "        tm.reset()\n",
    "        tm.compute(sdr, learn=False)\n",
    "        tm.activateDendrites(learn=False)\n",
    "        activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells().sparse]\n",
    "        predictedColumnIndices = [tm.columnForCell(i) for i in tm.getPredictiveCells().sparse]\n",
    "        print(\"Active cols: \" + _get_printable_ranges(sorted(set(activeColumnsIndices))))\n",
    "        print(\"Predicted cols: \" + _get_printable_ranges(sorted(set(predictedColumnIndices))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Present the sequence ABCD to the TM\n",
    "\n",
    "The TM will eventually learn the sequence and predict the upcoming characters. This can be measured by the prediction accuracy in plot.  \n",
    "__N.B__. In-between sequences the prediction accuracy is 0.0 as the TM does not output any prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_accuracy(x, y, title):\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Prediction Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "seqT = \"ABCDXY\"\n",
    "\n",
    "seq1 = \"ABCD\"\n",
    "x, y = trainTM(seq1, iterations=10, noiseLevel=0.0)\n",
    "plot_prediction_accuracy(x, y, \"Fig. 1: TM learns sequence ABCD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions without context\n",
    "\n",
    "Once the TM has learned the sequence ABCD, we will present the individual characters to the TM to know its prediction.  \n",
    "The TM outputs the columns that become active upon the presentation of a particular character as well as the columns predicted in the next time step.\n",
    "\n",
    "Here, you should see that A predicts B, B predicts C, C predicts D, and D does not output any prediction.  \n",
    "__N.B__. Here, we are presenting individual characters, that is, a character deprived of context in a sequence. There is no prediction for characters X and Y as we have not presented them to the TM in any sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Present the sequence XBCY to the TM\n",
    "\n",
    "As expected, the accuracy will drop until the TM learns the new sequence (see plot).  \n",
    "What would be the prediction of the TM if presented with the sequence BC? This would depend on what character anteceding B. This is an important feature of high-order sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = \"XBCY\"\n",
    "x, y = trainTM(seq2, iterations=10, noiseLevel=0.0)\n",
    "\n",
    "# In this figure you can see how the TM starts making good predictions for particular\n",
    "# characters (spikes in the plot). Then, it will get half of its predictions right, which\n",
    "# correspond to the times in which is presented with character C. After some time, it\n",
    "# will learn correctly the sequence XBCY, and predict its characters accordingly.\n",
    "plot_prediction_accuracy(x, y, \"Fig. 2: TM learns new sequence XBCY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present again each of the characters individually to the TM, that is, not within any of the two sequences.\n",
    "\n",
    "When presented with character A the TM predicts B, B predicts C, but this time C outputs a simultaneous prediction of both D and Y. In order to disambiguate, the TM would require to know if the preceding characters were AB or XB. When presented with character X the TM predicts B, whereas Y and D yield no prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Present noisy inputs to the TM.\n",
    "\n",
    "We would like to see how the TM responds to the presence of noise and how it recovers from it.\n",
    "\n",
    "We will add noise to the sequence XBCY by corrupting 30% of the bits in the SDR encoding each character. We would expect to see a decrease in prediction accuracy as the TM is unable to learn the random noise in the input (see plot). However, this decrease is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trainTM(seq2, iterations=50, noiseLevel=0.3)\n",
    "plot_prediction_accuracy(x, y, \"Fig. 3: Accuracy in TM with 30% noise in input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look again at the output of the TM when presented with noisy input (30%).  \n",
    "Here, the noise is low enough that the TM is not affected by it, which would be the case if we saw 'noisy' columns being predicted when presented with individual characters. Thus, we could say that the TM exhibits resilience to noise in its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will increase the noise to 60% of the bits in the characters.  \n",
    "As expected, the predictive accuracy decreases (see plot) and 'noisy' columns are predicted by the TM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trainTM(seq2, iterations=50, noiseLevel=0.6)\n",
    "plot_prediction_accuracy(x, y, \"Fig. 4: Accuracy in TM with 60% noise in input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPredictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will the TM be able to forget the 'noisy' columns learned in the previous step?\n",
    "\n",
    "We will present the TM with the original sequence XBCY so it forgets the 'noisy' columns.  \n",
    "After presenting the uncorrupted sequence XBCY to the TM, we would expect to see the predicted noisy columns from the previous step disappear and the prediction accuracy return to normal. (see plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trainTM(seq2, iterations=10, noiseLevel=0.0)\n",
    "plot_prediction_accuracy(x, y, \"Fig. 5: When noise is suspended, accuracy is restored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Present both sequences ABCD and XBCY randomly to the TM.\n",
    "\n",
    "Here, we might observe simultaneous predictions occurring when the TM is presented with characters D, Y, and C.\n",
    "\n",
    "For this purpose we will use a blank TM.  \n",
    "__NB__. Here we will not reset the TM after presenting each sequence with the purpose of making the TM learn different predictions for D and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TM(\n",
    "    columnDimensions = (2048,),\n",
    "    cellsPerColumn=8,\n",
    "    initialPermanence=0.21,\n",
    "    connectedPermanence=0.3,\n",
    "    minThreshold=15,\n",
    "    maxNewSynapseCount=40,\n",
    "    permanenceIncrement=0.1,\n",
    "    permanenceDecrement=0.1,\n",
    "    activationThreshold=15,\n",
    "    predictedSegmentDecrement=0.01,\n",
    ")\n",
    "\n",
    "for t in range(75):\n",
    "    seq = random.choice([ seq1, seq2 ])\n",
    "    for inp in seq:\n",
    "        tm.compute(dataset[inp], learn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a look at the output of the TM when presented with the individual characters A, B, C, D, X, and Y.  \n",
    "We should observe simultaneous predictions when presented with character D (predicting A and X), character Y (predicting A and X), and when presented with character C (predicting D and Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPredictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
