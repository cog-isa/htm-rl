{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../htm_rl/htm_rl/')\n",
    "\n",
    "from agent.agent import Agent, AgentRunner\n",
    "from agent.memory import Memory, TemporalMemory\n",
    "from agent.planner import Planner\n",
    "from common.sa_sdr_encoder import SaSdrEncoder, format_sa_superposition\n",
    "from common.base_sa import SaRelatedComposition, Sa, SaSuperposition\n",
    "from common.int_sdr_encoder import IntSdrEncoder, IntRangeEncoder\n",
    "from common.int_sdr_encoder import SequenceSdrEncoder\n",
    "from envs.gridworld_pomdp import GridWorld\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "#%\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_description = [[2,0,0],\n",
    "                     [1,1,0],\n",
    "                     [0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(world_description, (3, 3), agent_initial_position={'row': 2, 'column': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 12\n",
    "\n",
    "state_encoder = SequenceSdrEncoder('state',\n",
    "                                   encoders=[IntSdrEncoder('distance',\n",
    "                                                                   gw.world_size[0],\n",
    "                                                                   5,\n",
    "                                                                   5),\n",
    "                                             IntSdrEncoder('surface', 3, 5, 5),\n",
    "                                             IntRangeEncoder('row', (-(gw.world_size[0]-1),\n",
    "                                                                     gw.world_size[1]-1), 5, 5),\n",
    "                                             IntRangeEncoder('column', (-(gw.world_size[0]-1),\n",
    "                                                                     gw.world_size[1]-1), 5, 5),\n",
    "                                             IntSdrEncoder('direction', 4, 5, 5)],\n",
    "                                   size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_encoder.value_bits)\n",
    "state_encoder.total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder = IntSdrEncoder('action', gw.n_actions,\n",
    "                              value_bits=5, activation_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits, sa_encoder.value_bits, sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TemporalMemory(n_columns=sa_encoder.total_bits,\n",
    "                    cells_per_column=16,\n",
    "                    activation_threshold=sa_encoder.value_bits,\n",
    "                    learning_threshold=sa_encoder.value_bits,\n",
    "                    initial_permanence=0.5,\n",
    "                    connected_permanence=0.5,\n",
    "                    maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                    maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                    permanenceIncrement=0.1,\n",
    "                    permanenceDecrement=0.005,\n",
    "                    predictedSegmentDecrement=0.0001\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(tm, sa_encoder, sa_encoder.format, format_sa_superposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = [\n",
    "    [2, 2, 1, 2, 2, 1, 2, 2],\n",
    "    [1, 0, 2, 2, 1, 2, 2, 1, 2, 1, 0, 2],\n",
    "    [2, 2, 1, 1, 2, 2, 0, 0],\n",
    "    [2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1],\n",
    "    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    [2, 2, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        ]\n",
    "actions = [2, 2, 1, 2, 2, 1, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем случайные пути и добавим один правильный путь. Промерим, сколько случайных путей нужно добавить,\n",
    "чтобы планирование сломалось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(pars):\n",
    "    tm = TemporalMemory(**pars)\n",
    "    memory = Memory(tm, sa_encoder, sa_encoder.format, format_sa_superposition)\n",
    "    return memory\n",
    "\n",
    "def learn_way(way, memory, environment, verbosity=1):\n",
    "    memory.reset()\n",
    "    state, reward, done = environment.reset(), 0, False\n",
    "    for action in way:\n",
    "        if verbosity > 1:\n",
    "            environment.render()\n",
    "            print(f'Action {action} State: {state}')\n",
    "        memory.train(Sa(state, action), verbosity)\n",
    "        state, _, _, info = environment.step(action)\n",
    "\n",
    "def check_agent(memory, environment, verbosity=1):\n",
    "    goal_state = (0, 2, -2, 1, 2)\n",
    "    planner = Planner(memory, 10, 1)\n",
    "    agent = Agent(memory, planner, environment.n_actions)\n",
    "    run = AgentRunner(agent, environment, 1, max_steps, 0, verbosity)\n",
    "    run.agent.planner.add_goal(goal_state)\n",
    "    run.agent.set_planning_horizon(10)\n",
    "    run.run()\n",
    "    if run.train_stats.rewards[-1] > 0:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_way(max_steps, n_actions):\n",
    "    return [randint(0, n_actions-1) for _ in range(max_steps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учим агент оптимальному пути, потом добавляем постепенно случайные пути"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = init_memory(pars=dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=8,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.5,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.005,\n",
    "                                predictedSegmentDecrement=0.0001))\n",
    "learn_way(actions, memory, gw)\n",
    "ways_history = list()\n",
    "ways_history.append(actions)\n",
    "\n",
    "while check_agent(memory, gw):\n",
    "    way = random_way(max_steps, gw.n_actions)\n",
    "    ways_history.append(way)\n",
    "    learn_way(way, memory, gw)\n",
    "\n",
    "print('Steps:', len(ways_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = check_agent(memory, gw, verbosity=-1)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_way_before_experiments(pars, n_experiments=3, verbosity=0):\n",
    "    results = list()\n",
    "    for _ in tqdm(range(n_experiments)):\n",
    "        memory = init_memory(pars)\n",
    "        learn_way(actions, memory, gw)\n",
    "        ways_history = list()\n",
    "        ways_history.append(actions)\n",
    "        while check_agent(memory, gw, verbosity=verbosity):\n",
    "            way = random_way(max_steps, gw.n_actions)\n",
    "            ways_history.append(way)\n",
    "            learn_way(way, memory, gw)\n",
    "        results.append({'steps': len(ways_history)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=32,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.5,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.005,\n",
    "                                predictedSegmentDecrement=0.0001)\n",
    "results = run_way_before_experiments(pars, n_experiments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results.steps.mean(), df_results.steps.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verbosity = 1\n",
    "for i, way in enumerate(ways):\n",
    "    if verbosity>1:\n",
    "        print()\n",
    "        print(f\"### Way {i+1} ###\")\n",
    "        print()\n",
    "    for i in range(2):\n",
    "        if verbosity>1:\n",
    "            print()\n",
    "            print(f'*** cycle {i+1} ***')\n",
    "            print()\n",
    "        memory.reset()\n",
    "        state, reward, done = gw.reset(), 0, False\n",
    "        for action in way:\n",
    "            if verbosity > 1:\n",
    "                gw.render()\n",
    "                print(f'Action {action} State: {state}')\n",
    "            memory.train(Sa(state, action), verbosity)\n",
    "            state, _, _, info = gw.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = (0, 2, -2, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, gw.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, gw, 1, max_steps, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_stats.rewards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.n_episodes = 50\n",
    "run.pretrain = 25\n",
    "run.verbosity = 1\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "steps = np.array(run.train_stats.steps[:])\n",
    "plt.plot(np.arange(steps.size), steps, '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
