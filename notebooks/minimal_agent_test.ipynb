{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_rl.agent.agent import Agent, AgentRunner\n",
    "from htm_rl.agent.memory import Memory, TemporalMemory\n",
    "from htm_rl.agent.planner import Planner\n",
    "from htm_rl.envs.mdp import Mdp\n",
    "from htm_rl.common.sa_sdr_encoder import SaSdrEncoder, SaSuperposition\n",
    "from htm_rl.common.int_sdr_encoder import IntSdrEncoder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def format_sa_superposition(sa_superposition: SaSuperposition) -> str:\n",
    "    \"\"\"\n",
    "    Formats SA superposition\n",
    "    \"\"\"\n",
    "    states = sa_superposition.state\n",
    "    actions = sa_superposition.action\n",
    "\n",
    "    format_ = 'SA superposition: \\n ------------------ \\n'\n",
    "\n",
    "    format_ += 'Possible states:\\n'\n",
    "    for state in states:\n",
    "        format_ += str(state) + ' '\n",
    "\n",
    "    format_ += '\\nPossible actions:\\n'\n",
    "    for action in actions:\n",
    "        format_ += str(action) + ' '\n",
    "\n",
    "    format_ += '\\n'\n",
    "\n",
    "    return format_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic example for standard MDP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = Mdp(transitions = {\n",
    "            0: {0: 4, 1: 1},\n",
    "            1: {0: 1, 1: 2},\n",
    "            2: {0: 2, 1: 3},\n",
    "            3: {0: 3, 1: 0},\n",
    "            4: None\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_encoder = IntSdrEncoder('state',\n",
    "                              n_values=mdp.n_states,\n",
    "                              value_bits=10,\n",
    "                              activation_threshold=7)\n",
    "\n",
    "\n",
    "action_encoder = IntSdrEncoder('action',\n",
    "                               n_values=mdp.n_actions,\n",
    "                               value_bits=10,\n",
    "                               activation_threshold=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits\n",
    "sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TemporalMemory(n_columns=sa_encoder.total_bits,\n",
    "                    cells_per_column=1,\n",
    "                    activation_threshold=sa_encoder.activation_threshold,\n",
    "                    learning_threshold=action_encoder.activation_threshold,\n",
    "                    initial_permanence=0.49,\n",
    "                    connected_permanence=0.5,\n",
    "                    maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                    maxSynapsesPerSegment=sa_encoder.value_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(tm, sa_encoder, sa_encoder.format, format_sa_superposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory,\n",
    "                  planning_horizon=10,\n",
    "                  goal_memory_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, mdp.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, mdp,\n",
    "                  n_episodes=100,\n",
    "                  max_steps=10,\n",
    "                  pretrain=50,\n",
    "                  verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "steps = np.array(run.train_stats.steps)\n",
    "plt.plot(np.arange(steps.size), steps, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
