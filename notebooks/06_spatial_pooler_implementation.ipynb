{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Spatial Pooler\n",
    "\n",
    "Данная тетрадь содержит задачу реализации Spatial Pooler'а.\n",
    "\n",
    "Для начала посмотри эпизоды 0-8 видео гайда [HTMSchool](https://www.youtube.com/watch?v=XMB0ri4qgwc&list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Getting ready\n",
    "\n",
    "Данная секция содержит:\n",
    "\n",
    "- [опционально] установка `htm.core`\n",
    "- импорт необходимых пакетов (убедись, что все они установлены)\n",
    "- загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTM.Core\n",
    "\n",
    "Если у тебя не установлен пакет `htm.core`, раскомментируй и запусти следующую ячейку. В случае проблем, обратись к официальной странице пакета на [гитхабе](https://github.com/htm-community/htm.core#installation) и проверь требуемые зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -i https://test.pypi.org/simple/ htm.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import numpy as np\n",
    "# import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "# from time import sleep\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Следующая ячейка загружает датасет MNIST (займет порядка 10-20 сек)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name, num_test, shape=None):\n",
    "    \"\"\" \n",
    "    fetch dataset from openML.org and split to train/test\n",
    "    @param name - ID on openML (eg. 'mnist_784')\n",
    "    @param num_test - num. samples to take as test\n",
    "    @param shape - new reshape of a single data point (ie data['data'][0]) as a list. Eg. [28,28] for MNIST\n",
    "    \"\"\"\n",
    "    data = fetch_openml(name, version=1)\n",
    "    sz=data['target'].shape[0]\n",
    "\n",
    "    X = data['data']\n",
    "    if shape is not None:\n",
    "        new_shape = shape.insert(0, sz)\n",
    "        X = np.reshape(X, shape)\n",
    "\n",
    "    y = data['target'].astype(np.int32)\n",
    "    # split to train/test data\n",
    "    train_labels = y[:sz-num_test]\n",
    "    train_images = X[:sz-num_test]\n",
    "    test_labels  = y[sz-num_test:]\n",
    "    test_images  = X[sz-num_test:]\n",
    "\n",
    "    return train_labels, train_images, test_labels, test_images\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    return x[indices], y[indices]\n",
    "\n",
    "\n",
    "train_labels, train_images, test_labels, test_images = load_ds('mnist_784', 10000, shape=[28,28])\n",
    "\n",
    "np.random.seed(seed)\n",
    "train_images, train_labels = shuffle_data(train_images, train_labels)\n",
    "test_images, test_labels = shuffle_data(test_images, test_labels)\n",
    "\n",
    "n_train_samples = train_images.shape[0]\n",
    "n_test_samples = test_images.shape[0]\n",
    "image_shape = train_images[0].shape\n",
    "image_side = image_shape[0]\n",
    "image_size = image_side ** 2\n",
    "\n",
    "\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример формата данных датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])\n",
    "print(f'Label: {train_labels[0]}')\n",
    "print(f'Image shape: {image_shape}')\n",
    "print(f'Image middle row: {train_images[0][image_side//2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем датасет в бинарные изображения и дальше будем работать с бинарными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatten_image(flatten_image, image_height):\n",
    "    plt.imshow(flatten_image.reshape((image_height, -1)))\n",
    "\n",
    "def to_binary_flatten_images(images, n_samples):\n",
    "    # flatten every image to vector\n",
    "    images = images.reshape((n_samples, -1))\n",
    "    # binary encoding: each image pixel is encoded either 0 or 1 depending on that image mean value\n",
    "    images = (images >= images.mean(axis=1, keepdims=True)).astype(np.int8)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = to_binary_flatten_images(train_images)\n",
    "test_images = to_binary_flatten_images(test_images)\n",
    "plot_flatten_image(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Baseline: classifier on raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def test_bare_classification(x_tr,  y_tr, x_tst, y_tst):\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(x_tr, y_tr)\n",
    "    \n",
    "    score = linreg.predict(flatten_images(x_tst)) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '%')\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "test_bare_classification(x_tr, y_tr, x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Spatial Pooler: skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractSpatialPooler:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        return np.nonzero(sdr)[0]\n",
    "    \n",
    "    def _update_permanence(self, sdr, rows, cols):\n",
    "        ...\n",
    "        \n",
    "    def _update_activity_boost(self, rows, cols):\n",
    "        ...\n",
    "        \n",
    "    def _update_overlap_boost(self, sdr, rows, cols, overlaps):\n",
    "        ...\n",
    "        \n",
    "\n",
    "np.random.seed(seed)\n",
    "sp = AbstractSpatialPooler(train_images[0].shape, train_images[0].shape)\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size)\n",
    "assert sparse_sdr.size < sp.output_size\n",
    "sparse_sdr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Train/test SP performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def pretrain_sp(sp, images, n_samples):\n",
    "    for img in images[:n_samples]:\n",
    "        sp.compute(img, True)\n",
    "    \n",
    "def encode_to_csr_with_sp(images, sp, learn):\n",
    "    flatten_encoded_sdrs = []\n",
    "    indptr = [0]\n",
    "    for img in images:\n",
    "        encoded_sparse_sdr = sp.compute(img, learn)\n",
    "        flatten_encoded_sdrs.extend(encoded_sparse_sdr)\n",
    "        indptr.append(len(flatten_encoded_sdrs))\n",
    "\n",
    "    data = np.ones(len(flatten_encoded_sdrs))\n",
    "    csr = csr_matrix((data, flatten_encoded_sdrs, indptr), shape=(images.shape[0], sp.output_size))\n",
    "    return csr\n",
    "\n",
    "def test_classification_with_sp(x_tr,  y_tr, x_tst, y_tst, sp):\n",
    "    enc = SDR(sp.input_shape)\n",
    "    columns = SDR(sp.output_shape)\n",
    "\n",
    "    # a small pretrain SP before real work\n",
    "    pretrain_sp(sp, x_tr, n_samples=1000)\n",
    "    \n",
    "    # encode images and continuously train SP\n",
    "    csr = encode_to_csr_with_sp(x_tr, sp, learn=True)\n",
    "    \n",
    "    # train linreg\n",
    "    linreg = LogisticRegression(tol=.001, max_iter=100, multi_class='multinomial', penalty='l2', solver='lbfgs', n_jobs=3)\n",
    "    linreg.fit(csr, y_tr)\n",
    "    \n",
    "    # encode test images (without SP learning) and then test score\n",
    "    csr = encode_to_csr_with_sp(x_tst, sp, False)\n",
    "    score = linreg.predict(csr) == y_tst\n",
    "    score = score.mean()\n",
    "    print('Score:', 100 * score, '% for n =', len(x_tr))\n",
    "    return score\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "my_sp = AbstractSpatialPooler(train_images[0].shape, train_images[0].shape)\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, my_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Spatial Pooler: learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSpatialPooler(AbstractSpatialPooler):\n",
    "    def __init__(\n",
    "        self, input_shape, output_shape, \n",
    "        permanence_threshold, sparsity_level, synapse_permanence_deltas, min_activation_threshold\n",
    "    ):\n",
    "        super().__init__(input_shape, output_shape)\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        # todo\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.synapse_permanence_increment, self.synapse_permanence_decrement = synapse_permanence_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        # initialization\n",
    "        # todo all\n",
    "        self.joint_shape = output_shape + input_shape\n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[.2, .8])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        \n",
    "        # remove\n",
    "        self.dp = np.empty(input_shape, dtype=np.float)\n",
    "        \n",
    "    def compute(self, dense_sdr, learn):\n",
    "        dense_sdr = dense_sdr.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, :, dense_sdr] >= self.permanence_threshold\n",
    "        overlaps = (np.count_nonzero(active_cells, -1)).ravel()\n",
    "        activated_indices = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_indices = activated_indices[overlaps[activated_indices] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            rows, cols = np.unravel_index(activated_indices, self.output_shape)\n",
    "            self._update_permanence(dense_sdr, rows, cols)\n",
    "\n",
    "        return activated_indices\n",
    "    \n",
    "    def _update_permanence(self, sdr, rows, cols):\n",
    "        dp = self.dp\n",
    "        dp[sdr] = self.synapse_permanence_increment\n",
    "        dp[~sdr] = -self.synapse_permanence_decrement\n",
    "        perm = self.connections_permanence[rows, cols]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[rows, cols], 0, 1)\n",
    "\n",
    "        \n",
    "np.random.seed(seed)\n",
    "sp = BasicSpatialPooler(\n",
    "    train_images[0].shape, (10, 10),\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .02),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "sparse_sdr = sp.compute(train_images[0], True)\n",
    "\n",
    "print(sparse_sdr.size, sp.output_size, sp.n_active_bits)\n",
    "assert sparse_sdr.size == sp.n_active_bits\n",
    "sparse_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 1000\n",
    "x_tr, y_tr = train_images[:n], train_labels[:n]\n",
    "x_tst, y_tst = test_images[:n], test_labels[:n]\n",
    "\n",
    "sp = BasicSpatialPooler(\n",
    "    train_images[0].shape, (30, 30),\n",
    "    permanence_threshold=.5,\n",
    "    sparsity_level=.04,\n",
    "    synapse_permanence_deltas=(.1, .03),\n",
    "    min_activation_threshold=4\n",
    ")\n",
    "test_classification_with_sp(x_tr, y_tr, x_tst, y_tst, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img(img):\n",
    "    return (img >= img.mean()).astype(np.int8)\n",
    "\n",
    "sample = train_images[0]\n",
    "sample = encode_img(sample)\n",
    "\n",
    "class MySpatialPooler:\n",
    "    def __init__(self, input_shape, output_shape, permanence_threshold, sparsity_level, syn_perm_deltas, min_activation_threshold=1, max_boost_factor=1.5, boost_sliding_window=(1000, 1000)):\n",
    "        assert isinstance(input_shape, tuple) and isinstance(output_shape, tuple)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.joint_shape = output_shape + input_shape\n",
    "        self.output_size = output_shape[0] * output_shape[1]\n",
    "        \n",
    "        self.sparsity_level = sparsity_level\n",
    "        self.n_active_bits = int(self.output_size * sparsity_level)\n",
    "        \n",
    "        self.permanence_threshold = permanence_threshold\n",
    "        self.syn_perm_inc, self.syn_perm_dec = syn_perm_deltas\n",
    "        self.min_activation_threshold = min_activation_threshold\n",
    "        \n",
    "        self.max_boost_factor = max_boost_factor\n",
    "        self.activity_duty_cycle, self.overlap_duty_cycle = boost_sliding_window\n",
    "        \n",
    "        # init \n",
    "        self.receptive_fields = np.random.choice(2, size=self.joint_shape, p=[.2, .8])\n",
    "        self.connections_permanence = np.random.uniform(size=self.joint_shape) * self.receptive_fields\n",
    "        self.time_avg_activity = np.full(self.output_shape, self.sparsity_level, dtype=np.float)\n",
    "        self.time_avg_overlap = np.ones(self.output_shape, dtype=np.float)\n",
    "        self.dp = np.empty(input_shape, dtype=np.float)\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def compute(self, x, learn):\n",
    "        x = x.astype(np.bool)\n",
    "        active_cells = self.connections_permanence[:, :, x] >= self.permanence_threshold\n",
    "        overlaps = (np.count_nonzero(active_cells, -1) * self.boost).ravel()\n",
    "        activated_indices = np.argpartition(-overlaps, self.n_active_bits)[:self.n_active_bits]\n",
    "        activated_indices = activated_indices[overlaps[activated_indices] >= self.min_activation_threshold]\n",
    "        \n",
    "        if learn:\n",
    "            rows, cols = np.unravel_index(activated_indices, self.output_shape)\n",
    "            self._update_permanence(x, rows, cols)\n",
    "            self._update_activity_boost(rows, cols)\n",
    "#             self._update_overlap_boost(x, rows, cols, overlaps)\n",
    "\n",
    "        return activated_indices\n",
    "    \n",
    "    def _update_permanence(self, x, rows, cols):\n",
    "        dp = self.dp\n",
    "        dp[x] = self.syn_perm_inc\n",
    "        dp[~x] = -self.syn_perm_dec\n",
    "        perm = self.connections_permanence[rows, cols]\n",
    "        perm = np.clip(perm + dp * self.receptive_fields[rows, cols], 0, 1)\n",
    "        \n",
    "    def _update_activity_boost(self, rows, cols):\n",
    "        self.time_avg_activity *= (self.activity_duty_cycle - 1) / self.activity_duty_cycle\n",
    "        self.time_avg_activity[rows, cols] += 1 / self.activity_duty_cycle\n",
    "        self.boost = self._compute_boost()\n",
    "        \n",
    "    def _update_overlap_boost(self, x, rows, cols, overlaps):\n",
    "        self.time_avg_overlap += (overlaps.reshape(self.output_shape) - self.time_avg_overlap) / self.overlap_duty_cycle\n",
    "        k = int(.05 * self.output_size)\n",
    "        to_boost_indices = np.argpartition(self.time_avg_overlap.ravel(), k)[:k]\n",
    "        to_boost_indices = np.unravel_index(to_boost_indices, self.output_shape)\n",
    "        to_boost = self.connections_permanence[to_boost_indices]\n",
    "        to_boost = np.clip(to_boost + .1 * self.permanence_threshold, 0, 1)\n",
    "        \n",
    "    def _compute_boost(self):\n",
    "        return np.exp(-self.max_boost_factor * (self.time_avg_activity - self.time_avg_activity.mean()))\n",
    "        \n",
    "\n",
    "np.random.seed(1337)\n",
    "my_sp = MySpatialPooler(train_images[0].shape, (10, 10), .5, .04, (.1, .02), 4)\n",
    "my_sp.compute(sample, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
