{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_rl.agent.agent import Agent, AgentRunner\n",
    "from htm_rl.agent.memory import Memory, TemporalMemory\n",
    "from htm_rl.agent.planner import Planner\n",
    "from htm_rl.common.sa_sdr_encoder import SaSdrEncoder, format_sa_superposition\n",
    "from htm_rl.common.base_sa import SaRelatedComposition, Sa, SaSuperposition\n",
    "from htm_rl.common.int_sdr_encoder import IntSdrEncoder, IntRangeEncoder\n",
    "from htm_rl.common.int_sdr_encoder import SequenceSdrEncoder\n",
    "from htm_rl.envs.gridworld_pomdp import GridWorld\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(pars, sa_encoder, start_indicator=None, output_file=None):\n",
    "    tm = TemporalMemory(**pars)\n",
    "    memory = Memory(tm, sa_encoder, sa_encoder.format, format_sa_superposition,\n",
    "                    start_indicator=start_indicator, output_file=output_file)\n",
    "    return memory\n",
    "\n",
    "def learn_way(way, memory, environment, verbosity=1):\n",
    "    memory.reset()\n",
    "    state, reward, done = environment.reset(), 0, False\n",
    "    for action in way:\n",
    "        if verbosity > 1:\n",
    "            environment.render()\n",
    "            print(f'Action {action} State: {state}')\n",
    "        memory.train(Sa(state, action), verbosity)\n",
    "        state, _, _, info = environment.step(action)\n",
    "\n",
    "def check_agent(memory, environment, goal_state, planning_horizon=10, max_steps=12, verbosity=1):\n",
    "    planner = Planner(memory, planning_horizon, 1)\n",
    "    agent = Agent(memory, planner, environment.n_actions)\n",
    "    run = AgentRunner(agent, environment, 1, max_steps, 0, verbosity)\n",
    "    run.agent.planner.add_goal(goal_state)\n",
    "    run.agent.set_planning_horizon(planning_horizon)\n",
    "    run.run()\n",
    "    if run.train_stats.rewards[-1] > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_way(max_steps, n_actions):\n",
    "    return [randint(0, n_actions-1) for _ in range(max_steps)]\n",
    "\n",
    "def run_way_after_experiments(pars, sa_encoder, goal_state,\n",
    "                              start_indicator=None,\n",
    "                              n_experiments=3,\n",
    "                              verbosity=0,\n",
    "                              learning_true_count=1,\n",
    "                              max_noise_steps=1000,\n",
    "                              step_size=1,\n",
    "                              save_bad_way=False,\n",
    "                              save_ways=False,\n",
    "                              path=None,\n",
    "                              actions=[2, 2, 1, 2, 2, 1, 2],\n",
    "                              noise_ways=None,\n",
    "                              max_steps=12,\n",
    "                              planning_horizon=10):\n",
    "    results = list()\n",
    "    bad_ways_history = {'way': [], 'steps': [], 'experiment': []}\n",
    "    ways_history = None\n",
    "    for experiment in tqdm(range(n_experiments)):\n",
    "        n_steps = 0 # number of noise action sequences\n",
    "        while True:\n",
    "            if path is not None:\n",
    "                output_file = path + f'_exp_{experiment}_n_steps_{n_steps}.pkl'\n",
    "            else:\n",
    "                output_file = None\n",
    "            memory = init_memory(pars, sa_encoder, start_indicator=start_indicator, output_file=output_file)\n",
    "            way = None\n",
    "            ways_history = list()\n",
    "            if noise_ways is None:\n",
    "                for step in range(n_steps):\n",
    "                    way = random_way(max_steps, gw.n_actions)\n",
    "                    learn_way(way, memory, gw)\n",
    "                    if save_ways:\n",
    "                        ways_history.append(way)\n",
    "            else:\n",
    "                for step in range(n_steps):\n",
    "                    if step >= len(noise_ways):\n",
    "                        break\n",
    "                    learn_way(noise_ways[step], memory, gw)\n",
    "\n",
    "            for _ in range(learning_true_count):\n",
    "                learn_way(actions, memory, gw)\n",
    "                if save_ways:\n",
    "                    ways_history.append(actions)\n",
    "\n",
    "            if not check_agent(memory, gw, goal_state,\n",
    "                               max_steps=max_steps,\n",
    "                               planning_horizon=planning_horizon,\n",
    "                               verbosity=verbosity):\n",
    "                if way is not None and save_bad_way:\n",
    "                    bad_ways_history['way'].append(str(way))\n",
    "                    bad_ways_history['steps'].append(n_steps)\n",
    "                    bad_ways_history['experiment'].append(experiment)\n",
    "                break\n",
    "            else:\n",
    "                n_steps += step_size\n",
    "                if n_steps > max_noise_steps:\n",
    "                    print('Max steps are reached')\n",
    "                    break\n",
    "        results.append(n_steps)\n",
    "    return results, bad_ways_history, ways_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 12\n",
    "actions = [2, 2, 1, 2, 2, 1, 2, 2]\n",
    "world_description = [[2,0,0],\n",
    "                     [1,1,0],\n",
    "                     [0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(world_description, (3, 3), agent_initial_position={'row': 2, 'column': 0},\n",
    "               observable_vars=['distance', 'surface'], one_value_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.observable_state, gw.filtered_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_bits = 10\n",
    "action_bits = 10\n",
    "\n",
    "state_encoder = IntSdrEncoder('state', gw.n_obs_states+1, state_bits, state_bits-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_encoder = IntSdrEncoder('action', gw.n_actions+1,\n",
    "                              value_bits=action_bits, activation_threshold=action_bits-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder = SaSdrEncoder(state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_encoder.total_bits, sa_encoder.value_bits, sa_encoder.activation_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state = 6\n",
    "goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(n_columns=sa_encoder.total_bits,\n",
    "                                cells_per_column=32,\n",
    "                                activation_threshold=sa_encoder.value_bits,\n",
    "                                learning_threshold=sa_encoder.value_bits,\n",
    "                                initial_permanence=0.51,\n",
    "                                connected_permanence=0.5,\n",
    "                                maxNewSynapseCount=sa_encoder.value_bits,\n",
    "                                maxSynapsesPerSegment=sa_encoder.value_bits,\n",
    "                                permanenceIncrement=0.1,\n",
    "                                permanenceDecrement=0.05,\n",
    "                                predictedSegmentDecrement=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = init_memory(pars, sa_encoder, start_indicator=Sa(gw.n_obs_states, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Planner(memory, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(memory, planner, gw.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = AgentRunner(agent, gw, 1000, max_steps, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.agent.planner.add_goal(\n",
    "    goal_state)\n",
    "run.agent.set_planning_horizon(10)\n",
    "run.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_stats.rewards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
