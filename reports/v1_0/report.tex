\documentclass[a4paper]{article}
\usepackage{style}

\begin{document}

\title {!TBD! HTM + RL: Project details}
\author {Petr Kuderov}
\maketitle

\section{TODO}

\begin{itemize}
  \item Убрать упоминание о разновидности гридворлда с учетом направления взгляда агента.
  \item Примеры сред перегенерировать с белой пустотой и черными стенами
  \item Блоки-сноски, помеченные NB, можно сделать в виде нормальных сносок.
  \item Правильно проставить double quote marks через ``''
\end{itemize}

\section {Постановка задачи и среда}

Рассматривается классическая задача MDP - агент играет в детерминированной среде GridWorld, которая выглядит как лабиринт на квадратной сетке. Агент начинает игру в фиксированной точке старта, и ему требуется найти единственную награду, которая находится в фиксированном месте.

Агент может перемещаться в любую из соседних клеток, пронумерованных против часовой стрелки, начиная с востока: восток, север, запад, юг.

Когда агент пытается шагнуть "в стенку", он остается на месте. Переходы между состояниями однозначны - среда является детерминированной MDP. На каждом невыигрышном шаге агент получает небольшую отрицательную награду $-0.01$, на выигрышном - большую положительную $+1$.

\begin{framed}
  Изначально рассматривался еще один вариант GridWorld - с учетом направления взгляда агента.

  В этом случае в каждый момент времени агент находится в некоторой клетке лабиринта, указывая лицом одно из 4х направлений: восток, север, запад, юг. Доступны следующие действия: шагнуть прямо (в соседнюю клетку), повернуться на 90 градусов по или против часовой стрелки. В некоторых экспериментах одно из направлений поворота - по часовой стрелке - было запрещено.

  Такой вариант среды имеет некоторые, иногда предпочтительные, особенности: а) минимальное пространство действий (можно обойтись двумя действиями), б) усложнение задачи поиска награды в минимальных средах из-за более длинной последовательности действий до неё. Последнее удобно использовать для сравнения работы агента со случайной стратегией, т.к. даже в маленьких лабиринтах случайная стратегия выдает очень плохие результаты, и разница может быть хорошо заметна. Это в свою очередь важно на ранних быстрых циклах экспериментов.
\end{framed}

\section{Агент}

В текущей реализации стратегия агента представляет собой смесь из двух стратегий: случайной и спланированной. Спланированную стратегию предоставляет планировщик. В случае, если он не может её предоставить, используется случайная.

Основная логика агента содержится именно в реализации планировщика.

Задача планировщика - найти путь до награды, и этот путь не должен быть длиннее, чем \verb|planning_horizon| шагов. Последнее в свою очередь задает горизонт планирования.

Процесс планирования разделен на два этапа:

\begin{enumerate}
    \item Планирование до награды из текущего состояния.
    \item Нахождение обратного пути из состояния с наградой в текущее.
\end{enumerate}

\begin{function}
  \SetAlgoNoLine
  $s_0$: initial state \;
  $g: $ reached goals \;
  $act\_seg: $ active segments timeline \;
  \quad \\

  $g, act\_seg \leftarrow PredictToGoal(s_0)$ \;
  $plan \leftarrow BacktrackFromGoal(g, s_0, act\_seg)$ \;

  \Return{$[a_0, a_1, .. a_T] = plan$}
  \caption{PlanActions($s_0$)}
\end{function}

Чтобы агент мог планировать, он наделен памятью для запоминания увиденных переходов между состояниями, которая эмулирует (или аппроксимирует) функцию динамики среды $f: (s, a) \rightarrow s'$. Память агента имеет некоторые особенности.

\subsection{Особенности устройства памяти агента}

TODO поменять и разделить на 1) работает с SDR, их можно пересекать и объединять, 2) память устроена так, что не может, а работает с суперпозициями. Возможно еще стоит пока временно опустить слово суперпозиция. Кажется, что семантика отличия от объединения - в том, что объединяются именно цельные сущности.

Главная особенность работы памяти агента - способность работать с объединениями "воспоминаний". Это означает, что память может отвечать на запросы вида \textit{имея набор состояний $S=\{s_i\}$ и набор действий $A=\{a_j\}$, в какие состояния $S'=\{s'_m\}$ могут привести все возможные пары $(s_i, a_j)$}? Ответом на такой запрос является объединение состояний в следующий момент времени $S' = \cup s'_m$.

Вторая особенность - в ответе на запрос к памяти для каждого конкретного состояния $s'$ из множества предсказанных состояний $S'$ есть возможность узнать, какие пары состояние-действие приводят в $s'$: $\{(s, a)\} = f^{-1}(s')$. Обратите внимание, в общем случае в $s'$ может вести не единственная пара состояния-действия.

Третья особенность - память агента дополнительно снабжена подмодулем, запоминающим выигрышные состояния. На текущий момент этот подмодуль реализован с помощью наивного списка состояний, в которых агент получил положительную награду - мы называем этот подмодуль списком целей. Список целей имеет ограниченный размер, являющийся гиперпараметром, и работает как кольцевая очередь.

На основе этих особенностей и построен алгоритм работы планировщика.

\subsection{TM в качестве памяти агента}

Перед тем как перейти к описанию алгоритма планирования, опишем для начала, какими данными оперирует агент, и в каком формате они представлены.

Пусть трактория движения агента - $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, ...)$. Тогда пара состояние-действие $(s_t, a_t)$ некоторого момента времени задает предусловие перехода в среде: $(s_t, a_t) \rightarrow s_{t+1}$.

Память агента - это TM, которая запоминает последовательности вида $[(s_0, a_0), (s_1, a_1), \dots]$, сделанные на основе отыгранных траекторий в среде. Сама по себе TM работает с разреженными векторами (SDR), а значит пары состояние-действие перед подачей в ТМ требуется как-то закодировать (см. ссылка на подсекцию).

Обратите внимание, что TM в нашем случае учит
\begin{itemize}
  \item и переходы $(s, a) \rightarrow s'$,
  \item и какие действия $\{a^i_{t+1}\}$ совершал агент, попав в некоторое состояние $s_{t+1}$.
\end{itemize}

Обученная TM умеет делать предсказания - для текущей пары состояние-действие $(s_t, a_t)$ давать совокупность всех возможных продолжений $(s_{t+1}, a_{t+1})$. "Совокупность" выражается как побитовое объединение разреженных векторов, соответствующих этим парам. Такую совокупность мы называем суперпозицией состояний-действий. Cам же процесс предсказания называется деполяризацией и является естественным этапом цикла работы ТМ: активация-обучение-деполяризация-обучение-активация-обучение-деполяризация-\dots

\begin{figure}
  \includegraphics[width=1.\textwidth]{assets/memory_scheme.jpg}
  \caption{TODO Набросок схемы.}
\end{figure}

Стоит отметить, что вектор, являющийся суперпозицией пар состояние-действие, тоже является валидным входным значением для ее активации [, означая совокупность составляющих этой суперпозиции]. Эта особенность используется в процессе одного из шагов планирования.

\subsubsection{Кодирование состояний и действий}

В рамках рассматриваемых MDP все состояния $s \in S$ и действия $a \in A$ пронумерованы целыми числами в интервалах $[0, |S|)$ и $[0, |A|)$ соответственно. Каждый элемент пары состояние-действие кодируется независимо целочисленным кодировщиком - для состояний и действий он разный. Далее два полученных SDR вектора конкатенируются в итоговый SDR вектор.

Целочисленный кодировщик работает с числами некоторого фиксированного интервала $[0, N)$ и каждому числу интервала ставит в соответствие одинаковое количество активных бит. Принцип его работы довольно очевиден и его проще показать на примере. Возьмем для этой случай $N = 3$. Вот как кодируются числа 0, 1 и 2 соответственно:

\begin{itemize}
  \item $0 \rightarrow$ \verb|1111 0000 0000|
  \item $1 \rightarrow$ \verb|0000 1111 0000|
  \item $2 \rightarrow$ \verb|0000 0000 1111|
\end{itemize}

На примере видно, что итоговый вектор разбивается на $N$ групп одинаковой длины (buckets), и каждая группа активных бит соответствует только одному из значений интервала. Длина одной группы является гиперпараметром (в примере выше длина равна 4).

Такой кодировщик обладает следующими свойствами:

\begin{itemize}
  \item Плюсы
  \begin{itemize}
    \item простой и быстрый алгоритм кодирования/декодирования
    \item закодированные значения легкочитаемы во время дебага
    \item разные значения не пересекаются, упрощая некоторые допущения в алгоритмах
  \end{itemize}

  \item Минусы
  \begin{itemize}
    \item разные значения не пересекаются, так что с точки зрения меры "близости" все значения не имеют ничего общего между собой
    \item нет контроля над разреженностью закодированного вектора - она всегда равна $\frac{1}{N}$ и может оказаться вне удачных диапазонов. На протестированных нами примерах с низкой разреженностью ТМ показала себя хорошо. Тем не менее стоит помнить, что это потенциально проблемное место.
  \end{itemize}
\end{itemize}

TODO упомянуть про кодирование состояний в сложных средах или выбросить следующий абзац на английском. State SDR encoder

- For simple MDP environments we use integer encoding
- For complex environments encoding states [or observations] becomes tricky
  - of course, you still can enumerate all possible states and use Integer SDR encoder
  - but the number of all possible states grows very fast
    - in practice it works well only for small environments
  - also Integer SDR encoder cuts out information about states similarity, which may not be desirable
  - one possible solution is to encode every pixel [or grid cell] separately then concatenate results, which preserves information about similarity between states
- At the moment we use only simple MDP environments and hence integer encoding for states

TODO упомянуть про OR суперпозицию в виде примера. Пример суперпозиции двух векторов, кодирующих числа 2 и 5 отрезка [1, 6]:

\begin{verbatim}
  0000 1111 0000 0000 1111 0000
  =
  0000 1111 0000 0000 0000 0000
  OR
  0000 0000 0000 0000 1111 0000
\end{verbatim}

\subsection{Планировщик. Этап 1: планирование до цели}

TODO proofread не забыл ли я что-то из англ версии

Агент опирается на свой опыт и исходит из предположения, что награда может находиться в одном из выигрышных состояний предыдущих эпизодов. Но даже если и нет, то само по себе направленное перемещение будет выгоднее случайного блуждания с точки зрения исследования среды. Поэтому состояния из списка целей являются приоритетными.

Пусть в некоторый момент времени $t_0$ агент находится в состоянии $s_0$. Чтобы ответить на вопрос, есть ли достижимое состояние из списка целей в радиусе $N$ действий, он начинает планировать из суперпозиции $(S_0 = s_0, A = \cup a^i)$, где $A$ - все множество доступных действий агента.

TODO рисунок и аналогия с BFS

После первого шага предсказания планировщик имеет множество состояний $S_1 = \{s^i\}$, в которые можно попасть из текущего за одно действие агента. Далее он продолжает планировать уже из суперпозиции $S_1$, т.е. попросту подав всю предсказанную на первом шаге суперпозицию на вход своей ТМ. Такой процесс предсказания и подачи всего предсказанного обратно на вход ТМ продолжается до тех пор, пока либо не будет обнаружено хотя бы одно состояние из списка целей, либо не будет исчерпан лимит горизонта планирования $N =$ \verb|planning_horizon|.

Если цель найдена, то планировщик переходит к следующему этапу. В обратном случае планирование признается безуспешным, и агенту в текущем ходе остается воспользоваться случайной стратегией.

В процессе проведения этого этапа планировщик на каждом шаге сохраняет информацию об активных суперпозициях состояний, формируя их историю $[S_0, S_1, .. S_T]$. Более детально - для каждого шага сохраняются активации сегментов ячеек ТМ (TODO линк на что такое сегменты ТМ далее). Это позволяет планировщику не потерять информацию о зависимостях между переходами от одного шага к следующему - какие пары состояний-действий $\{(s, a)\}$ повлекли за собой активацию каких пар $\{(s', a')\}$.

TODO check different code styles
\begin{function}
  % \SetAlgoNoLine
  \KwData{initial state $s_0$}
  $sa_0$: state-action pair \;
  $p_i$: proximal input \;
  $act\_seg$: active segments timeline \;
  \quad \\

  $sa_0 \leftarrow (s_0, \cup a^i)$ \;
  $p_0 \leftarrow Encode(sa_0)$ \;

  \ForEach {backward step $i$} {
    $ActivateDepolarize(p_i)$ \tcc{Activate and depolarize TM cells}
    
    $p_{i+1} \leftarrow ColumnsFromCells(depolarized\_cells)$ \tcc{Set proximal input for next step}
    
    $act\_seg[i] \leftarrow active\_segments$ \tcc{Record active segments for $i$-th step}
    \quad \\
    
    $s, a \leftarrow Decode(p_{i+1})$ \;
    $reached\_goal \leftarrow MatchGoals(s)$ \;
    \If(){$reached\_goal \neq \emptyset$} {
      \textbf{break}
    }{}
  }
  
  \Return {$reached\_goal, act\_seg$} \;

  \caption{PredictForward(initial state $s_0$)}
\end{function}

\textbf{NB}: первый этап может обнаружить сразу несколько достижимых целевых состояний в радиусе $T$ шагов. Выбирается какое-то одно из них, причем процедура выбора в текущей реализации явно не определена.

\subsection{Планировщик. Этап 2: нахождение обратного пути}

Итак, на момент окончания первого этапа планировщик "верит", что некоторое целевое состояние $g = s_T$ может быть достигнуто за $T$ шагов, но не знает в точности как. Логичным образом цель второго этапа - восстановить 
точную последовательность действий, приводящих из начального состояния $s_0$ в целевое $s_T$.

Решать эту задачу предлагается с конца - двигаясь по графу истории активаций сегментов из конечного целевого состояния назад во времени, с целью попасть в начальное состояние на нулевом шаге. В связи с таким "движением обратно", данный этот этап назван бэктрекингом.

TODO рисунок-схема

\begin{function}
  \SetAlgoNoLine
  \KwData{$g = s_T$ - goal state; $act\_seg$ - active segments timeline}
  \quad \\

  $S_T$: state superposition at time $T$ \;
  $s_T \leftarrow s_T \cap S_T$ \;
  $(successful, actions) \leftarrow Backtrack(s_T, T-1, act\_seg)$ \;
  \Return {$successful, actions$}
  \caption{BacktrackFromGoal($g$, $act\_seg$)}
\end{function}

\subsubsection{Бэктрекинг: упрощенно}

Сперва рассмотрим алгоритм бэктрекинга в немного упрощенном виде на уровне идеи, а в следующей секции будут описаны детали реализации.

Напомним, что в процессе проведения первого шага сохраняется информация о суперпозициях состояний на каждом из этих шагов $[S_0, S_1, .. S_T]$. Также работа памяти планировщика устроена таким образом, что позволяет не только предсказывать следующее состояние $s' = f(s, a)$ на основе аппроксимированной функции перехода среды, но и обратно по целевому состоянию $s'$ определить пару состояние-действие $(s, a) = f^{-1}(s')$, которая привела к нему.

Получается, имея конечное целевое состояние $g = s_T$, планировщик может определить, какая пара состояние-действие $(s_{T-1}, a_{T-1})$ привела в него. Следовательно, для шага $T-1$ известно требуемое действие $a_{T-1}$ и состояние $s_{T-1}$, в которое нужно прийти, а значит задача поиска пути из $s_0$ в $s_T$ рекурсивно сводится к задаче поиска пути в $s_{T-1}$.

Повторяя процедуру, на каждом шаге $t$ находим пару состояние-действие $(s_{t-1}, a_{t-1}) = f^{-1}(s_t)$, пока не вернемся в начальную позицию $s_0$.

Стоит уточнить, что $f^{-1}(s')$ в общем случае возвращает не единственную пару $(s, a)$, а множество пар, каждая из которых удовлетворяет условию $s' = f(s, a)$. Однако не всякая пара рекурсивно приведет нас обратно в начальную позицию именно в начальй момент времени, т.е. реально достижима из начальной позиции за нужное число шагов. Поэтому рекурсивно проверяется каждая из пар-претендентов до первого успешного возвращения в $s_0$.

В результате работы бэктрекинга формируется однозначная последовательность действий $[a_0, .. , a_{T-1}]$, которая с точки зрения аппроксимированной функции перехода приведет агента из начального состояния $s_0$ в искомое целевое состояние $g = s_T$.

TODO high-level backtracking algo

Если процесс нахождения обратного пути завершился успешно, планировщик имеет политику действий на $T$ шагов вперед, которую агент безусловно выполняет. Если после исполнения плана он находит награду, эпизод завершится, иначе агент исключает рассмотренное целевое состояние из списка целей до окончания текущего эпизода и пытается заново спланировать действия.

\subsubsection{Бэктрекинг: детали реализации}

TODO эту часть возможно стоит наверх туда, где описана работа памяти планировщика.

Чтобы перейти к деталям бэктрекинга, напомним принцип работы TM планировщика. Она работает с последовательностями пар состояние-действие. Пары состояние-действие кодируются в SDR вектор и последовательно подаются на вход TM для запоминания.

Сама ТМ представлена в виде набора столбцов ячеек, где каждый столбец соответствует одному биту входного вектора. Входной SDR вектор вызывает активацию столбцов, что приводит к активации ячеек столбца. При этом активируются только так называемые деполяризованные ячейки столбца, если таковые имеются. Иначе активируются все ячейки столбца (bursting). Получается такая контекстно-зависимая активация ячеек. \textit{TODO выкинуть или написать понятнее В этой активации одновременно представлен и текущий "символ" последовательности, если смотреть с точки зрения активных столбцов (образно, вид анфас), и текущий префикс (или история) последовательности, если смотреть на уровне конфигурации активных ячеек (образно, вид в профиль).}

Активация ячеек ТМ делает возможным следующий промежуточный шаг ее работы - деполяризацию ячеек. Деполяризация формирует ожидание продолжения последовательности - какой SDR вектор будет подан на вход, а значит какие столбцы будут активированы в следующий момент времени. Ожидание выражается в переводе части ячеек этих столбцов в выделенное - деполяризованное - состояние.

Процесс деполяризации происходит следующим образом. Все ячейки ТМ потенциально имеют связи друг с другом, и в процессе обучения некоторые из этих связей формируются в способные к активации синапсы. Синапсы одной ячейки логически разделены на независимые группы - сегменты, где в разных сегментах могут независимо присутствовать одинаковые синапсы. Сегмент становится активным, когда количество активных пресинаптических ячеек больше некоторого порога. Активация сегмента ячейки в свою очередь ведет к ее деполяризации.

Именно сегменты олицетворяют собой "ожидания" ячейки, формируя условия для её деполяризации, а впоследствии и активации. Получается следующая логика от лица ячейки: если в текущий момент времени один из моих сегментов распознал некоторый контекст, с большой вероятностью следует ожидать, что следом будет активирован и мой столбец, т.к. похоже на то, что похожую последовательность раньше уже наблюдалась. В таком случае ячейка переходит в предсказательное деполяризованное состояние, и, если на следующем шаге ее столбец действительно оказывается активным, она тоже активируется.

\textbf{NB}. Активный сегмент устанавливает двустороннюю связь между деполяризованной ячейкой и ячейками ее деполяризующими. Именно эта особенность будет использована для обращения функции перехода. Основная трудность в этой задаче заключается в том, как подняться от уровня ячеек до уровня состояний-действий.

TODO схема активации и деполяризации

Из первого этапа планирования известно, что искомая цель $g = s_T$ содержится в финальной предсказанной суперпозиции $S_T$, т.е. пересечение соответствующих им SDR векторов выше определенного порога, и можно говорить о том, что данная цель является частью суперпозиции. Далее под искомой целью будет пониматься уже именно их пересечение с суперпозицией $g = s_T \cap S_T$.

Исходя из принципа работы планировщика на первом этапе, раз ячейка активна на шаге $t$, значит она была деполяризована на шаге $t-1$ - т.е. у нее имелся хотя бы один активный сегмент. Рассмотрим активные сегменты ячеек столбцов, соответствующих искомой цели $s_T$. Каждый такой сегмент задает условие деполяризации своей ячейки, и в нашем случае это некоторая пара состояние-действие.

\begin{framed}
  \textbf{NB}. В общем случае все не совсем так. Сегменты учатся распознавать наборы паттернов. Наиболее стойкие к наказанию в процессе обучения будут те наборы, которые имеют ненулевое пересечение паттернов между собой, что минимизирует ожидаемую частоту наказаний. Это означает, что каждый сегмент выучивает кластер или несколько кластеров близких между собой паттернов. Однако можно ограничить размеры сегментов, т.е. число допустимых в сегменте синапсов, до размера одного паттерна. В нашем случае это размер входного SDR вектора, или число столбцов. Тогда каждый сегмент сможет выучивать не больше одного очень плотно сгруппированного кластера паттернов. А, учитывая специфику кодирования пар состояние-действие, паттерн будет один с точностью до небольшой погрешности - одна единственная пара состояние-действие.
\end{framed}

Получается, множество активных сегментов, ответственных за деполяризацию целевого состояния, задает множество множеств активных ячеек, обеспечивающих её. Далее в ход идет предположение, что это множество множеств распадается на кластеры близких множеств, где каждый кластер соответствует конкретной паре состояние-действие, активных на $T-1$ шаге. Эти пары следом фильтруются на предмет того, достаточную ли деполяризацию каждая из них индуцирует.

\textbf{NB}. Алгоритм кластеризации, стартуя с набора кластеров, соответствующих каждому из рассматриваемых сегментов, итеративно склеивает существенно пересекающиеся кластеры пока это возможно.

Прошедшие отбор пары являются кандидатами на искомый переход $(s_{T-1}, a_{T-1}) \rightarrow s_T$, поэтому каждый из соответствующих им кластеров ячеек рекурсивно проверяется бэктрекингом до первого успеха. Получается, на первом шаге входным условием для шага бэктрекинга являлась деполяризация ячеек целевого состояния, на втором - деполяризация кластера ячеек одного из рассматриваемых кандидатов и так далее.

\begin{function}
  \KwData{$d_t$: required cells depolarization at time $t$, $t$: timestep}
  \quad \\
  
  \If(){$t \leq 0$} {
    \Return{True, $\emptyset$}
  }{}

  \tcc{Finds clusters capable of depolarizing most of the $d_t$ cells}
  Init set of clusters $C = \{c_i\}$ as presynaptic cells of active segments of the $d_t$ cells. \;
  \While{$\exists c_i, c_j: match(c_i, c_j) \geq \Theta_{clust}$} {
    $MergeClusters(i, j)$ \tcc{Merges them into one cluster $c = c_i \cup c_j$}
  }
  \quad \\

  \ForEach{$c \in C$}{
    \If() {$CountInducedDepolarization(c) < \Theta_{dep}$}{
      $C.remove(c)$ \;
    }
  }
  \quad \\

  $backtracking\_candidates \leftarrow C$ \;

  \tcc{Recursively checks each candidate cluster - is it possible to "come back" to $s_0$}
  \ForEach{$c \in backtracking\_candidates$}{
    $s_t, a_t \leftarrow Decode(ColumnsFromCells(c))$ \;

    $successful, actions \leftarrow Backtrack(s_t, t-1, active\_segments)$ \;
    \If() {successful} {
      $actions.append(a_t)$ \;
      \Return{True, actions}
    }
  }

  \Return{False, $\emptyset$}
  \caption{Backtrack($d_t$, $t$, $active\_segments$)}
\end{function}

\begin{function}
  \tcc{Другой вариант псевдокода бэктрекинга}
  \KwData{$d_t$: required cells depolarization at time $t$, $t$: timestep}
  \quad \\
  
  \If(){$t \leq 0$} {
    \Return{True, $\emptyset$}
  }{}

  Init set of clusters $C = \{c_i\}$ as presynaptic cells of active segments of the $d_t$ cells. \;
  Iteratively merge any two sufficiently intersecting clusters \;
  \quad \\

  \ForEach{$c \in C$}{
    Count how many of $d_t$ cells are depolarized by the cluster. \;
    Keep in $C$ only clusters with sufficient induced depolarization. \;
  }
  \quad \\

  $backtracking\_candidates \leftarrow C$ \;

  \tcc{Recursively checks each candidate cluster - is it possible to "come back" to $s_0$}
  \ForEach{$c \in backtracking\_candidates$}{
    $s_t, a_t \leftarrow Decode(ColumnsFromCells(c))$ \;

    $successful, actions \leftarrow Backtrack(s_t, t-1, active\_segments)$ \;
    \If() {successful} {
      $actions.append(a_t)$ \;
      \Return{True, actions}
    }
  }

  \Return{False, $\emptyset$}
  \caption{Backtrack($d_t$, $t$, $active\_segments$)}
\end{function}

\section{Тестирование}

Цель тестирования:

\begin{itemize}
    \item Исследовать влияние на производительность агента:
    \begin{itemize}
        \item длины горизонта планирования
        \item размера списка целей
    \end{itemize}
    
    \item Сравнить производительность агента с бейзлайнами:
    \begin{itemize}
        \item Random Walk агент
        \item DQN агент
    \end{itemize}
\end{itemize}

Тестирование агента содержало два различных набора экспериментов, отличающихся количеством сред в каждом из экспериментов.

В качестве реализации рандомной стратегии использовался частный случай htm агента с нулевым горизонтом планирования. У DQN агента в сравнении использовались результаты обеих стратегий - жадной и $\epsilon$-жадной.


\subsection{Тестирование на задаче RL в фиксированной среде}

Испытания проводились в трех заданных вручную средах-лабиринтах типа Gridworld без учета направления взгляда агента.

В рамках испытания (т.е. на протяжении всего времени жизни агента) среда фиксирована, но местоположение награды изменялось каждые N эпизодов. Исследовалась способность агентов находить и адаптироваться к новому местоположению награды.

Ниже к каждой из трёх сред приведены графики результатов рассматриваемых агентов: а) награда за эпизод, б) количество шагов за эпизод и в) длительность выполнения одного эпизода (в секундах). На всех графиках изображена скользящая средняя с окном, указанным в заголовке графика (например, \verb|MA = 20|)

Легенда к названиям агентов на графиках:

\begin{itemize}
    \item \verb|htm| в названии обозначает htm агента
    \begin{itemize}
        \item суффикс \verb|_X| означает горизонт планирования \verb|X|, например:
        \begin{itemize}
            \item \verb|htm_0| - нулевой горизонт планирования, т.е. случайный агент
            \item \verb|htm_4| - горизонт планирования 4
        \end{itemize}
        
        \item суффикс \verb|_Xg| означает размер списка целей \verb|X|, например:
        \begin{itemize}
            \item \verb|htm_2_1g| - горизонт планирования 2, длина списка целей 1
            \item \verb|htm_1_16g| - горизонт планирования 1, длина списка целей 16
            \item если размер списка целей не указан явно, предполагается неограниченная длина списка
        \end{itemize}
    \end{itemize}
    
    \item dqn в названии обозначает DQN агента
    \begin{itemize}
        \item \verb|_greedy| - суффикс жадной стратегии
        \item \verb|_eps| - суффикс $\epsilon$-жадной стратегии
    \end{itemize}
\end{itemize}

Легенда к схемам тестовых сред:

\begin{itemize}
    \item \verb|-| пустая клетка,
    \item \verb|#| стена,
    \item \verb|@| агент,
    \item \verb|X| награда
\end{itemize}

\subsubsection {\texttt{multi\_way\_v0}}

Последовательность из четырех испытаний по 100 эпизодов. Положения награды в испытаниях было следующим:


\begin{verbatim}
  #####   #####   #####   #####
  #@--#   #@-X#   #@--#   #@--#
  #-#-#   #-#-#   #-#-#   #-#X#
  #--X#   #---#   #X--#   #---#
  #####   #####   #####   #####
\end{verbatim}

\subsubsection{\texttt{multi\_way\_v1}}

Последовательность из пяти испытаний по 200 эпизодов. Положения награды:

\begin{verbatim}
  #######   #######   #######   #######   #######
  #@--###   #@--###   #@--###   #@--###   #@--###
  #-#-###   #-#-###   #-#X###   #-#-###   #-#-###
  #X----#   #-----#   #-----#   #-----#   #----X#
  ###-#-#   ###X#-#   ###-#-#   ###-#-#   ###-#-#
  ###---#   ###---#   ###---#   ###--X#   ###---#
  #######   #######   #######   #######   #######
\end{verbatim}

\subsubsection{\texttt{multi\_way\_v2}}

Последовательность из шести испытаний по 200 эпизодов. Положения награды:

\begin{verbatim}
  #########    #########    #########    #########    #########    #########
  #---#####    #-X-#####    #---#####    #---#####    #---#####    #---#####
  #-#-#####    #-#-#####    #-#-#####    #-#-#####    #-#-#####    #-#-#####
  #--@---##    #--@---##    #--@--X##    #--@---##    #X-@---##    #--@---##
  ###--####    ###--####    ###--####    ###--####    ###--####    ###--####
  ###-#--X#    ###-#---#    ###-#---#    ###-#---#    ###-#---#    ###-#--X#
  ###---###    ###---###    ###---###    ###---###    ###---###    ###---###
  ###-#####    ###-#####    ###-#####    ###X#####    ###-#####    ###-#####
  #########    #########    #########    #########    #########    #########
\end{verbatim}

\begin{figure}
  \centering
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__steps}
    \subcaption{\texttt{multi\_way\_v0}}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v1__steps}
    \subcaption{\texttt{multi\_way\_v1}}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v2__steps}
    \subcaption{\texttt{multi\_way\_v2}}
  \end{minipage}
  \caption{Сравнение агента с бейзлайнами.}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{.98\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__steps}
    \subcaption{Сравнение агента с бейзлайнами}
  \end{minipage}

  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__dqn__steps}
    \subcaption{Сравнение жадного и $\epsilon$-жадного агента DQN!}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__1__steps}
    \subcaption{Горизонт планирования: 1}
  \end{minipage}
  
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__2__steps}
    \subcaption{Горизонт планирования: 2}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/multi_way_v0__4__steps}
    \subcaption{Горизонт планирования: 4}
  \end{minipage}

  \caption{ \texttt{multi\_way\_v0}: cравнение агента c бейзлайнами и между собой - с разными горизонтами планирования и размером списка целей.}
\end{figure}

\subsubsection{Ключевые выводы для экспериментов с неизменной средой}

В результате проведенных экспериментов сделаны следующие выводы:

\begin{itemize}
  \item Чем больше горизонт планирования, тем лучше играет агент. Причем улучшение кумулятивно, т.е. каждые следующие +1 горизонта дают больший прирост, чем предыдущий.

  \item Агент с планированием играет лучше, чем случайный. Чем сложнее среда, тем менее заметен в относительных масштабах переход от \verb|horizon_planning = 0| к \verb|horizon_planning = 1|. При фиксированном горизонте планирования с увеличением сложности среды результат в относительных масштабах начинает "приближаться" к результатам случайного агента.

  \item Агент с большим накопленным набором целей направленнее (=оптимальнее) исследует среду. Мы предполагаем это потому, что менее вероятно использование агентом случайной стратегии. В частных случаях это субоптимально. Например, когда награда сразу достижима в рамках имеющегося горизонта планирования и ее положение не изменилось с предыдущего эпизода - тогда агент с большим списком целей может выбрать для исследования псевдо-цель. Вероятно, причина данного эффекта кроется в специальном виде использованных сред, начального положения агента и положения наград.

  \item Увеличение размера списка целей и величины горизонта планирования положительно сказываются на способности агента приспосабливаться к новому местоположению награды.

  \item Увеличение горизонта планирования ухудшает производительность агента. Однако это ухудшение происходит до некоторого момента, пока горизонт планирования не приблизится к длине оптимального пути к цели
  \item Агент учится и приспосабливается быстрее, чем DQN [в терминах числа эпизодов].

  \item Если горизонта планирования достаточно, то агент быстро учится решать задачу планирования идеально, достигая оптимальной стратегии. По нашим оценкам он учится примерно в ~1.5-2.5 быстрее DQN. Тем не менее, для любого фиксированного горизонта, если начать усложнять среду, то в какой-то момент DQN обгонит по результатам, потому что DQN решает задачу [почти всегда] оптимально, а htm агент - только в случае достаточного горизонта планирования.

  \item Иногда htm agent сходится к субоптимальному решению. Например, если горизонта планирования достаточно для субоптимального решения из начальной точки и имеется несколько длинных узких "ходов" до цели, имеющих примерно одинаковую длину. Тогда есть ненулевая вероятность, что агент не успеет запомнить путь через оптимальный коридор и будет планировать только через субоптимальный. В такой ситуации $\epsilon$-жадная стратегия DQN, наоборот, рано или поздно откроет оптимальное решение.
\end{itemize}

\subsection{Тестирование на задаче RL в сменяющихся средах}

Испытание данного режима проводились на множестве случайно сгенерированных сред-лабиринтах типа GridWorld без учета направления взгляда агента. Схема каждого испытания следующая:

\begin{itemize}
  \item испытание проводится последовательно на $N_{env}$ средах
  \item при фиксированной среде последовательно рассматриваются $N_{rew}$ положений награды
  \item при фиксированном положении награды рассматривается $N_{s_0}$ начальных положений агента
  \item в фиксированной конфигурации агент играет $N$ эпизодов
\end{itemize}

Таким образом испытание в общей сложности содержит $N_{env} \cdot N_{rew} \cdot N_{s_0} \cdot N$ эпизодов.

Исследовалась способность агента адаптироваться не только к новому местоположению награды в рамках одной среды, но и к новой среде целиком.

Среды генерировались случайно на некотором фиксированном квадрате размера $n \times n$. Ииспытания проводились в средах $5 \times 5$, $6 \times 6$ и $8 \times 8$, но в отчете приведены результаты только в средах размера $5 \times 5$ - в остальных случаях результаты были схожими.

Ключевым различием между испытаниями были суммарное число эпизодов с фиксированной наградой и средой, которые влияли на сложность обратно пропорционально - чем меньше эпизодов в распоряжении агента, тем быстрее ему требуется адаптироваться (к новой награде и/или к новой среде, соответственно).

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_1_1_200_1337_map_0_1173222464}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_1_1_200_1337_map_2_1561234712}
  \end{minipage}
  \caption{Примеры сгенерированных сред размера 5x5. Легенда: темно фиолетовый - стены, желтый - начальная позиция агента, салатовый - награда.}
\end{figure}

\subsubsection{Эксперименты с редкой сменой наград и сред}

\begin{itemize}
  \item Gridworld 5x5
  \item $N_{s_0} = 100$
  \item $N_{rew} = 2$
  \item $N_{env} = 8$
\end{itemize}

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_1337__steps}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_42__steps}
  \end{minipage}
  \caption{Сравнение агента с бейзлайнами в эксперименте с редкой сменой наград и сред.}
\end{figure}

\begin{figure}
  \centering
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_1337__dqn__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_42__dqn__steps}
    \subcaption{Жадный и $\epsilon$-жадный DQN агент}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_1337__2__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_42__2__steps}
    \subcaption{Горизонт планирования 2}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_1337__4-8__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_100_2_8_42__4-8__steps}
    \subcaption{Горизонт планирования 4}
  \end{minipage}
  \caption{Результаты агентов в эксперименте с редкой сменой наград и сред.}
\end{figure}

\subsubsection{Эксперименты со средней частотой смены наград и сред}

\begin{itemize}
  \item Gridworld 5x5
  \item $N_{s_0} = 50$
  \item $N_{rew} = 4$
  \item $N_{env} = 4$
\end{itemize}

\begin{figure}
  \centering
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__steps_rel_htm_0}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__dqn__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__2__steps}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__1__steps}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_50_4_4_1337__4-8__steps}
  \end{minipage}
  \caption{Результаты агентов в эксперименте со средней частотой смены наград и сред.}
\end{figure}

\subsubsection{Эксперименты с частой сменой наград и сред}

\begin{itemize}
  \item Gridworld 5x5
  \item $N_{s_0} = 20$
  \item $N_{rew} = 1$
  \item $N_{env} = 20$
\end{itemize}

\begin{figure}
  \centering
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_20_1_20_1337__steps}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_20_1_20_1337__1__steps}
  \end{minipage}
  \begin{minipage}{.32\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_20_1_20_1337__2__steps}
  \end{minipage}
  \caption{Результаты агентов в эксперименте с частой сменой наград и сред.}
\end{figure}

\subsubsection{Эксперименты с ультрачастой сменой наград и сред}

TODO Нужен ли этот эксперимент вообще?

\begin{itemize}
  \item Gridworld 5x5
  \item $N_{s_0} = 1$
  \item $N_{rew} = 1$
  \item $N_{env} = 200$
\end{itemize}

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_1_1_200_1337__steps}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{./assets/gridworld_5x5_1_1_200_1337__2__steps}
  \end{minipage}
  \caption{Результаты агентов в эксперименте с ультрачастой сменой наград и сред.}
\end{figure}

\subsubsection {Ключевые выводы для экспериментов со сменой сред}

HTM агент в среднем быстрее чем DQN адаптируется к изменениям среды или в среде. Благодаря этому обычно он ведет себя стабильнее.

Увеличение списка целей негативно сказывается на результатах агента. Получается, что польза от выполнения псевдо-целей в среднем ниже штрафа. Мы считаем это потому, что в среднем псевдо-цели не приближают агента к реальной и только увеличивают ожидаемое число шагов с использованием случайной стратегии.

Данный результат оказался для нас неожиданным, т.к. расходится с выводами, полученными по результатам экспериментов без смены среды. Судя по всему, дело в специфике тех экспериментов - в них следование псевдо-целям всегда уменьшало расстояние до цели (кроме эксперимента \verb|multi_way_v2|, где это верно не всегда, но все же с достаточно большой вероятностью). Из-за этого мы наблюдали положительный эффект от увеличения списка подцелей.

В вырожденном случае смены среды и награды на каждом эпизоде все агенты проигрывают случайному. При этом DQN играет значительно хуже HTM агента. TODO Почему?

\end{document}
