\documentclass[default]{beamer}
\usepackage{slides_style}
\setbeamertemplate{navigation symbols}{}
\usetheme{Boadilla}

\begin{document}

\title[]{
	Planning with Hierarchical Temporal Memory for Deterministic Markov Decision Problem
}
\author[Petr Kuderov]{
    Petr Kuderov, Aleksandr I. Panov
}
\institute[MIPT]{
    Moscow Institute of Physics and Technology, Moscow, Russia\\
	kuderov.pv@phystech.edu
}
\date[]{Feb 2021}
	
\begin{frame}
	\titlepage
	\centering
	\includegraphics[height=20pt]{mipt_logo} \hspace{10pt}
	\includegraphics[height=20pt]{cdsl_logo} \hspace{10pt}
\end{frame}

\begin{frame}{Model-based RL}
	\begin{itemize}
		\item learning the environment allows using planning methods
		\item can remedy sample inefficiency of model-free RL
		\item popular choice - Artificial Neural Networks
		\item we used more human-like memory model - Hierarchical Temporal Memory (HTM)
	\end{itemize}
\end{frame}

\begin{frame}{Research question}
	- Check applicability of TM
  - Build planning method
\end{frame}

\begin{frame}{HTM and TM}
	SDR, prediction and backward prediction	
\end{frame}

\begin{frame}{Agent's architecture}
    \includegraphics[width=.65\linewidth]{agent_scheme}
	- explain how it works on a high level	
\end{frame}

\begin{frame}{Planning, stages}
	- the goal of planning
	- two stages, what their goals
	- forward planning: goal reachability and keep track of predictions
	- backtracking: find the exact path to the goal
\end{frame}

\begin{frame}{Forward planning}
	explain how it's done
\end{frame}

\begin{frame}{Backtracking}
	explain recursive algo
\end{frame}

\begin{frame}{Experiments in fixed environments}
\begin{columns}
	\begin{column}{.55\textwidth}
	Setup:
	\begin{itemize}
		\item fixed handcrafted mazes
		\item reward position changes every 100 episodes
	\end{itemize}

	\vspace*{12pt}
	Conclusions:
	\begin{itemize}
		\item better than random policy in any situations
		\item learns and adapt faster than DQN
		\item optimal solution requires planning horizon to be large enough to reach the reward from the initial state
	\end{itemize}

	\end{column}

	\begin{column}{.45\textwidth}
	\begin{center}
		\includesvg[width=\linewidth]{multi_way_v0__steps}
		\includesvg[width=\linewidth]{multi_way_v2__steps}
	\end{center}
	\end{column}
\end{columns}	
\end{frame}

\begin{frame}{Experiments in changing environments}
\begin{columns}
	\begin{column}{.55\textwidth}
	Setup:
	\begin{itemize}
		\item randomly generated mazes
		\item maze periodically changes too
		\begin{itemize}
			\item up: every 200 episodes
			\item bottom: every 20 episodes
		\end{itemize}
	\end{itemize}

	\vspace*{12pt}
	Conclusions:
	\begin{itemize}
		\item learns and adapt faster than DQN
		\item more stable performance - less degradation spikes
		\item suboptimal solutions with faster adaptation win over potentially optimal solutions unable to adapt in time
	\end{itemize}

	\end{column}

	\begin{column}{.45\textwidth}
	\begin{center}
		\includesvg[width=\linewidth]{gridworld_5x5_100_2_8_1337__steps}
		\includesvg[width=\linewidth]{gridworld_5x5_20_1_20_1337__steps}
	\end{center}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}{Conclusions}
	regarding research questions
\end{frame}

\begin{frame}{Limitations and possible improvements}
	- deal with planning horizon: two ways
	- regading random strategy	
\end{frame}

\end{document}