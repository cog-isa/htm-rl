@misc{ahmad_2016_sdr,
  title         = {How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites},
  author        = {Subutai Ahmad and Jeff Hawkins},
  year          = {2016},
  eprint        = {1601.00720},
  archiveprefix = {arXiv}
}
@article{Brown_Sandholm_2017_Poker,
  title     = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  volume    = {359},
  url       = {http://dx.doi.org/10.1126/science.aao1733},
  doi       = {10.1126/science.aao1733},
  number    = {6374},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Brown, Noam and Sandholm, Tuomas},
  year      = {2017},
  month     = {Dec},
  pages     = {418–424}
}

@article{Campbell_Hoane_Hsu_2002_DeepBlue,
  title     = {Deep Blue},
  volume    = {134},
  url       = {http://dx.doi.org/10.1016/S0004-3702(01)00129-1},
  doi       = {10.1016/s0004-3702(01)00129-1},
  number    = {1–2},
  journal   = {Artificial Intelligence},
  publisher = {Elsevier BV},
  author    = {Campbell, Murray and Hoane, A.Joseph, Jr. and Hsu, Feng-hsiung},
  year      = {2002},
  month     = {Jan},
  pages     = {57–83}
}

@inbook{Coulom_2007_mcts,
  title     = {Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search},
  url       = {http://dx.doi.org/10.1007/978-3-540-75538-8\_7},
  doi       = {10.1007/978-3-540-75538-8\_7},
  booktitle = {Computers and Games},
  publisher = {Springer Berlin Heidelberg},
  author    = {Coulom, Rémi},
  year      = {2007},
  pages     = {72–83}
}

@article{George_Hawkins_2009,
  title     = {Towards a Mathematical Theory of Cortical Micro-circuits},
  volume    = {5},
  url       = {http://dx.doi.org/10.1371/journal.pcbi.1000532},
  doi       = {10.1371/journal.pcbi.1000532},
  number    = {10},
  journal   = {PLoS Computational Biology},
  publisher = {Public Library of Science (PLoS)},
  author    = {George, Dileep and Hawkins, Jeff},
  editor    = {Friston, Karl J.},
  year      = {2009},
  month     = {Oct},
  pages     = {e1000532}
}

@article{Ha_Schmidhuber_2018_worldmodels,
  title        = {World Models},
  url          = {https://zenodo.org/record/1207631},
  doi          = {10.5281/ZENODO.1207631},
  abstractnote = {We explore building generative neural network models of popular reinforcement learning environments. Our <em>world model</em> can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this article is available at worldmodels.github.io.},
  journal      = {Zenodo},
  publisher    = {Zenodo},
  author       = {Ha, David and Schmidhuber, Jürgen},
  year         = {2018},
  month        = {Mar}
}

@misc{haarnoja_2018_sac,
  title         = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author        = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  year          = {2018},
  eprint        = {1801.01290},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{Hassabis_2017_neuro,
  title     = {Neuroscience-Inspired Artificial Intelligence},
  volume    = {95},
  url       = {http://dx.doi.org/10.1016/j.neuron.2017.06.011},
  doi       = {10.1016/j.neuron.2017.06.011},
  number    = {2},
  journal   = {Neuron},
  publisher = {Elsevier BV},
  author    = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year      = {2017},
  month     = {Jul},
  pages     = {245–258}
}
@article{hawkins_TM,
  author   = {Hawkins, Jeff and Ahmad, Subutai},
  title    = {Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
  journal  = {Frontiers in Neural Circuits},
  volume   = {10},
  pages    = {23},
  year     = {2016},
  url      = {https://www.frontiersin.org/article/10.3389/fncir.2016.00023},
  doi      = {10.3389/fncir.2016.00023},
  issn     = {1662-5110},
  abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@misc{kaiser_2020_modelbased,
  title         = {Model-Based Reinforcement Learning for Atari},
  author        = {Lukasz Kaiser and Mohammad Babaeizadeh and Piotr Milos and Blazej Osinski and Roy H Campbell and Konrad Czechowski and Dumitru Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Afroz Mohiuddin and Ryan Sepassi and George Tucker and Henryk Michalewski},
  year          = {2020},
  eprint        = {1903.00374},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{Mnih_2015_Atari,
  title     = {Human-level control through deep reinforcement learning},
  volume    = {518},
  url       = {http://dx.doi.org/10.1038/nature14236},
  doi       = {10.1038/nature14236},
  number    = {7540},
  journal   = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and et al.},
  year      = {2015},
  month     = {Feb},
  pages     = {529–533}
}

@misc{openai_2019_dota,
  title         = {Dota 2 with Large Scale Deep Reinforcement Learning},
  author        = {OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique Pondé de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  year          = {2019},
  eprint        = {1912.06680},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{Puterman_1994,
  title     = {Markov Decision Processes},
  author    = {Puterman, Martin},
  url       = {http://dx.doi.org/10.1002/9780470316887},
  doi       = {10.1002/9780470316887},
  journal   = {Wiley Series in Probability and Statistics},
  publisher = {John Wiley \& Sons, Inc.},
  year      = {1994},
  month     = {Apr}
}

@misc{schulman_2017_ppo,
  title         = {Proximal Policy Optimization Algorithms},
  author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  year          = {2017},
  eprint        = {1707.06347},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{silver_2020_muzero,
  title         = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  author        = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
  year          = {2020},
  eprint        = {1911.08265},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@article{Silver_Go,
  title   = {Mastering the game of Go with deep neural networks and tree search},
  author  = {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  year    = {2016},
  url     = {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html},
  journal = {Nature},
  pages   = {484--503},
  volume  = {529}
}

@book{sutton_barto_2018,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  isbn      = {0262039249},
  publisher = {A Bradford Book},
  address   = {Cambridge, MA, USA},
  abstract  = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}
