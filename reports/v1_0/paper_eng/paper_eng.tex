\documentclass[runningheads]{llncs}
\usepackage{style_eng}
\bibliographystyle{splncs04}

\begin{document}

\title {Planning with Hierarchical Temporal Memory for deterministic Markov Decision Problem}
\author {Petr Kuderov \inst{1} \and Aleksandr I. Panov \inst{1}}

\authorrunning{P. Kuderov, A. Panov}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Moscow Institute of Physics and Technology, Russia}

\maketitle

\begin{abstract}
  Hierarchical Temporary Memory (HTM) is a biologically inspired model of neocortex. Its key properties proved it to be useful useful for sequence learning and anomaly detection problems. While they also make it potentially useful for a model-based RL, this particular application of HTM has hardly been studied yet.

  We show a possible way to utilize HTM in RL setting to model an environment dynamics. We accompany it with an example of planning algorithm which is used to test learning abilities and find limitations of such a model.
\end{abstract}

\keywords{First keyword  \and Second keyword \and Another keyword.}

\section{Introduction}

Reinforcement Learning (RL) has recently got a lot of ... A great part of its success was due to advances in application of Artificial Neural Networks (ANN). Although ANNs model is very popular and successful at the moment, it's built on top of very simplified model of a neuron.

There are more sophiscticated models of neuron [TODO links], which are mostly used by neurophysiologists and serve their needs - to reverse engineer internal physical structure and operational model of neuron. These models are very computationally expensive compared to Artificial neuron, so they are rarely used by Machine Learning practitioners as a basis for complex neural networks to solve real ML tasks.

Still, it's very tempting to take inspiration from [human] brains and how they function as, in the end, this's what AI researchers are trying to recreate [TODO Hassabis paper on inspiration].

HTM neuron model is on of the models aiming the middle point between neurophysiological plausibility, simplicity and computational efficiency. TODO more about it, it's applications and TM. EDIT \textit{This model works mostly with discrete [binary] sparse signals. And it gives the model some cool features - the natural way to set similarity metric between these vector, resilience to the noise, computational efficency and so on. This model also proposes some basic Neural Network building blocks, like sequential Temporal Memory, which is somewhat similar to what we have in mainstream ML. It doesn't use backpropagation for learning, and it doesn't need to set up very small learning rate, so in the end TM learns much faster than common ANNs.}

In our work we studied if and how Temporal Memory could be reliably used to model an environment an agent's opeates in - how an environment changes over time and how it reacts to agent's actions. The agent's own inner model of the environment allows it to plan ahead, to see the outcomes of its actions and therefore to identify desirable or dangerous moves.

The trial and error strategy can be very expensive in the real world. For example, you couldn't be able to afford breaking the car a thousand times to teach it how to bypass an obstacle, especially if the obstacle is human. Planning allows an agent to spend less time acting in the real world by spending more time learning in its own internal simulation, like in a "dream".

We provide an example of planning strategy to test both: 

\begin{itemize}
  \item The quality of memory - how good it approximates the environment's dynamics?
  \item The viability of the planning method - does this method allow an agent to solve tasks and how much it helps?
\end{itemize}

We also studied how the planning method's parameters affect the performance of an agent.

We restricted tasks to be held in very simple deterministic MDPs with perfectly distinguishable environment states. In this case Temporal Memory proved to learn fast and reliably the model of the environment. We also found out that our planning method mostly helps an agent to solve problems, and in special cases it allows an agent to solve them in optimal manner (or nearly optimal).

\section{Problem statement}

Consider classic Markov Decision Problem (MDP) - when agent plays in deterministic GridWorld environment, which is a maze on a square grid. Agent starts in the fixed point, and its goal is to find a single reward placed at some fixed point.

Agent can move to the adjacent grid cells in horizontal and vertical directions. Each action move is enumerated from 0 to 3 counter-clockwise starting from east. When agent tries to move into the maze wall, it stays at the same place.

We consider only deterministic environments. When agent finds the goal it is rewarded with high positive value $+1$, otherwise each unsuccessful step it is slightly punished with $-0.01$ value.

\section{Method description}
In our implementation an agent's policy is a mixture of two: random and planned. Planned policy is provided by the corresponding part of an agent called \textit{planner}. If planner cannot provide a plan, agent degrades to a fully random exploration strategy.

So, the primary part of our agent is a planning algorithm.

The main goal of a planner is to find a path to the reward. This path should not take more than \verb|planning_horizon| steps, which defines planning horizon of an agent.

Planning process is split into two stages (see function ~\ref{func_PlanActions}):

\begin{enumerate}
  \item Forward planning from the current state until reward is reached.
  \item Deducing the backward path from the reward state to the current.
\end{enumerate}

\begin{function}
  \caption{PlanActions($s_0$)}\label{func_PlanActions}
  \SetAlgoNoLine
  $s_0$: initial state \;
  $g: $ reached goal \;
  $H: $ Temporal Memory active segments history \;
  \quad \\

  $g, H \leftarrow PredictToGoal(s_0)$ \;
  $plan \leftarrow BacktrackFromGoal(g, s_0, H)$ \;

  \Return{$[a_0, a_1, .. a_T] = plan$}
\end{function}

To support planning capability agent is provided with the memory module to remember environment transitions that it saw. In other words it learns environment dynamics by approximating its transition function $f: (s, a) \rightarrow s'$. Hence given any state and action it could make a prediction on what state is coming next.

Agent's memory has some particular features which we describe before heading to the planning algorithm details.

\subsection{Agent's memory}

The agent's memory has an ability to work with unions of what it has ``remembered''. By working with union we mean working with a set as a whole without an ability to split it into its separate elements. In our case the memory can make a prediction about next states for a union of state-action pairs. Precisely, it allows queries of the form:

\begin{quote}
  \textit{given a union of states $S=\cup s_i$ and a union of actions $A=\cup a_j$, the union of which next states $S'= \cup s'_m$ all possible state-action pairs $(s_i, a_j)$ lead to?}
\end{quote}

In some specific circumstances the memory can be used backward - as a reverse of an environment transition function $f$. This feature becomes available after answering a prediction query and only for the states $s'_m$ from the result $S' = \cup s'_m$. For each such state $s'$ the memory can answer which state-action pairs from the original query lead to it: $\{(s, a)\} = f^{-1}(s')$. Note that in general case it's possible that \textit{a multiple} state-action pairs lead to $s'$.

Also the memory module has a submodule to keep track of rewarding states that agent has ever met. We call it a \textit{goal list}. This list has restricted maximum size, which is a hyperparameter. At the moment it's implemented as a naive circular queue (with traits of a set, so that it only keeps unique elements).

These features make a basis for the planning algorithm.

\subsubsection{Agent's memory implementation details}

Consider an agent's trajectory $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, ...)$. Then each pair $(s_t, a_t)$ at some moment $t$ defines precondition to the environment transition: $(s_t, a_t) \rightarrow s_{t+1}$.

Under the hood the agent's memory implementation is based on HTM Temporal Memory (TM) ~\cite{hawkins:TM}. TM learns sequences $[(s_0, a_0), (s_1, a_1), \dots]$ from the trajectories the agent plays.

For each state-action pair $(s, a)$ the memory can answer which state-action pairs $(s', a')$ it expects to see next. The answer is provided as a union of these pairs. We call such unions \textit{state-action superpositions}. Note that it not only learns transitions $(s, a) \rightarrow s'$, but also what actions $\{a'_i\}$ agents did after getting to the next state $s'$.

The memory works with sparse binary vectors. And the union operation for binary sparse vectors is defined naturally as a bitwise OR operation.

Both state and action are encoded independently as sparse binary vectors and then two resulting vectors are concatenated together. Encoding scheme has each integer number from the range $[0, N)$ mapped to the fixed number of active bits. For example for $N = 3$ numbers 0, 1 and 2 are encoded correspondingly as following:

\begin{itemize}
  \item $0 \rightarrow$ \verb|1111 0000 0000|
  \item $1 \rightarrow$ \verb|0000 1111 0000|
  \item $2 \rightarrow$ \verb|0000 0000 1111|
\end{itemize}

As you see, resulting vector is split into buckets of bits. The size of buckets is a hyperparameter (in an example above it's 4).

And an example of superposition of numbers 1 and 4 (from the $[0, 5]$ range) is OR applied to two corresponding sparse vectors:

\begin{verbatim}
  0000 1111 0000 0000 1111 0000
  =
  0000 1111 0000 0000 0000 0000
  OR
  0000 0000 0000 0000 1111 0000
\end{verbatim}

As you see, in our current encoding scheme different states and actions could be distinguished from each other in a superposition. But the planning algorithm only requires a separation of state superpositions from action superpositions, because for the future work with more complex environments encoding scheme is planned to change.

As superposition of state-action pairs is also a valid binary sparse vector, the memory can naturally work with them as an input to prediction query as well. In this case it makes prediction for a multiple different environment transitions simultaneously. The result of prediction is also a superposition, because even for a single transition agent could do multiple different actions from the next state $s'$.

\subsection{Planner. Stage 1: forward planning to a goal}

Agent uses its experience and expects the reward to be in one of the previous rewarding states. But even if it is wrong, we expect that direct goal-based exploration is more optimal than random walking. Hence it keeps the goal list and gives such goal states a priority.

Consider that at some timestep $t_0$ the agent is in state $s_0$. To find out if there is a reachable goal state in a radius of $N$ actions, it start planning from the superposition $(S_0 = s_0, A = \cup a^i)$, where $A$ is a union of all available actions (we use superscripts for indices here as subscripts are used to denote timestep). It is similar to making all possible actions simultaneously in parallel. To achieve this planner uses the memory and gets a superposition of next state-action pairs.

After the first planning step the planner has a superposition of the next states $S_1 = \cup s^i$, reachable in one agent's action. To get all reachable states in two agent's action, planner makes another prediction step - now from the superposition $(S_1, A)$, and so on (see function ~\ref{func_PredictToGoal}).

\begin{function}
  \caption{PredictToGoal($s_0$)} \label{func_PredictToGoal}
  \SetNoFillComment
  \KwData{$s_0$ - initial state}

  $sa_0 \leftarrow Encode((s_0, \cup a^i))$ \tcp*{Init starting superposition}
  $H \leftarrow []$ \tcp*{Init active segments history}

  \ForEach {timestep $i$} {
    $memory.ActivatePredict(sa_i)$ \;
    \quad \\

    $H[i] \leftarrow memory.active\_segments$ \;
    $sa_{i+1} \leftarrow PredictionFromSegments(H[i])$ \;
    \quad \\
    
    $s, a \leftarrow Decode(sa_{i+1})$ \tcp*{Both $s$ and $a$ are superpositions!}
    $reached\_goal \leftarrow MatchGoals(s)$ \;
    \If(){$reached\_goal \neq \emptyset$} {
      \Return {$reached\_goal, H$} \;
    }{}
  }
  \Return{$\emptyset$}
\end{function}

This process is continued until either some state from the goal list is reached, or planner exceeds its planning horizon limit. In the former case planner proceeds to the next state of planning process. In the latter, agent uses its random strategy to make a move before it tries to plan again.

To test if the goal state is in superposition we intersect their sparse vectors and sum active bits, which is the same as dot product of such vectors. If they match is over some predefined threshold, the state is considered presented in this superposition.

During this stage planner collects a history of active superpositions $H = [S_0, S_1, \dots S_T]$. In more details, he keeps the track of Temporal Memory cells' segments activations. Each segment is a pair $(p, \{c_i\})$, where $p$ - index of predicted cell and $\{c_i\}$ is a set of cell indices that induce prediction of $p$. This information allows planner to use inversed environment transition function to get which $\{(s, a)\}$ has lead to a state $s'$ by tracking which cells predict $s'$ cells.

It's possible to find multiple reachable goals at the end of this stage. In our current implementation we use the first matched from the goal list.

\subsection{Planner. Stage 2: deducing the backward path}

After the first stage planner ``believes'' that some goal state $g = s_T$ can be reached in $T$ steps. The problem is that he doesn't know how to get there as until now he worked with superpositions. So the main goal of this stage is to restore the sequence of actions leading from $s_0$ to $s_T$. Further on this sequence of action is meant to be the agent's plan in environment.

Planner solves the problem from the end. It moves backward in the history of segments activations graph, trying to get to the starting state $s_0$ at the first timestep. Due to its ``backward moving'' nature we call this stage \textit{backtracking} (see function ~\ref{func_BacktrackFromGoal}).

\begin{function}
  \caption{BacktrackFromGoal($g$, $H$)} \label{func_BacktrackFromGoal}
  % \SetAlgoNoLine
  \KwData{$g = s_T$ - goal state; $H$ - active segments history}

  $T \leftarrow |H| + 1$ \tcp*{As the last recorded transition is for $T-1 \rightarrow T$}
  $S_T \leftarrow PredictionFromSegments(H[T-1])$ \tcp*{State superposition at $T$}
  $s_T \leftarrow s_T \cap S_T$ \tcp*{Keeps only matched bits}
  \quad \\
  $is\_successful, actions \leftarrow Backtrack(s_T, T-1, H)$ \;
  \Return {$is\_successful, actions$}
\end{function}

Recall that planner keeps track of all state superpositions $H = [S_0, S_1, \dots S_T]$ it has reached on each step $t$, and for any state $s'$ from each of these superpositions $S_i$ the memory can answer which state-action pairs has lead to it.

Therefore, given the reached goal state $g = s_T$ planner can get which state-action pair $(s_{T-1}, a_{T-1})$ has lead to it. Hence he knows for $T-1$ step which action is required to get from $s_{T-1}$ to the goal state $s_T$. And also he can recursively reduce the $s_0 \leadsto s_T$ pathfinding problem to the  $s_0 \leadsto s_{T-1}$. Planner repeats this procedure and for each timestep $t$ sequentially finds state-action pairs $(s_{t-1}, a_{t-1}) = f^{-1}(s_t)$ until he returns to the initial state at the first timestep (see ~\ref{func_Backtrack} function).

In general case $f^{-1}(s_{t+1})$ gives multiple pairs $(s_t, a_t)$, and each leads to $s_{t+1}$. But not every pair is actually reachable in exactly $t$ steps from the $s_0$, i.e. not from every pair you can backtrack to the initial state at time $0$. Hence planner sequentially tests each pair until he find the first successful.

As the result of the backtracking stage planner forms a plan of actions $plan = [a_0, a_1, \dots a_{T-1}]$ which he ``believes'' [in terms of his approximation of the environment dynamics] will lead the agent from the initial state $s_0$ to the target state $g = s_T$.

\begin{function}
  \caption{Backtrack($p_t$, $t$, $H$)} \label{func_Backtrack}
  \KwData{$p_t$ - indices of required predicted cells at time $t$, $t$ - current timestep, $H$ - history of active segments}
  
  \If(){$t \leq 0$} {
    \Return{True, $[]$}
  }{}

  $P \leftarrow H[t][p_t]$ \tcp*{List of cell sets, each predicts one of $p_t$ cells}
  $C \leftarrow P$ \tcp*{Initializes cell clusters - candidates for backtracking}
  \quad \\

  \tcp{Iteratively merges any two sufficiently matching cell clusters}
  \While{$\exists c_i, c_j \in C: match(c_i, c_j) \geq \Theta$} {
    $C[c_i] = c_i \cup c_j$ \;
    $C[c_j].remove()$ \;
  }
  \quad \\

  \ForEach{$c \in C$}{
    \tcp{Filters by the amount of $p_t$ cells cluster $c$ predicts}
    $p = PredictionFromCluster(c, H[t])$ \;
    \If(){$match(p, p_t) \le \Omega$}{
      \textbf{continue}
    }{}
    \quad \\
    
    \tcp{Recursively checks candidate cluster $c$}
    $s_t, a_t \leftarrow Decode(ColumnsFromCells(c))$ \;
    
    $is\_successful, actions \leftarrow Backtrack(s_t, t-1, H)$ \;
    \If() {is\_successful} {
      $actions.append(a_t)$ \;
      \Return{True, actions}
    }
  }

  \Return{False, $[]$}
\end{function}

After successful backtracking the planner has a policy for the next $T$ steps, which an agent uses. And if it ends up in a rewarding state, an episode finishes. Otherwise it continues and the planner removes that state from its goal list for the rest of the episode.

\section{Results}

The main goal of this work was to prove that the memory built upon Temporal Memory from the HTM framework is able to reliably model the environment's dynamics and therefore it allows an agent to make successful plans with it. We compare the method's performance with baselines: random walk and DQN[TODO link].

We also investigated, how planning horizon and the size of the goal list affects agent's performance.

Our experiments were split into two sets. The first set was in a fixed environment and the second - in a changing environment.

For an implementation of the random walk agent we used our agent with the zeroed planning horizon. We also used both greedy and $\epsilon$-greedy learned DQN strateies in our comparisons.

\subsection{Experiments in fixed environments}

In this experiment setup we used a number of handcrafted GridWorld mazes. Each experiment has a fixed maze environment, but the place of the reward was being changed each $N$ episodes. We tested an agent's ability to find the reward and how it adapts to its new position.

Below is the notation used in the environment schemes below:
\begin{itemize}
    \item \verb|-| empty cell,
    \item \verb|#| wall,
    \item \verb|@| agent,
    \item \verb|X| reward
\end{itemize}

\texttt{multi\_way\_v0} was the smallest environment. Each experiment had 4 different rewarding places being sequentially changed each 100 episodes:

\begin{verbatim}
  #####   #####   #####   #####
  #@--#   #@-X#   #@--#   #@--#
  #-#-#   #-#-#   #-#-#   #-#X#
  #--X#   #---#   #X--#   #---#
  #####   #####   #####   #####
\end{verbatim}

\texttt{multi\_way\_v2} was the largest of tested environments. Each experiment had 6 rewarding places being changed each 200 episodes:

\begin{verbatim}
  #########  #########  #########  #########  #########  #########
  #---#####  #-X-#####  #---#####  #---#####  #---#####  #---#####
  #-#-#####  #-#-#####  #-#-#####  #-#-#####  #-#-#####  #-#-#####
  #--@---##  #--@---##  #--@--X##  #--@---##  #X-@---##  #--@---##
  ###--####  ###--####  ###--####  ###--####  ###--####  ###--####
  ###-#--X#  ###-#---#  ###-#---#  ###-#---#  ###-#---#  ###-#--X#
  ###---###  ###---###  ###---###  ###---###  ###---###  ###---###
  ###-#####  ###-#####  ###-#####  ###X#####  ###-#####  ###-#####
  #########  #########  #########  #########  #########  #########
\end{verbatim}

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v0__steps}
    \subcaption{\texttt{multi\_way\_v0}}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v2__steps}
    \subcaption{\texttt{multi\_way\_v2}}
  \end{minipage}
  \caption{Comparison with the baselines. \texttt{htm\_X\_Yg} is our agent with the planning horizon X and the goal list size Y, \texttt{htm\_0} - random walk agent, \texttt{dqn\_greedy} - dqn agent.} \label{fig_comparison_mw}
\end{figure}

The results in these two environments are presented on Figure ~\ref{fig_comparison_mw}.

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v0__dqn__steps}
    \subcaption{Comparison with greedy and $\epsilon$-greedy DQN agent}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v0__1__steps}
    \subcaption{Planning horizon = 1}
  \end{minipage}
  
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v0__2__steps}
    \subcaption{Planning horizon = 2}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{multi_way_v0__4__steps}
    \subcaption{Planning horizon = 3}
  \end{minipage}

  \caption{Detailed results for \texttt{multi\_way\_v0}: comparison with DQN and with itself for different goal list sizes and planning horizons.}
\end{figure}

As the results of this set of experiments we concluded:

\begin{itemize}
  \item The longer planning horizon the better agent performs. And each increment is cumulative, i.e. each next increment is more advantageous.
  
  \item Agent with planning performs better than fully random. But the more complicated an environment the less distinctive the difference between the absence of planning and \verb|horizon_planning = 1|. Even more, for a fixed planning horizon results are getting closer with the rising the size and complexity of an environment.
  
  \item Agent with the longer goal list size better explores the environment. We think that's because agent less frequently uses random strategy. But in some cases this behavior is suboptimal. For example, when the rewarding goal state is reachable from the current position, but there's a fake goal state closer to the agent in the opposite direction. We think that these results are highly biased due to the special type of handcrafted mazes.
  
  \item Agent with longer goal list and planning horizon better adapts to the changed reward position.
  \item Agent learns and adapts faster than DQN agent [in terms of the required number of episodes].
  \item If planning horizon is big enough to solve find a path from the starting position, then agent learns optimal policy very fast. In our experiments it learns ~1.5-2.5 times faster than DQN. But for any fixed planning horizon if we start rising an environment complexity at some point DQN start to perform better than our agent. That's because DQN always find an optimal path and our agent only when it has enough planning horizon.
\end{itemize}

\subsection{Experiments in changing environments}

In this experiment setup we used a number of randomly generated GridWorld mazes of fixed size. Each experiment has a following scheme:

\begin{itemize}
  \item it's made sequentially in $N_{env}$ randomly generated mazes
  \item for a fixed maze the reward position is changed sequentially $N_{rew}$ times
  \item for a fixed reward position the initial position is changed sequentially $N_{s_0}$ times
  \item for a fixed whole configuration agent plays $N$ episodes.
\end{itemize}

So each experiment has $N_{env} \cdot N_{rew} \cdot N_{s_0} \cdot N$ total episodes. We tested an agent's ability not only to find reward and adapt to its new position, but also how it adapts to the whole new maze.

We tested agent in mazes generated on $5 \times 5$, $6 \times 6$ and $8 \times 8$ square grids. Here we present only results for $5 \times 5$ mazes, but for other sizes the results are similar.

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{gridworld_5x5_1_1_200_1337_map_0_1173222464}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{gridworld_5x5_1_1_200_1337_map_2_1561234712}
  \end{minipage}
  \caption{Example of generated mazes of 5x5 and 8x8 sizes. Legend: dark purple - walls, yellow - starting agent's position, salad - reward.}
\end{figure}

The key difference in complexity of each experimental setup is the total number of episodes with fixed reward and maze. Less episodes has agent to adapt, the more agile it's required to be.

We present results for the following experimental setups (Figure 4):

\begin{enumerate}
  \item Experimental setup with \textbf{rare} change of reward and maze
  \begin{itemize}
    \item Gridworld 5x5
    \item $N_{s_0} = 100$
    \item $N_{rew} = 2$
    \item $N_{env} = 8$
  \end{itemize}

  \item Experimental setup with \textbf{frequent} change of reward and maze
  \begin{itemize}
    \item Gridworld 5x5
    \item $N_{s_0} = 20$
    \item $N_{rew} = 1$
    \item $N_{env} = 20$
  \end{itemize}
\end{enumerate}

\begin{figure}
  \centering
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{gridworld_5x5_100_2_8_1337__steps}
    \subcaption{Comparison with baselines}
    \includesvg[width=\linewidth]{gridworld_5x5_100_2_8_1337__dqn__steps}
    \subcaption{Greedy and $\epsilon$-greedy DQN agent}
    \includesvg[width=\linewidth]{gridworld_5x5_100_2_8_1337__2__steps}
    \subcaption{Planning horizon: 2}
    \includesvg[width=\linewidth]{gridworld_5x5_100_2_8_1337__4-8__steps}
    \subcaption{Planning horizon: 4-8}
  \end{minipage}
  \begin{minipage}{.49\linewidth}
    \includesvg[width=\linewidth]{gridworld_5x5_20_1_20_1337__steps}
    \subcaption{Comparison with baselines}
    \includesvg[width=\linewidth]{gridworld_5x5_20_1_20_1337__dqn__steps}
    \subcaption{Greedy and $\epsilon$-greedy DQN agent}
    \includesvg[width=\linewidth]{gridworld_5x5_20_1_20_1337__2__steps}
    \subcaption{Planning horizon: 2}
    \includesvg[width=\linewidth]{gridworld_5x5_20_1_20_1337__4-8__steps}
    \subcaption{Planning horizon: 4-8}
  \end{minipage}

  \caption{Results for experiments in changing environments. Left row: results for the setup with rare changes; right row: results for the setup with frequent changes.}
\end{figure}

From this set of experiments we conclude that in average our agent adapts to changes faster than DQN. Which also makes it perform more stable.

In contradiction to our results for the experiments in fixed handcrafted environments we found that longer goal list makes our agent to perform worse.

\section{Conclusion}

\bibliography{citations}

\end{document}
